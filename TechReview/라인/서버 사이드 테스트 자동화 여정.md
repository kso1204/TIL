# 서비스 안전성 (테스트 자동화)

1. https://engineering.linecorp.com/ko/blog/server-side-test-automation-journey-1/

- 본 글은 위 링크를 읽고, 해당 내용을 정리해본 글이다.

# 서버 사이드 테스트 자동화 여정 - 1

1. 왜 테스트를 자동화 해야하는가?

- 높은 수준의 서비스 안전성을 보장하기 위해

- 소프트웨어 개발 과정에 있어 테스트는 빼놓을 수 없는 매우 중요한 단계 중 하나이다. 

```

객체지향 프로그래밍에서는 의존성 주입, 제어의 역전 등으로 다형성과 슈퍼 클래스, 서브 클래스로 인해

컴파일 타임과 런타임 때 동작이 다르기에 테스트 코드가 필요하다고 생각하지만,

정해진 결과 값이 나오는 함수형 프로그래밍에서는 이런 테스트 코드가 꼭 필요하지 않다는 의견을 들었다.

아직 함수형 프로그래밍으로 구축해본적이 없어 잘은 모르지만 이런 의견도 있다는 점

```

- 시스템이 요구 사항에 맞게 개발되었는지 평가하고 기대한 대로 동작하는지 확인하는 가장 기본적인 활동

- 테스트는 경우에 따라 단순히 기능의 동작을 확인하는 수준을 넘어 지속적으로 서비스의 품질을 측정하고 개선하기 위한 데이터로도 활용된다.

- 소프트웨어에 대한 의존성이 날로 증가하는 오늘날엔, 수많은 사람들의 일상 속으로 파고든 서비스의 오동작과 장애가 우리의 일상에 미치는 영향이 과거보다 더 커졌다.

- 또한, 새롭고 편리한 기능을 사용자에게 보다 빠르게 전달하기 위해 신규 버전의 소프트웨어를 실제 서비스에 적용하는 배포 주기가 더 빨라지면서 테스트에 대한 관심과 중요성이 높아지고 있다.

2. 어떻게 달성하는가?

- 코드 리뷰, 테스트 자동화, 기술 스터디, 테크 토크, 장애 회고 등 다양한 방법

# 지금 당장 시작할 수 있는 테스트 자동화 활동 찾기

# 단위 테스트 실행 자동화 및 결과 확인 방법 개선하기

1. 실패한 테스트 케이스가 존재하면 배포할 수 없도록 강제하는 정책은 빌드 파일의 기능적 안전성은 어느 정도 보장할 수 있었지만,

2. 개발이나 테스트 환경에 배포하는 중에 실패한 테스트 케이스가 나타나면 배포가 중단되면서 업무 진행 속도를 심각하게 저하시키는 문제가 발생했다.

3. 테스트 케이스 자체가 잘못 작성된 경우가 문제를 일으키는 경우도 있었기에 이 방법을 고수해야 하는지에 대한 의문이 들었다.

4. 이 문제를 해결하기 위해 '코드 리뷰 단계'에서 배포 가능 여부가 확인 될 수 있다면 좋겠다고 생각했다.

5. Jenkins의 'GitHub pull request builder'라는 플러그인을 통해 이 생각을 실현할 수 있었다.

- 참고 사이트 : https://taetaetae.github.io/2020/09/07/github-pullrequest-build/

- User <-> GitHub <-> Jenkins

6. 최종 동작을 결정짓는 하나의 설정 객체를 조립하기 위해서는, 웹 콘솔 서비스를 제공하는 설정 관리(Configuration Management)에서 주이합 설정값과 빌드 결과물에 함께 패키징되어 배포될 파일을 조합해야 한다.

- 이 부분에 대해서는 정확하게 이해가 안 됨

7. 이로 인해 설정 구성에 조금이라도 잘못이 있다고 판단되면 서버 모듈의 기동 자체가 실패하도록 개발했는데, 그 때문에 서버를 기동하는 중에 문제가 탐지되어 서버 기동에 실패하는 경우가 많이 발생했다.

8. 비록 이 문제가 서비스 안전성에 영향을 끼치지 않는다고 해도, 설정 오류가 포함된 변경 사항이 수많은 개발자들이 내려받는 메인 개발 브랜치에 머지된다면,

9. 개발자의 개발 환경에서 서버 모듈을 기동해 볼 수 없는 상황이 발생하여 팀 전체 생산성을 저하시키는 문제가 발생할 수 있었다.

- 왜 이런 문제가 발생했을까?

10. 이 문제는 설정 객체를 조립하는 단위 테스트를 작성하고 이를 배포 환경별로 빌드하여 테스트를 실행하도록 구성함으로써 쉽게 해결할 수 있었다.

11. 다만 실제로 이 구성을 기반으로 업무를 진행하니 전체 테스트 코드가 배포 환경별로 매번 실행되어 Jenkins 서버 자원이 부족해지는 문제가 발생했다.

12. 특정 단위 테스트만 수행시킬 방법을 찾아야 했는데, Maven의 플러그인에서 특정 단위 테스트만 실행시키는 기능을 제공하고 있어서 이를 이용하여 문제를 해결했다.

# 연계 컴포넌트 상태 확인 테스트 자동화 및 확인 방법 개선하기

1. 신규 서비스를 미디어 플랫폼에서 제공하는 기능과 연동할 때, 각 서비스의 요구 사항에 맞게 스토리지 용량, 스토리지 성능, 스토리지 내 파일 저장 규칙, 미디어 콘텐츠 가공 방식 등 다양한 항목에 대한 설정을 구성하게 된다.

2. 설정에 포함된 연계 컴포넌트들의 준비 상황을 확인하지 못한 채 배포하여 문제가 발생하는 사례가 있었다.

3. 이 문제를 해결하기 위해 우리 모듈과 연동된 컴포넌트들이 우리가 기대하는 상태로 구성되어 있는지 점검 하는 테스트 자동화가 필요하다는 생각을 하게 됐다.

4. 각 제약 사항을 만족하는 환경에서 인프라 구성, 연계 서비스 서버 및 API 구성 준비 현황 등을 확인할 수 있는 테스트 코드와 스크립트를 실행할 수 있도록 구성했다.

5. 이 문제는 대형 장애를 초래할 수 있기 때문에 이 결과 또한 코드 리뷰 단계에서 확인할 수 있도록 만들었다.

# 성능 테스트 자동화하기

1. LINE LIVE 서비스에선 실시간성을 보장하는 것이 매우 중요하다. 이를 위해선 시스템을 구성하는 서버 모듈이 항상 최대의 성능을 낼 수 있다는 것을 보장해야 하기 때문에 성능 테스트를 자동화하는 것이 시급했다.

2. 하지만 일반적인 API 트래픽이 아니라 미디어 스트림(Stream) 트래픽을 가상으로 대량 발생시켜야 하기 때문에 개발 업무용 PC 환경에서 테스트를 실행하는 게 어려웠다.

3. 또한 개발자가 이를 매번 검증하는 데 들어가는 비용도 높았다.

4. 이 문제를 기존의 테스트 자동화 방법을 변형하여 해결할 수 있지 않을까 고민한 결과, 테스트 전용 서버에 테스트 대상이 되는 서버 모듈을 배포하고 테스트용 트래픽만 생성할 수 있다면 어렵지 않게 해결할 수 있을 것 같았다.

5. PR이 생성되면 성능 테스트 대상이 되는 코드를 빌드하여 배포하고, 성능 테스트를 수행한다.

```

모르겠는 부분

미디어 스트림 트래픽을 어떻게 가상으로 대량 발생시키는지..?

테스트 수행 후 나온 지표를 분석하는 개념?

```

# 피드백을 좀 더 빨리 전달하기

1. Jenkins 서버에서 각 PR마다 수행하는 테스트 항목이 많아 병목 현상이 발생하고 있었다.

2. Jenkins는 이런 상황에서 활용할 수 있는 노드 추가 기능을 제공한다.

3. Jenkins에 슬레이브 노드를 추가하면 컴퓨팅 파워를 선형적으로 늘릴 수 있다.

# 첫 테스트 자동화 결과

1. 테스트를 자동화하면서 테스트 작업이 정형화되고, 반복 업무와 실수가 많이 줄어들었다.

2. 무엇보다 훨씬 많은 자동화 테스트 작업을 수행하면서 결과 확인까지 10분이 채 걸리지 않아 생산성 향상

3. 하지만 이정도 테스트 활동으로는 오랜 기간에 걸쳐 정립한 일관된 기준에 따른 시스템 평가와 정량화 된 테스트 지표 수집, 그리고 자동화 측면에서 부족하다는 문제를 해결하기에 충분하지 않다고 생각했다.

# 서버 사이드 테스트 자동화 여정 - 2

1. 안정적인 서비스 개발과 운영을 위한 테스트 자동화 환경에 Docker를 활용한 사례

# 대규모 코드 변경 결과 검증하기

1. 팀의 전체적인 생산성이 올라갔지만, 기능 단위 또는 API 단위로 발생하는 문제를 탐지할 수 없다는 문제가 아직 남아 있었다.

# 통합 테스트 개발 및 적용

1. 미디어 플랫폼 주요 서버 모듈의 대부분이 Netty 기반으로 개발되었기 때문에 Spring MVC 테스트(Mock MVC)와 같이 잘 만들어진 프레임워크를 활용할 수 없었다.

2. 또한 통합 테스트는 일반적으로 외부 시스템 모킹(mocking)이 어렵다. 유지보수에 비용이 많이 든다라고 알려져 있어서 더욱 고민이 깊어졌다.

3. 하지만 곰곰이 생각해보니 플랫폼 조직이 제공하는 API는 한번 개발되면 명세가 잘 바뀌지 않고, 연동된 서비스 팀과의 협의 없이 API의 동작을 바꿀 수 없으며,

4. 수많은 서비스팀과 연동되어 있는 API는 사실상 변경이 불가능하다는 특성이 있어서 통합 테스트를 개발해도 유지 보수 비용이 많이 들 것 같지 않았다.

```

매물클린관리센터에서 특정 API response값이 변경되는 경우가 있었는데, 이 변경과 위 내용이 동일한 부분인지 잘 모르겠다.

```

5. 외부 시스템 모킹이 어렵다는 부분도, 미디어 플랫폼을 구성하는 모듈이 외부 시스템과 연동하는 부분을 모두 설정으로 제어할 수 있게 설계된 구조 덕분에 쉽게 해결할 수 있을 것 같았다.

6. 무엇보다 PR 단위로 통합 테스트를 수행한다면 회귀 테스트를 수행하는 효과를 얻을 수 있으므로 변경 영향의 범위와 발생할 가능성이 있는 사이드 이펙트를 금방 찾을 수 있겠다는 생각이 들었다.

7. 통합 테스트 시나리오를 어떻게 작성하고 관리할 것인가에 대한 답이 떠오르지 않았다.

- 이 방법을 해결하기 위해 python, pytest를 이용해 쉽고 빠르게 통합 테스트를 작성했다.

```

왜 Spring으로 작성하지 않은 것일까?

```

8. 우연히 접하게 된 python과 pytest 덕분에 테스트 환경을 아주 적은 공수로 구성했다.

9. 하지만 이 통합 테스트 결과를 어떻게 보관하고 또 어떻게 보고해야 하는지에 대한 고민에 빠졌다.

10. pytest 보고서를 GitHub에 댓글로 첨부하는 것은 가독성도 좋지 않고 PR 리뷰 페이지의 로딩 속도를 저하시켜 고민이 되었다.

11. 그래서 HTML 파일 형태의 보고서를 생성할 수 있다면 보관하기도 좋고 나중에 커스터마이징 할 수 있다고 생각했는데, pytest-html이라는 플러그인을 찾을 수 있었다.

12. pytest-html에서 생성된 단일 HTML 보고서 파일을 S3에 업로드 한 뒤 이곳의 URL을 GitHub 댓글로 달게 구성했다.

13. PR이 생성되면 통합 테스트가 실행되고, 그 결과는 아래 그림과 같이 GitHub 댓글로 달리게 구성하여 통합 테스트 결과를 보기 쉬우면서도 오래 보관할 수 있게 되었다.

# 통합 테스트 보고서 생성 시간 단축하기

1. 코드 리뷰 과정에 통합 테스트가 포함된 초기에는 통합 테스트 환경 자체의 오동작으로 테스트 결과를 신뢰할 수 없었던 기간이 있었다.

2. 테스트가 PR 단위로 격리된 환경에서 병렬로 실행될 수 있게 구성해야 했다.

3. 격리된 환경을 구성하기 위한 시스템을 개발하기 위해 해결하는 기술적인 문제들이 몇 가지 있었다.

```

1. 테스트 대상이 되는 PR의 빌드 결과물은 어떤 테스트용 장비에 배포해야 할까?

2. 테스트 대상이 되는 PR의 빌드 결과물은 테스트용 장비의 몇 번 포트에 할당해서 테스트해야 할까?

3. 동적으로 결정되는 PR의 테스트 대상 모듈의 IP 주소와 포트를 테스트 시나리오에서 어떻게 알 수 있을까?

4. 테스트 환경 구성은 내 주 담당 업무가 아닌데 직접 개발하면 나중에 유지 보수가 가능할까?

5. 각 테스트의 실행 환경을 서로 잘 격리시키면서 단기간에 개발하는 것이 가능할까?

6. 테스트 환경 시스템의 상태에 따라 테스트의 성공과 실패가 결정되는 불확실성이 존재한다면 테스트 결과를 신뢰할 수 있을까?

```

# Docker를 이용하여 통합 테스트 환경 개선하기

1. 1, 2, 5번 문제는 Docker가 지원하는 컨테이너 오케스트레이션 도구를 이용하여 해결할 수 있었고, 

2. 3번 문제는 Docker 클러스터 내 서비스를 자동으로 발견하고 도메인을 할당해 주는 'traefik'과 같은 프록시 서버를 이용하면 해결할 수 있었다.

3. 4, 6번 문제는 기술적 완성도가 높은 Docker를 이용하여 항상 동일한 형상으로 서버 구성을 프로비저닝하면 해결할 수 있을 것이라고 판단했다.

# 외부 시스템 의존 로직도 테스트할 수 있게 만들기

1. PR 단위로 격리된 환경에서 실행 가능한 통합 테스트 환경이 구성되었지만, 외부 시스템에 의존하는 로직에 대한 테스트가 일관성 있게 동작하게 만드는 것은 쉽지 않았다.

2. 외부 시스템 API가 변경디면서 테스트 실패가 발생하기도 하고, 외부 시스템 API의 처리 속도가 테스트 실행 속도에 영향을 주기도 했다.

3. 이런 문제를 해결하기 위해 테스트 더블(test double) 역할을 수행해 줄 수 있는 모의(mock) 서버를 개발하고 이 모의 서버도 PR 단위로 격리시켜 생성할 수 있도록 Docker 클러스터에 배치, 구성했다.

4. 이 모의 서버는 외부 시스템의 API를 직접 모킹(mocking)하는 방식으로 개발하지 않고, 모의 서버의 동작을 결정할 수 있는 명세서를 주입할 수 있도록 설계했다.

```

API를 직접 모킹하는 방식과 모의 서버의 동작을 결정할 수 있는 명세서 주입의 차이는..?

```

5. 덕분에 외부 시스템이 새로 추가되더라도 모의 서버는 전혀 수정할 필요가 없어 유지보수가 편리하다.

# 서버 사이드 테스트 자동화 여정 - 3

1. Docker의 기술적 특성 때문에 발생하는 문제들을 해결하는 과정과, 해결하기 위해 구축했던 인프라 환경

# Docker 적용 후 발생한 문제점과 해결 방법

1. Docker를 활용하니 이전에는 생각하지 못했던 형태의 테스트(PR 단위로 격리되어 구성되는 가상 테스트 환경에서 통합 테스트 수행)가 가능해졌다.

2. 그런데 Docker 컨테이너는 종료될 때 컨테이너 내부 데이터가 사라져 버리는 특성이 있었다. **

3. 이 특성 때문에 로그를 확인할 수 없어서 테스트가 실패했을 때 원인을 찾기가 어려웠다.

4. Docker는 이런 문제를 해결하기 위해 호스트 머신에 데이터를 저장할 수 있는 방법을 제공하고 있다.

5. 이를 활용하면 로그를 지속적으로 보관할 수 있지만, 로그가 물리적으로 어느 서버에 저장되어 있는지 확인할 방법이 필요했다.

6. 이 문제를 해결하기 위해서 보통 중앙 집중형(centralized) 로깅을 많이 사용한다.

7. 중앙 집중형 로깅이 가능한 환경은 다양한 방법으로 구성할 수 있다.

8. 이미 팀 내 많은 개발자들이 사용하고 운영한 경험이 있는 ELK를 고려해봤지만, 접근(access)로그와 같이 한 줄로 표현되는 로그가 아니라 여러 줄로 표현 되는 디버깅 로그는 Kibana로 보기에 적합하지 않았다.

```

ELK 스택

참고 사이트 : https://www.elastic.co/kr/what-is/elk-stack

Elasticsearch, Logstash, Kibana, 이 오픈 소스 프로젝트 세 개의 머리글자

Elasticsearch는 검색 및 분석 엔진, Logstash는 여러 소스에서 동시에 데이터를 수집하여 변환한 후 Elasticsearch 같은 "stash" 서버로 전송하는 서버 사이드 데이터 처리 파이프라인이다.

Kibana는 사용자가 Elasticsearch에서 차트와 그래프를 이용해 데이터를 시각화할 수 있게 해준다.

```

9. 그래서 중앙화된 저장소에서 디버깅 로그를 찾을 수 있는 도구를 따로 만들까 고민하던 차에, Kibana 6.7 버전에서 Logs라는 기능이 정식으로 릴리스 되었다는 것을 알게 되었다.

10. 이 기능은 요청 식별 ID를 이용해 여러 줄로 구성된 디버그 로그를 보기에 적합한 기능이었다.

11. 그래서 이를 바탕으로 Docker 컨테이너에서 발생하는 서버 모듈의 로그를 모두 Elasticsearch 저장소에 저장할 수 있도록 구성했다.

12. 통합 테스트 환경에서 수행된 테스트 결과로 생성되는 모듈의 디버깅 로그를 Elasticsearch에 저장하고 Kibana로 조회하는 구조이다.

13. 적용 결과, 테스트가 어느 Docker 노드에서 실행됐는지 확인할 필요 없이 요청 식별 ID를 이용하여 디버깅에 필요한 로그를 쉽게 찾을 수 있게 되었다.

# 로그 검색 인프라를 활용하여 디버깅 시간 단축하기

1. Elasticsearch와 Kibana를 이용하여 통합 테스트 환경에서 발생하는 로그를 찾을 수 있게 되었지만, 로그를 찾기 위해 필요한 요청 ID를 찾는 게 어렵다는 문제가 발생했다.

2. 테스트에 사용된 요청 ID가 모든 테스트 시나리오의 실행 결과 로그에 포함되도록 개발했지만, 통합 테스트 구조를 모르는 사용자 입장에서는 디버깅 대상이 되는 요청 ID를 찾는 게 어려웠다.

3. 이 문제를 해결하기 위해 pytest-html 플러그인이 제공하는 보고서 커스터마이징 기능을 활용했다.

4. 커스터마이징을 통해 테스트 케이스에서 발생된 모든 트랜잭션 기록을 보고서에 첨부할 수 있었다.

5. 또한 각 트랜잭션 기록을 클릭하면 디버그 로그를 바로 확인할 수 있도록 바로가기 링크도 연결했다.

# 테스트 병렬화로 테스트 수행 시간 단축하기

1. 테스트를 수행하는 데 오랜 시간이 걸리는 원인은 테스트 케이스가 서버로 요청을 보내고 응답이 올 때까지 기다린 후 검증하는 구조 때문이었다.

2. 병렬화하면 어느 정도는 해결될 것 같아 pytest에 동시에 여러 개의 테스트 케이스를 실행할 수 있는 플러그인을 찾아서 적용했고, 그 결과 테스트 수행 시간을 기존 대비 1/10으로 단축시킬 수 있었다.

# 통합 테스트 실행 환경 준비 시간 단축하기

1. 평균 작업 수행시간이 긴 작업과 짧은 작업이 섞여서 동작하기 때문에 각 PR에서 수행해야 하는 테스트 케이스가 모두 완료되기까지 기다려야 하는 시간이 증가한다는 것을 깨닫게 되었다.

2. 이 문제는 여러 테스트 작업을 각 작업 특성에 맞는 전용 Jenkins 풀(pool)에서 실행하면 해결할 수 있겠다고 생각했다.

3. 막상 문제를 해결하기 위해 새로운 Jenkins 풀을 구성하려니 발생하게 될 운영 비용이 부담으로 다가왔다.

4. Jenkins에선 이런 경우에 활용할 수 있는 라벨(label) 기능을 제공하고 있었다.

5. 라벨을 이용하면 Jenkins에서 잡을 실행할 때 어느 노드에서 실행할지 결정할 수 있어서 각 직업별 특성에 맞게 서로 분리된 전용 노드에서 작업을 실행할 수 있도록 구성을 변경했다.

# 서버 사이드 테스트 자동화 여정 - 4

1. 실제 서비스에서 API를 호출하는 흐름 중에 발생하는 문제도 코드 리뷰 시작 전부터 확인하는 등 많은 도움을 받고 있는데, 성능 테스트를 자동화하며 겪은 일들을 공유한다.

# 성능 테스트 자동화 목표 세우기

1. 성능 테스트 결과에서 확인해야 하는 기본적인 항목들

```

기본적인 시스템 자원 지표(CPU 사용량, 메모리 사용량, 로드(load), 스왑(swap) 사용량, 디스크 I/O 등)

초당 요청 처리 건수

처리 시간

처리량

```

2. 위 지표들이 개발 단계에서 성능과 관련된 문제를 발견하고 개선하는 데 유의미한 인사이트를 제공할 수 있을지 의문이 들었다.

3. 이런 지표를 통해 '성능에 문제가 있다 혹은 없다'라는 사실은 확인할 수 있지만, '무엇이 문제인지' 또는 '어디가 문제인지'와 같은 정보는 얻을 수 없다고 생각했기 때문이다.

4. 해답은 성능과 관련된 문제가 발생했을 때 확인할 필요가 잇는 애플리케이션 상태 관련 지표를 모두 함께 수집해 두는 것이었다.

5. JVM에서 동작하는 시스템에선 아래와 같은 정보를 수집한다면 성능과 관련된 문제를 파악하면서 해결의 실마리도 찾을 수 있다.

```

GC(Garbage Collection) 및 힙(Heap) 메모리 상태

스레드(Thread) 상태 정보

다이렉트 메모리 사용률

스레드 개수

열려있는 파일 지시자(File Descriptor)의 수

발행되는 로그 양

```

6. 성능 테스트를 수행한 후 위 지표들을 리포트로 제공할 수 있다면, '문제가 있다'는 사실과 함께 어떤 부분에 중점을 두고 추적하면 되는지 단서도 제공할 수 있겠다고 생각했다.

# 자동화 테스트 범위 정하기

1. 정확한 성능 테스트를 수행하기 위해서는 테스트 환경을 실제 서비스 환경과 최대한 유사한 환경으로 구성할 필요가 있다.

2. 하지만 규모가 큰 시스템에서는 실제 서비스 환경과 유사한 테스트 환경을 구성하는 게 쉽지 않다.

3. 실제 서비스는 전 세계의 사용자가 불편함 없이 사용할 수 있도록 다수의 PoP(Point of Presense)를 이용하여 구성하는데,

4. 각 PoP별로 운용 가능한 서버 사양이나 구성이 상이하기 때문에 '실제 서비스 환경과 유사한 구성'이 무엇인지 정의하기가 쉽지 않다.

5. 실제 서비스 환경을 모방하기 보다는, 우리 시스템에 맞는 표준 테스트 환경을 정의 및 구성한 뒤 표준 테스트 환경에서 생성된 결과를 참조하여 각 PoP별 상황에 맞게 튜닝하는 게 좋겠다고 생각했다.

```

표준 테스트 환경

- 테스트 대상 모듈이 현재 서비스에서 사용 중이거나, 앞으로 사용하게 될 사양과 비슷한 수준의 서버 중 사내에서 가장 수급이 원활한 서버와 유사한 네트워크 환경으로 구성

테스트 범위

- 로직이 가장 자주 변하거나 스토리지 입출력을 전담하기 때문에 병목 구간이 될 수 있는 단일 모듈(캐시 레이어를 모두 비활성화한 상태로 테스트)

특이한 점은 캐시 레어이를 모두 비활성화 상태로 테스트 한다는 부분, 실제로 서비스에 투입하기 전에 진행하는 테스트에서는 캐시 레이어가 포함된 상황에서 검증한다.

```

# 어떤 형태의 성능 테스트를 진행할 것인가?

1. 단순한 테스트를 넘어 시스템을 보다 폭넓게 이해할 수 있도록 도와주는 테스트?

2. 이런 생각을 바탕으로 다양한 성능 테스트 방법 중 로드(load)테스트와 스트레스 테스트, 그리고 스파이크(spike) 테스트를 먼저 적용하기로 결정했다.

3. 매년 각 국가가 속한 시간대별로 1월 1일 0시 0분 즈음에는 새해맞이 덕담을 나누며 미디어 파일을 송수신하는 사용자가 폭증하기 때문에 스파이크 테스트는 반드시 포함해야 겠다고 판단했다.

4. 반면 내구(endurance) 테스트는 고민 끝에 포함시키지 않는 것으로 결정했다.

5. 테스트 목적 자체가 오랜 시간 부하를 가하며 시스템을 평가하는 것이기 때문에 개발자에게 빠른 피드백을 제공하기 어려웠기 때문이다.

6. 또한 실제 서비스 배포 전에 카나리(canary) 배포를 통해 실 환경에서의 안정성도 평가하고 있어서 제외해도 큰 문제가 없다고 판단했다.

```

로드 테스트, 스트레스 테스트 참고 자료 : https://oracle.tistory.com/467, https://nesoy.github.io/articles/2018-08/Testing-Performance

로드 테스트 - 일정 시간 동안 부하를 가하여 서버가 처리할 수 있는 최대 TPS와 응답시간을 구하는 것

스트레스 테스트 - 정상보다 더 많은 부하를 주는 테스트

스파이크 테스트 - 순간적으로 사용자 수를 증가시키는 테스트

롤링, 블루/그린, 카나리 배포 전략 참고 자료 : https://reference-m1.tistory.com/211

카나리 배포 - 미리 위험을 감지하기 위한 카나리, 즉 위험을 빠르게 감지할 수 있는 배포 전략이다

지정한 서버 또는 특정 user에게만 배포했다가 정상적이면 전체를 배포한다.

서버의 트래픽 일부를 신 버전으로 분산하여 오류를 확인할 수 있다.

이런 전략은 A/B 테스트가 가능하며, 성능 모니터링에 유용하다.

트래픽을 분산시킬 때는 라우팅을 랜덤하게 할 수 있고, 사용자로 분류할 수 있다.

```

# 테스트는 어떻게 수행할 것인가?

1. 테스트 범위와 수행할 테스트의 종류를 정하고 나니 이제 어떻게 부하를 생성할지 결정해야 했다.

2. 사내에서는 nGrinder를 주요 IDC(Internet Data Center)마다 설치해 서비스 형태로 제공하고 있으며, nGrinde(https://naver.github.io/ngrinder/)r에서 REST API를 제공하고 있기도 하다.

3. 하지만 기존의 테스트 자동화 시스템과 연동하는 과정에서 검토해 보았을 때 자동화하기 어려운 부분이 있어서 사용하기 어렵다고 판단

4. 20년이 넘는 기간 동안 많은 개발자가 사용하고 있는 JMeter(https://jmeter.apache.org/)도 검토했지만 적합하지 않았다.

5. JMeter는 그 역사와 명성에 걸맞게 여러 기능을 지원하고 있었는데, 원하는 기능을 사용하기 위해서는 시스템이 복잡해질 가능성이 높았다.

```

CLI(Command Line Interface)를 제공하는 것은 물론, RPS(Request Per Second)와 처리량(throughput), 지연 속도(latency), 응답 시간(response time) 등을 확인할 수 있는 리포트도 제공하고 있어서

테스트 자동화 시스템에 적용하기 편리해 보였다.

```

5. 결국 python을 통해 직접 부하 생성기를 만들었다.

6. async, await 키워드를 활용해 코루틴과 함께 asyncio 라이브러리를 활용하면 python으로도 비동기 처리가 쉽게 가능하기 때문에 대량의 HTTP 요청을 동시에 생성할 수 있었다.

```

코루틴이 고루틴이랑 비슷한 줄 알았는데 아니었다.

코루틴은 루틴의 일종으로서, 협동 루틴이라 할 수 있다. 상호 연계 프로그램을 일컫는다고도 표현 가능하다.

루틴과 서브 루틴은 서로 비대칭적인 관계이지만, 코루틴들은 완전히 대칭적인, 즉 서로가 서로를 호출하는 관계이다.

코루틴에서는 무엇이 무엇의 서브루틴인지를 구분하는 것이 불가능하다.

```

# 결과 리포트는 어떻게 생성할 것인가?

1. 직접 만든 부하 생성기를 이용하니 자유도가 높아서 좋기는 했지만 결과 리포트를 생성하는 게 문제였다.

2. 이미 잘 만들어져 있는 외부 솔루션을 사용하면 해당 솔루션에서 제공하는 결과 리포트를 그대로 사용하면 되지만, 직접 만드는 바람에 결과 리포트도 스스로 해결해야 했다.

3. 코딩 없이 시각화가 가능한 도구를 이용하여 시각화를 진행한 후 결과 이미지를 캡처해서 보고서에 첨부하는 방법을 선택했다.

# 성능 테스트 자동화 환경 구성하기

1. 성능 테스트를 하기 위한 빌드 결과물 배포는 기존의 자동화 테스트 파이프라인을 조금 수정해서 쉽게 구성할 수 있었다.

2. 테스트 자동화 환경을 구성하던 초기부터 테스트 파이프라인이 복잡하게 변화해 나갈 것을 예상하고 Jenkins에서 제공하는 Pipeline 기능을 이용하여 테스트 파이프라인을 구성해 두었기 때문이다.

3. Jenkins Pipeline은 여러 Jenkins 노드와 잡(job)이 엮인 작업을 정의할 때 유용하다.

4. Blue Ocean UX를 이용하면 현재 구성되어 있는 파이프라인의 흐름을 쉽게 이해할 수 있고 확장도 간단하게 진행할 수 있다.

5. 성능 테스트 목적의 빌드 결과물은 Pipeline을 통해 Docker 이미지로 배포되는데, 빌드 결과물을 이미지로 만들어 두면 문제를 재현하고 싶을 때 쉽게 재현할 수 있다는 장점이 있기 때문에 Docker를 활용했다.

# 서버 사이드 테스트 자동화 여정 - 5

# 시스템과 JVM 지표 시각화하기

1. 미디어 플랫폼 조직에선 각종 지표를 좀 더 정밀하게 모니터링하면서 쉽게 분석하기 위해 기본적으로 제공되는 전사 모니터링 플랫폼 외에도 

2. Grafana(https://grafana.com/)와 Prometheus(https://prometheus.io/)를 이용해 다양한 시스템 지표를 수집하고 모니터링하고 있다.

3. Prometheus가 제공하는 대표적인 익스포터(expoter) 중 하나인 노드 익스포터를 사용하면 CPU와 메모리, 로드, swap, 디스크와 같은 기본적인 시스템 자원 지표를 확인할 수 있는 대시보드를 쉽게 만들 수 있다.

```

해당 지표에서 확인하는 것이 AWS에서 제공하는 기능과 다른 것일까? 아니면 AWS를 사용하지 않아 오픈소스로 대체한 것일까?

```

4. 또한 JVM 지표는 Spring Boot 2.0부터 기본 탖배된 Micrometer를 이용하면 모니털이 시스템을 위한 코드를 별도로 작성하지 않아도 여러 지표를 쉽게 확인할 수 있다.

5. Spring Boot Actuator에서 제공하는 Prometheus Endpoint를 활용하면, GC와 힙 메모리 상태, 스레드 블로킹 정보, 다이렉트 메모리 사용률, 열려있는 파일 지시자(File Descriptor) 수 등을 확인할 수 있는 대시보드를 만들 수 있다.

# 애플리케이션 지표 시각화하기

1. 애플리케이션 영역의 지표는 문제가 발생할 경우 추적할 수 있어야 하기 때문에 단순히 시각화만 가능한 도구를 사용하는 것은 좋지 않겠다고 생각했다.

2. 일반적으로 APM(Application Performance Management) 도구를 사용하면 요청을 추적할 수 있는데, 여러 APM 도구 중 Naver Pinpoint를 검토했다.

3. 하지만 아쉽게도, 미디어 플랫폼에서 운혀나는 서버 모듈이 Netty 기반으로 개발되어 완전하게 추적할 수 없거나 혹은 아예 추적이 불가능해서 사용할 수 없었다.

4. 접속 로그를 기반으로 문제를 추적해 나가는 것이 충분히 가능하기 때문에 ELK를 사용했다.

5. Kibana가 제공하는 강력한 시각화 기능과 Elasticsearch의 조건 검색 기능을 활용하면, 문제가 되는 요청이나 패턴을 쉽게 찾을 수 있어서

6. 테스트 자동화를 진행하기 전과 비교할 때 문제를 추적하는 데 소요되는 시간을 획기적으로 줄일 수 있었다.

7. Kibana를 선택한 또 다른 중요한 이유 중 하나는 Kibanar가 시간의 경과에 따른 성능 패턴이나 이상치를 쉽게 확인할 수 있는 산포도(scatter plot)를 지원하기 때문이다.

8. Kibana는 6.2버전부터 시각화 라이브러리 중 하나인 'Vega'를 기반으로 새로운 형태의 시각화 방법을 제공한다.

9. 이를 통해 기존 Kibana 시각화가 지원하지 못했던 형태의 인터랙티브 시각화가 가능해졌다.

10. Vega 방식의 시각화는 마우스 클릭으로 그래프를 그릴 수 있는 Kibana의 다른 시각화와 비교할 때 다소 불편한 것은 사실이지만,

11. Hjson 문법에 맞춰서 Elasticsearch에 저장된 데이터 값과 차트에 표시할 값을 매핑하면, 고품질의 인터랙티브 차트를 단기간에 그릴 수 있다는 장점이 있다.