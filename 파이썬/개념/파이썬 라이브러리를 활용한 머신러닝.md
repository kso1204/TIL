# 파이썬 라이브러리를 활용한 머신 러닝

# 소개

1. 머신러닝은 데이터에서 지식을 추출하는 작업이다.

2. 머신러닝은 통계학, 인공지능 그리고 컴퓨터 과학이 얽혀 있는 연구 분야이며 예측 분석이나 통계적 머신러닝으로도 불린다.

# 머신러닝으로 풀 수 있는 문제

1. 가장 많이 사용되는 머신러닝 알고리즘들은 이미 알려진 사례를 바탕으로 일반화된 모델을 만들어 의사 결정 프로세스를 자동화하는 것들이다.

2. 이 방식을 지도 학습이라고 하며 사용자는 알고리즘에 입력과 기대되는 출력을 제공하고 알고리즘은 주어진 입력에서 원하는 출력을 만드는 방법을 찾는다.

3. 지도 학습의 예는 다음과 같다

- 편지 봉투에 손으로 쓴 우편번호 숫자 판별

- 의료 영상 이미지에 기반한 종양 판단

- 의심되는 신용카드 거래 감지

4. 이런 사례들에서 주목할 점은 입력과 출력이 상당히 직관적으로 보이지만, 데이터를 모으는 과정은 세 경우가 많이 다르다는 것이다.

5. 이 책에서 다룰 또 다른 알고리즘은 비지도 학습 알고리즘이다.

6. 비지도 학습에서는 알고리즘에 입력은 주어지지만 추력은 제공되지 않는다.

7. 이 알고리즘의 성공 사례는 많지만 비지도 학습을 이해하거나 평가하는 일은 쉽지 않다.

8. 비지도 학습의 예는 다음과 같다.

- 블로그 글의 주제 구분

- 고객들을 취향이 비슷한 그룹으로 묶기

- 비정상적인 웹사이트 접근 탐지

# 문제와 데이터 이해하기

1. 머신러닝 프로세스에서 가장 중요한 과정은 사용할 데이터를 이해하고 그 데이터가 해결해야 할 문제와 어떤 관련이 있는지를 이해하는 일이다.

# 첫 번째 애플리케이션: 붓꽃의 품종 분류

1. 한 아마추어 식물학자가 들에서 발견한 붓꽃의 품종을 알고 싶다고 가정해보자

2. 이 식물학자는 붓꽃의 꽃잎과 꽃받침의 폭과 길이를 센티미터 단위로 측정했다.

3. 또 전문 식물학자가 setosa, versicolor, virginica 종으로 분류한 붓꽃의 측정 데이터도 가지고 있다.

4. 이 측정값을 이용해서 앞에서 채집한 붓꽃이 어떤 품종인지 구분하려고 한다.

5. 이 아마추어 식물학자가 야생에서 채집한 붓꽃은 이 세 종류뿐이라고 가정해보자

6. 붓꽃의 품종을 정확하게 분류한 데이터를 가지고 있으므로 이 문제는 지도 학습에 속한다.

7. 이 경우에는 몇 가지 선택사항(붓꽃의 품종) 중 하나를 선택하는 문제다.

8. 그러므로 이 예는 분류(classification) 문제에 해당한다.

9. 출력될 수 있는 값(붓꽃의 종류)들을 클래스(class)라고 한다.

10. 데이터셋에 있는 붓꽃 데이터는 모두 세 클래스 중 하나에 속한다.

11. 따라서 이 예는 세 개의 클래스를 분류하는 문제이다.

12. 데이터 포인트 하나(붓꽃 하나)에 대한 기대 출력은 꽃의 품종이 된다.

13. 이런 특정 데이터 포인트에 대한 출력, 즉 품종을 레이블(label)이라고 한다.

# 데이터 적재

1. 우리가 사용할 데이터셋은 머신러닝과 통계 분야에서 오래전부터 사용해온 붓꽃 데이터셋이다.

2. 이 데이터는 scikit-leran의 datasets 모듈에 포함되어 있다.

3. load_iris 함수를 사용해서 데이터를 적재해보자.

```

from skleran.datasets import load_iris
iris_dataset = load_iris()

```

4. load_iris가 반환한 iris 객체는 파이쎤의 딕셔너리와 유사한 Bunch 클래스의 객체이다. 즉 키와 값으로 구성되어 있다.

```

iris_dataset.keys()

iris_dataset의 키:
dick_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])

```

5. DESCR 키에는 데이터셋에 대한 간략한 설명이 들어 있다.

6. target_names의 값은 우리가 예측하려는 붓꽃 품종의 이름을 문자열 배열로 가지고 있다.

```

iris_dataset['target_names']
['setosa', 'versicolor', 'virginica']

```

7. feature_names의 값은 각 특성을 설명하는 문자열 리스트이다.

```

iris_dataset(['feature_names'])
['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']

```

8. 실제 데이터는 target 과 data 필드에 들어 있다.

9. data는 꽃잎의 길이와 폭, 꽃받침의 길이와 폭을 수치 값으로 가지고 있는 Numpy 배열이다.

10. data 배열의 행은 개개의 꽃이 되며 열은 각 꽃에서 구한 네 개의 측정치이다.

11. 이 배열은 150개의 붓꽃 데이터를 가지고 있다.

12. 머신러닝에서 각 아이템은 샘플이라 하고 속성은 특성이라 부른다.

13. 그러므로 data 배열의 크기는 샘플의 수에 특성의 수를 곱한 값이 된다.

14. target 배열은 샘플 붓꽃의 품종을 담은 NumPy 배열이다.

15. target은 각 원소가 붓꽃 하나에 해당하는 1차원 배열이다.

# 성과 측정: 훈련 데이터와 테스트 데이터

1. 이 데이터로 머신러닝 모델을 만들고 새로운 데이터의 품종을 예측하려 한다.

2. 하지만 만든 모델을 새 데이터에 적용하기 전에 이 모델이 진짜 잘 작동하는지 알아야 한다.

3. 즉 우리가 만든 모델의 예측을 신뢰할 수 있는지 알아야 한다.

4. 불행히도 모델을 만들 때 쓴 데이터는 평가 목적으로 사용할 수 없다.

5. 모델이 훈련 데이터를 그냥 전부 기억할 수 있으니 훈련 데이터에 속한 어떤 데이터라도 정확히 맞출 수 있기 때문이다.

6. 이렇게 데이터를 기억한다는 것은 모델을 잘 일반화하지 않았다는 뜻이다.

7. 모델의 성능을 측정하려면 레이블을 알고 있는(이전에 본 적 없는?) 새 데이터를 모델에 적용해봐야 한다.

8. 이를 위해 우리가 가지고 있는 레이블된 데이터(150개의 붓꽃 데이터)를 두 그룹으로 나눈다.

9. 그중 하나는 머신러닝 모델을 만들 때 사용하며, 훈련 데이터 혹은 훈련 세트라고 한다.

10. 나머지는 모델이 얼마나 잘 작동하는지 측정하는 데 사용하며, 이를 테스트 데이터, 테스트 세트 혹은 홀드아웃 세트라고 부른다.

11. scikit-learn은 데이터셋을 섞어서 나눠주는 train_test_split 함수를 제공한다.

12. 이 함수는 전체 행 중 75%를 레이블 데이터와 함께 훈련 세트로 뽑는다.

13. 나머지 25%는 레이블 데이터와 함께 테스트 세트가 된다.

14. 훈련 세트와 테스트 세트를 얼만큼씩 나눌지는 상황에 따라 다르지만 전체의 25%를 테스트 세트로 사용하는 것은 일반적으로 좋은 선택이다.

15. scikit-learn에서 데이터는 대문자 X로 표시하고 레이블은 소문자 y로 표기한다.

16. train_test_split 함수로 데이터를 나누기 전에 유사 난수 생성기를 사용해 데이터셋을 무작위로 섞어야 한다.

```

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(iris_dataset['data'], iris_dataset['target'], random_state=0)

```

# 가장 먼저 할 일: 데이터 살펴보기

1. 머신러닝 모델을 만들기 전에 머신러닝이 없어도 풀 수 있는 문제는 아닌지, 혹은 필요한 정보가 누락되는지 않았는지 데이터를 조사해보는 것이 좋다.

2. 시각화는 데이터를 조사하는 아주 좋은 방법이다. 산점도(scatter point)가 그중 하나이다.

3. 산점도는 데이터에서 한 특성을 x 축에 놓고 다른 하나는 y 축에 놓아 각 데이터 포인트를 하나의 점으로만 나타내는 그래프이다.

4. 아쉽게도 컴퓨터 화면은 2ㅊ아ㅝㄴ이라 한 번에 2개(혹은 3개)의 특성만 그릴 수 있다.

# 첫 번째 머신러닝 모델: k-최근접 이웃 알고리즘

1. scikit-leran은 다양한 분류 알고리즘을 제공한다.

2. 여기서는 비교적 이해하기 쉬운 k-최근접 이웃 분류기를 사용한다.

3. 이 모델은 단순히 훈련 데이터를 저장하여 만들어진다.

4. k-최근접 이웃 알고리즘에서 k는 가장 가까운 이웃 '하나'가 아니라 훈련 데이터에서 새로운 데이터 포인트에 가장 가까운 'k개'의 이웃을 찾는다는 뜻이다.

5. 그런 다음 이 이웃들의 클래스 중 빈도가 가장 높은 클래스를 예측값으로 사용한다.

6. k-최근접 이웃 분류 알고리즘은 neighbors 모듈 아래 KNeighborsClassifer 클래스에 구현되어 있다.

7. 모델을 사용하려면 클래스로부터 객체를 만들어야 한다.

8. 이때 모델에 필요한 매개변수를 넣는다.

9. KNeighborsClassifier에서 가장 중요한 매개변수는 이웃의 개수다.

10. 우리는 1로 지정한다.

```

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=1)

```

# 예측하기

1. 이제 이 모델을 사용해서 정확한 레이블을 모르는 새 데이터에 대해 예측을 만들 수 있다.

2. 야생에서 꽃받침의 길이가 5cm, 폭이 2.9cm이고 꽃잎의 길이가 1cm, 폭이 0.2cm인 붓꽃을 보았다고 가정해보자

3. 그럼 이 붓꽃의 품종은 무엇일까?

4. 먼저 이 측정값을 Numpy 배열, 즉 샘플의 수(1)에 특성의 수(4)를 곱한 크기의 NumPy 배열로 만들어보자

```

X_new = np.array([[5, 2.9, 1, 0.2]])
X_new.shape: (1,4)

```

5. 붓꽃 하나의 측정값은 2차원 NumPy 배열에 행으로 들어간다.

6. scikit-learn은 항상 데이터가 2차원 배열일 것으로 예상한다.

7. 예측에는 knn 객체의 predict 메서드를 사용한다.

```

prediction = knn.predict(X_new)
iris_dataset['target_names'][predicition]

setosa

```

8. 우리가 만든 모델이 새로운 붓꽃을 setosa 품종을 의미하는 클래스 0으로 예측했다.

9. 그런데 어떻게 이 모델의 결과를 신뢰할 수 있을까?

10. 이 샘플의 정확한 품종을 모른다는 사실이 모델을 구축하는 데 중요한 의미를 가진다.

# 모델 평가하기

1. 앞서 만든 테스트 세트를 사용할 때가 왔다.

2. 이 데이터는 모델을 만들 때 사용하지 않았고 테스트 세트에 있는 각 붓꽃의 품종을 정확히 알고 있다.

3. 따라서 테스트 데이터에 있는 붓꽃의 품종을 예측하고 실제 레이블(품종)과 비교할 수 있다.

4. 얼마나 많은 붓꽃 품종이 정확히 맞았는지 정확도를 계산하여 모델의 성능을 평가한다.

```

y_pred = knn.predict(X_test)

print("테스트 세트의 정확도: {:.2f}".format(np.mean(y_pred == y_test)))

0.97

```

5. 또 knn 객체의 score 메서드로도 테스트 세트의 정확도를 계산할 수 있다.

```

print("테스트 세트의 정확도: {:.2f}".format(knn.score(X_test, y_test)))

0.97

```

6. 이 모델의 테스트 세트에 대한 정확도는 약 0.97이다.

7. 이 말은 테스트 세트에 포함된 붓꽃 중 97%의 품종을 정확히 맞혔다는 뜻이다.

8. 이 결과 이 모델은 새로운 붓꽃에 대한 정확도가 97%일 것이라고 기대할 수 있다.

9. 정확도가 높으므로 아마추어 식물학자는 이 애플리케이션을 충분히 신뢰하고 사용할만하다.

10. 이후의 장들에서 모델의 성능을 높이는 방법과 모델을 튜닝할 때 주의할 점을 살펴보자.

# 요약 및 정리

1. 머신러닝과 머신러닝 애플리케이션에 대해 간략한 소개에서 시작해, 지도 학습과 비지도 학습 차이를 설명했다.

2. 그리고 실측한 자료를 사용하여 붓꽃의 품종이 무엇인지 예측하는 작업을 자세히 묘사했다.

3. 모델을 구축하기 위해 전문가가 정확한 품종으로 구분해놓은 데이터셋을 사용했으므로 지도 학습에 해당하는 문제다.

4. 또한 품종이 세 개(setosa, versicolor, virginica)이므로 세 개의 클래스를 분류하는 문제이다.

5. 분류 문제에서는 각 품종을 클래스라고 하며 개별 붓꽃의 품종은 레이블이라 한다.

6. 붓꽃 데이터셋은 두 개의 NumPy 배열로 이루어져 있다.

7. 하나는 데이터를 담고 있으며 scikit-learn 에서는 X로 표기한다.

8. 다른 하나는 정확한 혹은 기대하는 출력을 가지고 있으며 y로 표기한다.

9. 배열 X는 특성들의 2차원 배열이므로 각 데이터 포인트는 행 하나로 나타나고, 각 특성은 열 하나가 된다.

10. 배열 y는 1차원 배열로 각 샘플의 클래스 레이블에 해당하는 0에서 2사이의 정수를 담고 있다.

11. 이 데이터셋을 모델 구축에 사용할 훈련 세트와 모델이 새로운 데이터에 얼마나 잘 적용될 수 있을지 평가하기 위한 테스트 세트로 나눴다.

```

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

iris_dataset = load_iris()

X_train, X_test, y_train, y_test = train_test_split(iris_dataset['data'], iris_datset['target'], random_state=0)

knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train, y_train)

print("테스트 세트의 정확도: {:.2f}".format(knn.score(X_test, y_test)))

테스트 세트의 정확도: 0.97

```

12. 이 짧은 코드에는 scikit-learn의 머신러닝 알고리즘들이 가진 핵심이 담겨 있다.

13. fit, predict, score 메서드는 scikit-learn 지도 학습 모델의 공통 인터페이스이다.

14. 이 장에서 소개한 개념과 함께 이런 모델들을 많은 머신러닝 작업에 적용할 수 있다.

# 지도 학습

1. 지도 학습은 가장 널리 그리고 성공적으로 사용되는 머신러닝 방법 중 하나이다.

2. 지도 학습은 입력과 출력 샘플 데이터가 있고, 주어진 입력으로부터 출력을 예측하고자 할 때 사용한다는 것을 기억하기 바란다.

3. 이란 입력/출력 샘플 데이터, 즉 훈련 세트로부터 머신러닝 모델을 만든다.

# 분류와 회귀

1. 지도 학습에는 분류(classification)와 회귀(regression)가 있다.

2. 분류는 미리 정의된, 가능성 있는 여러 클래스 레이블 중 하나를 예측하는 것이다.

3. 1장에서 붓꽃을 세 품종 중 하나로 분류하는 예를 보았다.

4. 분류는 딱 두개의 클래스로 분류하는 이진 분류와 셋 이상의 클래스로 분류하는 다중 분류로 나뉜다.

5. 이진 분류는 질문의 답이 예/아니오만 나올 수 있도록 하는 것이라고 생각할 수 있다.

6. 이메일에서 스팸을 분류하는 것이 이진 분류 문제의 한 예이다.

7. 반면에 붓꽃의 예는 다중 분류 문제이다.

8. 회귀는 연속적인 숫자, 또는 프로그래밍 용어로 말하면 부동소수점수(수학 용어로는 실수)를 예측하는 것이다.

9. 어떤 사람의 교육 수준, 나이, 주거지를 바탕으로 연간 소득을 예측하는 것이 회귀 문제의 한 예이다.

10. 출력 값에 연속성이 있는지 질문해보면 회귀와 분류 문제를 쉽게 구분할 수 있다.

11. 예상 출력 값 사이에 연속성이 있다면 회귀 문제이다.

12. 연소득을 예측하는 경우를 생각해보면 출력에 확연한 연속성이 있다.

```

어떤 사람이 1년에 40,000원 또는 40,001원을 벌 수 있다. 그 양은 다르지만 큰 차이는 아니다.

즉 우리 알고리즘이 40,000원을 예측해야 하는데 39,999원이나 40,001원을 예측했다고 하더라도 큰 문제가 되지 않는다.

반대로 웹사이트가 어떤 언어로 되어 있는지 인식하는 작업(분류 문제)에는 어느 정도란 것이 없다.

즉 웹사이트 언어는 한 언어가 아니면 다른 언어이다. 영어와 프랑스어 사이에 다른 언어는 없다.

```

# 일반화, 과대적합, 과소적합

1. 지도 학습에서는 훈련 데이터로 학습한 모델이 훈련 데이터와 특성이 같다면 처음 보는 새로운 데이터가 주어져도 정확히 예측할 거라 기대한다.

2. 모델이 처음 보는 데이터에 대해 정확하게 예측할 수 있으면 이를 훈련 세트에서 테스트 세트로 일반화(generalization) 되었다고 한다.

3. 그래서 모델을 만들 때는 가능한 한 정확하게 일반화 되도록 해야 한다.

4. 가진 정보를 모두 사용해서 너무 복잡한 모델을 만드는 것을 과대적합(overfition)이라고 한다.

5. 과대적합은 모델이 훈련 세트의 각 샘플에 너무 가깝게 맞춰져서 새로운 데이터에 일반화되기 어려울 때 일어난다.

6. 반대로 모델이 너무 간단하면, 즉 "집이 있는 사람은 모두 요트를 사려고 한다"와 같은 경우에는 데이터의 면면과 다양성을 잡아내지 못할 것이고 훈련 세트에도 잘 맞지 않으 ㄹ것이다.

7. 너무 간단한 모델이 선택되는 것을 과소적합(underfiting)이라고 한다.

8. 모델을 복잡하게 할수록 훈련 데이터에 대해서는 더 정확히 예측할 수 있다.

9. 그러나 너무 복잡해지면 훈련 세트의 각 데이터 포인트에 너무 민감해져 새로운 데이터에 잘 일반화되지 못한다.

10. 우리가 찾으려는 모델은 일반화 성능이 최대가 되는 최적점에 있는 모델이다.

# 모델 복잡도와 데이터셋 크기의 관계

1. 모델의 복잡도는 훈련 데이터셋에 담긴 입력 데이터의 다양성과 관련이 깊다.

2. 데이터셋에 다양한 데이터 포인트가 많을수록 과대적합 없이 더 복잡한 모델을 만들 수 있다.

3. 보통 데이터 포인트를 더 많이 모으는 것이 다양성을 키워주므로 큰 데이터셋은 더 복잡한 모델을 만들 수 있게 해준다.

# 예제에 사용할 데이터셋

1. 두 개의 특성을 가진 forge 데이터셋은 인위적으로 만든 이진 분류 데이터셋이다.

2. 다음 코드는 이 데이터셋의 모든 데이터 포인트를 산점도로 그린다.

3. x 축은 첫 번째 특성이고 y 축은 두 번째 특성이다.

```

# 데이터셋을 만든다
X, y = mgleran.datasets.make_forge()

# 산점도를 그린다
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
plt.legend(["클래스 0", "클래스 1"], loc=4)
pit.xlable("첫 번째 특성")
pit.ylabel("두 번째 특성")
print("X.shape:", X.shape)

X.shape : (26, 2)

```

4. X.shape 값에서 알 수 있듯이 이 데이터셋은 데이터 포인트 26개와 특성 2개를 가진다.

5. 회귀 알고리즘 설명에는 인위적으로 만든 wave 데이터셋을 사용한다.

6. wave 데이터셋은 입력 특성 하나와 모델링할 타깃 변수(또는 응답)를 가진다.

```

X, y = mglearn.datsets.make_wave(n_samples=40)
plt.plot(X, y, 'o')
plt.ylim(-3, 3)

```

7. 종이는 2차원이라 둘이 넘는 특성은 표현하기 어려우니 손쉽게 시각화하기 위해서 간단한 저차원 데이터셋을 사용해보자

8. 특성이 적은 데이터셋(저차원 데이터셋)에서 얻은 직관이 특성이 많은 데이터셋(고차원 데이터셋)에서 그대로 유지되지 않을 수 있다.

9. 하지만 이런 사실을 유념해둔다면 알고리즘을 배울 때 저차원 데이터셋을 사용하는 것이 매우 좋다.

10. 인위적인 소규모 데이터셋 외에 scikit-learn에 들어 있는 실제 데이터셋도 두 개를 사용해보자

11. 하나는 유방암 종양의 임상 데이터를 기록해놓은 위스콘신 유방암 데이터셋이다(줄여서 cancer)

12. 각 종양은 양성(benign, 해롭지 않은 종양)과 악성(malignant, 암 종양)으로 레이블되어 있고, 조직 데이터를 기반으로 종양이 악성인지를 예측할 수 있도록 학습하는 것이 과제다.

```

from sklearn.datasets import load_breast_cancer
import numpy as np
cancer = load_breast_cancer()
print("cacner.keys():\n", cancer.keys())

print("유방암 데이터의 형태:", cancer.data.shape)

print("클래스별 샘플 개수:\n",
     {n: v for n, v in zip(cancer.target_names, np.bincount(cancer.target))})

cacner.keys():
 dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])
유방암 데이터의 형태: (569, 30)
클래스별 샘플 개수:
 {'malignant': 212, 'benign': 357}

```

13. 이 데이터셋은 569개의 데이터 포인트를 가지고 있고 특성은 30개이다.

14. 569개 데이터 중 212개는 악성이고 357개는 양성이다.

15. feature_names 속성을 확인하면 각 특성의 의미를 알 수 있다.

16. 데이터에 관한 더 자세한 정보는 cancer.DESCR에서 확인할 수 있다.

17. 또 회귀 분석용 실제 데이터셋으로는 보스턴 주택가격 데이터셋을 사용해보자.

18. 이 데이터셋으로 할 작업은 범죄율, 찰스강 인접도, 고속도로 접근성 등의 정보를 이용해 1970년대 보스턴 주변의 주택 평균 가격을 예측하는 것이다.

19. 이 데이터셋에는 데이터 포인트 506개와 특성 13개가 있다.

```

from sklearn.datasets import load_boston
boston = load_boston()
print("데이터의 형태:", boston.data.shape)

데이터의 형태: (506, 13)

```

20. 이 데이터셋에서도 boston 객체의 DESCR 속성에서 더 자세한 정보를 확인할 수 있다.

21. 이 데이터셋에서는 13개의 입력 특성뿐 아니라 특성끼리 곱하여 (또는 상호작용이라 부름) 의도적으로 확장해보자.

22. 다시 말하면 범죄율과 고속도로 접근성의 개별 특성은 물론, 범죄율과 고속도로 접근성의 곱도 특성으로 생각한다는 뜻이다.

23. 이처럼 특성을 유도해내는 것을 특성 공학(feature engineering)이라고 하며 4장에서 자세히 다룬다.

24. 유도된 데이터셋은 load_extended_boston 함수를 사용하여 불러들일 수 있다.

```

X, y = mglearn.datasets.load_extended_boston()
print("X.shape:", X.shape)

X.shape: (506, 104)

13개의 원래 특성에 13개에서 2개씩 (중복을 포함해) 짝지은 91개의 특성을 더해 총 104개가 된다. 

(13 + 12 + 11 + ... + 1) = 91

```

# k-최근접 이웃

1. k-NN 알고리즘은 가장 간단한 머신러닝 알고리즘이다.

2. 훈련 데이터셋을 그냥 저장하는 것이 모델을 만드는 과정의 전부이다.

3. 새로운 데이터 포인트에 대해 예측할 땐 알고리즘이 훈련 데이터셋에서 가장 가까운 데이터 포인트, 즉 '최근접 이웃'을 찾는다.

4. forge 데이터셋에 대한 1-최근접 이웃 모델의 예측

```

mglearn.plots.plot_knn_classification(n_neighbors=1)

```

5. forge 데이터셋에 대한 3-최근접 이웃 모델의 예측

```

mglearn.plots.plot_knn_classification(n_neighbors=3)

```

6. 이제 scikit-learn을 사용해서 k-최근접 이웃 알고리즘을 어떻게 적용하는지 살펴보자

7. 먼저 1장에서 한 것처럼 일반화 성능을 평가할 수 있도록 데이터를 훈련 세트와 테스트 세트로 나눈다.

```

X, y = mglearn.datasets.make_forge()

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

clf = KNeighborsClassifier(n_neighbors=3) //n_neighbors 기본값은 5이다

clf.fit(X_train, y_train) // 훈련 세트를 사용하여 분류 모델을 학습 시킨다

print("테스트 세트 예측:", clf.predict(X_test)) // 테스트 데이터에 대해 predict 메서드를 호출해서 

print("테스트 세트 정확도: {:.2f}".format(clf.score(X_test, y_test)))



```

# KNeigborsClassifier 분석

1. 2차원 데이터셋이므로 가능한 모든 테스트 포인트의 예측을 xy 평면에 그려볼 수 있다.

2. 그리고 각 데이터 포인트가 속한 클래스에 따라 평면에 색을 칠한다.

3. 이렇게 하면 알고리즘이 클래스 0과 클래스 1로 지정한 영역으로 나뉘는 결정 경계(decision boundary)를 볼 수 있다.

4. 이웃을 적게 사용하면 모델의 복잡도가 높아지고 많이 사용하면 복잡도는 낮아진다.

```

fig, axes = plt.subplots(1, 3, figsize=(10, 3))

for n_neighbors, ax in zip([1, 3, 9], axes):
    clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X, y)
    mglearn.plots.plot_2d_separator(clf, X, fill=True, eps=0.5, ax=ax, alpha=.4)
    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)
    ax.set_title("{} 이웃".format(n_neighbors))
    ax.set_xlabel("특성 0")
    ax.set_ylabel("특성 1")

axes[0].legend(loc=3)

```


```

from sklearn.datasets import load_breast_cancer

cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(
cancer.data, cancer.target, stratify=cancer.target, random_state=66)

training_accuracy = []
test_accuracy = []

neighbors_settings = range(1, 11)

for n_neighbors in neighbors_settings:
    # 모델 생성
    
    clf = KNeighborsClassifier(n_neighbors=n_neighbors)
    clf.fit(X_train, y_train)
    
    # 훈련 세트 정확도 저장
    training_accuracy.append(clf.score(X_train, y_train))
    
    # 일반화 정확도 저장
    test_accuracy.append(clf.score(X_test, y_test))
    
plt.plot(neighbors_settings, training_accuracy, label="훈련 정확도")
plt.plot(neighbors_settings, test_accuracy, label = "테스트 정확도")
plt.ylabel("정확도")
plt.xlabel("n_neighbors")
plt.legend()

```

11. 이 그림은 n_neighbors 수(x 축)에 따른 훈련 세트와 테스트 세트 정확도(y 축)를 보여준다.

12. 실제 이런 그래프는 매끈하게 나오지 않지만, 여기서도 과대적합과 과소적합의 특징을 볼 수 있다.

5. 최근접 이웃의 수가 하나일 때는 훈련 데이터에 예측이 완벽하다.

6. 하지만 이웃의 수가 늘어나면 모델은 단순해지고 훈련 데이터의 정확도는 줄어든다.

7. 이웃을 하나 사용한 테스트 세트의 정확도는 이웃을 많이 사용했을 때보다 낮다.

8. 이것은 1-최근접 이웃이 모델을 너무 복잡하게 만든다는 것을 설명해준다.

9. 반대로 이웃을 10개 사용했을 때는 모델이 너무 단순해서 정확도는 더 나빠진다.

10. 정확도가 가장 좋을 때는 중간 정도인 여섯 개를 사용한 경우이다.

# k-최근접 이웃 회귀

1. k-최근접 이웃 알고리즘은 회귀 분석에도 쓰인다.

2. 이번에는 wave 데이터셋을 이용해서 이웃이 하나인 최근접 이웃을 사용해보자.

3. x 축에 세 개의 테스트 데이터를 흐린 별 모양으로 표시했다.

4. 최근접 이웃을 한 개만 이용할 때 예측은 그냥 가장 가까운 이웃의 타깃 값이다.

```

mglearn.plots.plot_knn_regression(n_neighbors=1)

```

5. 여러 개의 최근접 이웃을 사용할 땐 이웃 간의 평균이 예측이 된다.

```

mglearn.plots.plot_knn_regression(n_neighbors=3)

```

6. scikit-learn에서 회귀를 위한 k-최근접 이웃 알고리즘은 KNeighborsRegressor에 구현되어 있다.

7. 사용법은 KNeighborsClassifier와 비슷하다.

```

from sklearn.neighbors import KNeighborsRegressor

X, y = mglearn.datasets.make_wave(n_samples=40)

# wave 데이터셋을 훈련 세트와 테스트 세트로 나눈다.
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

# 이웃의 수를 3으로 하여 모델의 객체를 만든다.
reg = KNeighborsRegressor(n_neighbors=3)

# 훈련 데이터와 타깃을 사용하여 모델을 학습시킨다.
reg.fit(X_train, y_train)

print("테스트 세트 예측:\n", reg.predict(X_test))

테스트 세트 예측:
 [-0.05396539  0.35686046  1.13671923 -1.89415682 -1.13881398 -1.63113382
  0.35686046  0.91241374 -0.44680446 -1.13881398]

```

8. 역시 score 메서드를 사용해 모델을 평가할 수 있다.

9. 이 메서드는 회귀일 땐 R^2 값을 반환한다.

10. 결정 계수라고도 하는 R^2 값은 회귀 모델에서 예측의 적합도를 측정한 것으로 보통 0과 1사이의 값이 된다.

11. 1은 예측이 완벽한 경우이고, 0은 훈련 세트의 출력값인 y_train의 평균으로만 예측하는 모델의 경우이다.

12. R^2은 음수가 될 수도 있다.

13. 이 때는 예측과 타깃이 상반된 경향을 가지는 경우이다.

```

print("테스트 세트 R^2: {:.2f}".format(reg.score(X_test, y_test)))

테스트 세트 R^2: 0.83

```

14. 우리가 얻은 점수는 0.83이라 모델이 비교적 잘 들어맞은 것 같다.

# KNeighborsRegressor 분석

1. 이 1차원 데이터셋에 대해 가능한 모든 특성 값을 만들어 예측해볼 수 있다.

2. 이를 위해 x 축을 따라 많은 포인트를 생성해 테스트 데이터셋을 만든다.

```

fig, axes = plt.subplots(1, 3, figsize=(15, 4))
# -3과 3 사이에 1,000개의 데이터 포인트를 만든다.
line = np.linspace(-3, 3, 1000).reshape(-1, 1)

for n_neighbors, ax in zip([1, 3, 9], axes):
    # 1, 3, 9 이웃을 사용한 예측을 한다.
    reg = KNeighborsRegressor(n_neighbors = n_neighbors)
    reg.fit(X_train, y_train)
    ax.plot(line, reg.predict(line))
    ax.plot(X_train, y_train, '^', c=mglearn.cm2(0), markersize=8)
    ax.plot(X_test, y_test, 'v', c=mglearn.cm2(1), markersize=8)
    
    ax.set_title(
    "{} 이웃의 훈련 스코어: {:.2f} 테스트 스코어: {:.2f}".format(n_neighbors, reg.score(X_train, y_train), reg.score(X_test, y_test)))
    ax.set_xlabel("특성")
    ax.set_ylabel("타깃")
    
axes[0].legend(["모델 예측", "훈련 데이터/타깃", "테스트 데이터/타깃"], loc="best")

```

3. 이 그림에서 볼 수 있듯이 이웃을 하나만 사용할 때는 훈련 세트의 각 데이터 포인트가 예측에 주는 여향이 커서 예측값이 훈련 데이터 포인트를 모두 지나간다.

4. 이는 매우 불안정한 예측을 만들어 낸다.

5. 이웃을 많이 사용하면 훈련 데이터에는 잘 안 맞을 수 있지만 더 안정된 예측을 얻게 된다.

# 장단점과 매개변수

1. 일반적으로 KNeighbors 분류기에 중요한 매개변수는 두 개이다.

2. 데이터 포인트 사이의 거리를 재는 방법과 이웃의 수이다.

3. 실제로 이웃의 수는 3개나 5개 정도로 적을 때 잘 작동하지만, 이 매개변수는 잘 조정해야 한다.

4. 거리 재는 방법을 고르는 문제는 이 책에서 다루지 않지만, 기본적으로 여러 환경에서 잘 동작하는 유클리디안 거리 방식을 사용한다.

5. k-NN의 장점은 이해하기 매우 쉬운 모델이라는 점이다.

6. 그리고 많이 조정하지 않아도 자주 좋은 성능을 발휘한다.

7. k-최근접 이웃 앍리즘이 이해하긴 쉽지만, 예측이 느리고 많은 특성을 처리하는 능력이 부족해 현업에서는 잘 쓰지 않는다.

8. 이런 단점이 없는 알고리즘이 다음에 설명할 선형 모델이다.

# 선형 모델

1. 선형 모델(linear model)은 100여 년 전에 개발되었고, 지난 몇십 년 동안 폭넓게 연구되고 현재도 널리 쓰인다.

2. 곧 보겠지만 선형 모델은 입력 특성에 대한 선형 함수를 만들어 예측을 수행한다.

# 회귀의 선형 모델

1. 회귀의 경우 선형 모델을 위한 일반화된 예측 함수는 다음과 같다.

```

y꺽새?= w[0] * x[0] + w[1] * x[1] + .. + w[p] * x[p]  + b

```

2. 이 식에서 x[0]부터 x[p]까지는 하나의 데이터 포인트에 대한 특성을 나타내며 (특성의 개수는 p+1), w와 b는 모델이 학습할 파라미터이다.

3. 그리고 y^는 모델이 만들어낸 예측값이다.

4. 특성이 하나인 데이터셋이라면 이 식은 다음과 같아진다.

```

y^ = w[0] * x[0] + b

```

5. 수학 시간에 배운 직선의 방정식을 기억하는가? 이 식에서 w[0]은 기울기고 b는 y축과 만나는 절편이다.

6. 특성이 많아지면 w는 각 특성에 해당하는 기울기를 모두 가진다.

7. 회귀를 위한 선형 모델은 다양하다.

8. 이 모델들은 훈련 데이터로부터 모델 파라미터 w와 b를 학습하는 방법과 모델의 복잡도를 제어하는 방법에서 차이가 난다.

9. 우리는 회귀에서 가장 인기 있는 선형 모델들을 살펴보자.

# 선형 회귀(최소제곱법)

1. 선형 회귀(linear regression) 또는 최소제곱법(OLS, ordinary least squares)은 가장 간단하고 오래된 회귀용 선형 알고리즘이다.

2. 선형 회귀는 예측과 훈련 세트에 있는 타깃 y 사이의 평균제곱오차(mean squared error)를 최소화하는 파라미터 w와 b를 찾는다.

3. 평균제곱오차는 예측값과 타깃 값의 차이를 제곱하여 더한 후에 샘플의 개수로 나눈 것이다.

4. 선형 회귀는 매개변수가 없는 것이 장점이지만, 그래서 모델의 복잡도를 제어할 방법도 없다.

5. 다음은 [그림 2-11]의 선형 모델을 만드는 코드이다.

6. 기울기 파라미터 (w)는 가중치(weight) 또는 계수(coefficient)라고 하며 lr 객체의 coef_ 속성에 저장되어 있고 편향(offset) 또는 절편(intercept) 파라미터(b)는 intercept_ 속성에 저장되어 있다.

```

from sklearn.linear_model import LinearRegression
X, y = mglearn.datasets.make_wave(n_samples=60)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

lr = LinearRegression().fit(X_train, y_train)

print("lr.coef_", lr.coef_)
print("lr.intercept_:", lr.intercept_)

lr.coef_ [0.39390555]
lr.intercept_: -0.031804343026759746

coef_와 intercept_ 뒤의 밑줄이 이상하게 보일지 모르겠지만 scikit-learn은 훈련 데이터에서 유도된 속성은 항상 끝에 밑줄을 붙인다. (사용자가 지정한 매개변수와 구분하기 위해서)

```

7. intercept_ 속성은 항상 실수값 하나지만, coef_ 속성은 각 입력 특성에 하나씩 대응되는 NumPy 배열이다.

8. wave 데이터셋에는 입력 특성이 하나뿐이므로 lr.coef_도 원소를 하나만 가지고 있따.

9. 훈련 세트와 테스트 세트의 성능을 확인해보자

```

print("훈련 세트 점수:{:.2f}".format(lr.score(X_train, y_train)))

print("테스트 세트 점수:{:.2f}".format(lr.score(X_test, y_test)))

훈련 세트 점수:0.67

테스트 세트 점수:0.66

```

10. LinearRegression 모델이 보스턴 주택가격 데이터셋 같은 복잡한 데이터셋에서 어떻게 동작하는지 한번 살펴보자.

11. 이 데이터셋에는 샘플이 506개가 있고 특성은 유도된 것을 합쳐 104개이다.

12. 먼저 데이터셋을 읽어 들이고 훈련 세트와 테스트 세트로 나눈다.

13. 그런 다음 이전과 같은 방식으로 선형 모델을 만든다.

```

X, y = mglearn.datasets.load_extended_boston()

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

lr = LinearRegression().fit(X_train, y_train)

print("훈련 세트 점수:{:.2f}".format(lr.score(X_train, y_train)))
print("테스트 세트 점수:{:.2f}".format(lr.score(X_test, y_test)))

훈련 세트 점수:0.95
테스트 세트 점수:0.61

```

14. 훈련 세트와 테스트 세트의 점수를 비교해보면 훈련 세트에서는 예측이 매우 정확한 반면 테스트 세트에서는 R^2 값이 매우 낮다.

15. 훈련 데이터와 테스트 데이터 사이의 이런 성능 차이는 모델이 과대적합되었다는 확실한 신호이므로 복잡도를 제어할 수 있는 모델을 사용해야 한다.

16. 기본 선형 회귀 방식 대신 가장 널리 쓰이는 모델은 다음에 볼 리지 회귀이다.

# 리지 회귀

1. 리지(Ridge)도 회귀를 위한 선형 모델이므로 최소적합법에서 사용한 것과 같은 예측 함수를 사용한다.

2. 하지만 리지 회귀에서의 가중치(w) 선택은 훈련 데이터를 잘 예측하기 위해서 뿐만 아니라 추가 제약 조건을 만족시키기 위한 목적도 있다.

3. 가중치의 절댓값을 가능한 한 작게 만드는 것이다.

4. 다시 말해서 w의 모든 원소가 0에 가깝게 되길 원한다.

5. 직관적으로 생각하면 이는 모든 특성이 출력에 주는 영향을 최소한으로 만든다(기울기를 작게 만든다).

6. 이런 제약을 규제(regularization)라고 한다.

7. 규제란 과대적합이 되지 않도록 모델을 강제로 제한한다는 의미이다.

8. 리지 회귀에 사용하는 규제 방식을 L2 규제라고 한다.

9. 리지 회귀는 linear_model.Ridge에 구현되어 있다. 리지 회귀가 확정된 보스턴 주택 가격 데이터셋에 어떻게 적용되는지 살펴보자

```

from sklearn.linear_model import Ridge

ridge = Ridge().fit(X_train, y_train)
print("훈련 세트 점수: {:.2f}".format(ridge.score(X_train, y_train)))
print("테스트 세트 점수: {:.2f}".format(ridge.score(X_test, y_test)))

훈련 세트 점수: 0.89
테스트 세트 점수: 0.75

```

10. 결과를 보니 훈련 세트에서의 점수는 LinearRegression보다 낮지만 테스트 세트에 대한 점수는 더 높다. 기대한 대로다.

11. 선형 회귀는 이 데이터셋에 과대적합되지만 Ridge는 덜 자유로운 모델이기 때문에 과대적합이 적어진다.

12. 모델의 복잡도가 낮아지면 훈련 세트에서의 성능은 나빠지지만 더 일반화된 모델이 된다.

13. 관심 있는 것은 테스트 세트에 대한 성능이기 때문에 LinearRegression 보다 Ridge 모델을 선택해야 한다.

14. Ridge는 모델을 단순하게 (계수를 0에 가깝게) 해주고 훈련 세트에 대한 성능 사이를 절충할 수 있는 방법을 제공한다.

15. 사용자는 alpha 매개변수로 훈련 세트의 성능 대비 모델을 얼마나 단순화할지를 지정할 수 있다.

16. 앞의 예제에서는 매개변수의 기본값인 alpha=1.0을 사용했다.

17. 하지만 이 값이 최적이라고 생각할 이유는 없다.

18. 최적의 alpha 값은 사용하는 데이터셋에 달렸다.

19. alpha 값을 높이면 계수를 0에 더 가깝게 만들어서 훈련 세트의 성능은 나빠지지만 일반화에는 도움을 줄 수 있다.

20. 예를 들면 다음과 같다.

```

ridge10 = Ridge(alpha=10).fit(X_train, y_train)
print("훈련 세트 점수: {:.2f}".format(ridge10.score(X_train, y_train)))
print("테스트 세트 점수: {:.2f}".format(ridge10.score(X_test, y_test)))

훈련 세트 점수: 0.79
테스트 세트 점수: 0.64

```

21. alpha 값을 줄이면 계수에 대한 제약이 그만큼 풀리면서 [그림 2-1]의 오른쪽으로 이동하게 된다.

22. 아주 작은 alpha 값은 계수를 거의 제한하지 않으므로 LinearRegression으로 만든 모델과 거의 같아진다. (alpha=0.00001)

```

ridge01 = Ridge(alpha=0.1).fit(X_train, y_train)
print("훈련 세트 점수: {:.2f}".format(ridge01.score(X_train, y_train)))
print("테스트 세트 점수: {:.2f}".format(ridge01.score(X_test, y_test)))

훈련 세트 점수: 0.93
테스트 세트 점수: 0.77

```

23. 이 코드에서 alpha=0.1이 꽤 좋은 성능을 낸 것 같다.

24. 테스트 세트에 대한 성능이 높아질 때까지 alpha 값을 줄일 수 있을 것이다.

25. 여기서는 alpha 값이 [그림 2-1]의 모델 복잡도와 어떤 관련이 있는지 살펴보았다.

26. 또한 alpha 값에 따라 모델의 coef_ 속성이 어떻게 달라지는지를 조사해보면 alpha 매개변수가 모델을 어떻게 변경시키는지 더 깊게 이해할 수 있다.

27. 높은 alpha 값은 제약이 더 많은 모델이므로 작은 alpha 값일 때보다 coef_의 절댓값 크기가 작을 것이라고 예상할 수 있다.

28. [그림 2-12]를 보면 이러한 사실을 확인할 수 있다.

```

plt.plot(ridge10.coef_, '^', label="Ridge alpha=10")
plt.plot(ridge.coef_, 's', label="Ridge alpha=1")
plt.plot(ridge.coef_, 'v', label="Ridge alpha=0.1")

plt.plot(lr.coef_, 'o', label="LinearRegression")
plt.xlabel("계수 목록")
plt.ylabel("계수 크기")
xlims = plt.xlim()
plt.hlines(0, xlims[0], xlims[1])
plt.xlim(xlims)
plt.ylim(-25, 25)
plt.legend()

```

29. 선형 회귀와 몇 가지 alpha 값을 가진 리지 회귀의 계수 크기 비교

30. 이 그림에서 x 축은 coef_의 원소를 위치대로 나열한 것이다.

31. 즉 x=0은 첫 번째 특성에 연관된 계수이고 x=1은 두 번째 특성에 연관된 계수이다.

32. 이런식으로 x=100까지 계속된다.

33. y 축은 각 계수의 수치를 나타낸다.

34. alpha=10일 때 대부분의 계수는 -3과 3사이에 위치한다.

35. alpha=1일 때 Ridge 모델의 계수는 좀 더 커졌다.

36. alpha=0.1일 때 계수는 더 커지며 아무런 규제가 없는(alpha=0) 선형 회귀의 계수는 값이 더 커져 그림 밖으로 넘어간다.

37. 규제의 효과를 이해하는 또 다른 방법은 alpha 값을 고정하고 훈련 데이터의 크기를 변화시켜 보는 것이다.

38. [그림 2-13]은 보스턴 주택가격 데이터셋에서 여러 가지 크기로 샘플링하여 LinearRegression과 Ridge(alpha=1)을 적용한 것이다.

39. 데이터셋의 크기에 따른 모델의 성능 변화를 나타낸 그래프를 학습 곡선(learning curve)이라고 한다.

```

mglearn.plots.plot_ridge_n_samples()

```

40. 예상대로 모든 데이터셋에 대해 리지와 선형 회귀 모두 훈련 세트의 점수가 테스트 세트의 점수보다 높다.

41. 리지에는 규제가 적용되므로 리지의 훈련 데이터 점수가 전체적으로 선형 회귀의 훈련 데이터 점수보다 낮다.

42. 그러나 테스트 데이터에서는 리지의 점수가 더 높으며 특별히 작은 데이터셋에서는 더 그렇다.

43. 데이터셋 크기가 400 미만에서는 선형 회귀는 어떤 것도 학습하지 못하고 있다.

44. 두 모델의 성능은 데이터가 많아질수록 좋아지고 마지막에는 선형 회귀가 리지 회귀를 따라잡는다.

45. 여기서 배울 수 있는 것은 데이터를 충분히 주면 규제 항은 덜 중요해져서 리지 회귀와 선형 회귀의 성능이 같아질 것이라는 점이다.

46. [그림 2-13]에서 또 하나의 흥미로운 점은 선형 회귀의 훈련 데이터 성능이 감소한다는 것이다.

47. 이는 데이터가 많아질수록 모델이 데이터를 기억하거나 과대적합하기 어려워지기 때문이다.

# 라소

1. 선형 회귀에 규제를 적용하는 데 Ridge의 대안으로 Lasso가 있다.

2. 리지 회귀에서와 같이 라소(lasso)도 계수를 0에 가깝게 만들려고 한다.

3. 하지만 방식이 조금 다르며 이를 L1 규제라고 한다.

4. L1 규제의 결과로 라소를 사용할 때 어떤 계수는 정말 0이 된다.

5. 이 말은 모델에서 완전히 제외되는 특성이 생긴다는 뜻이다.

6. 어떻게 보면 특성 선택(feature selection)이 자동으로 이뤄진다고 볼 수 있다.

7. 일부 계수를 0으로 만들면 모델을 이해하기 쉬워지고 이 모델의 가장 중요한 특성이 무엇인지 드러내준다.

8. 확장된 보스턴 주택가격 데이터셋에 라소를 적용해보자

```

from sklearn.linear_model import Lasso

lasso = Lasso().fit(X_train, y_train)
print("훈련 세트 점수: {:.2f}".format(lasso.score(X_train, y_train)))
print("테스트 세트 점수: {:.2f}".format(lasso.score(X_test, y_test)))
print("사용할 특성의 개수:", np.sum(lasso.coef_ != 0))


훈련 세트 점수: 0.29
테스트 세트 점수: 0.21
사용할 특성의 개수: 4

```

9. 결과에서 볼 수 있듯이 Lasso는 훈련 세트와 테스트 세트 모두에서 결과가 좋지 않다.

10. 이는 과소적합이며 104개의 특성 중 4개만 사용한 것을 볼 수 있다.

11. Ridge와 마찬가지로 Lasso도 계수를 얼마나 강하게 0으로 보낼지를 조절하는 alpha 매개변수를 지원한다.

12. 앞에서는 기본값인 alpha=1.0을 사용했다.

13. 과소적합을 줄이기 위해서 alpha값을 줄여보자.

14. 이렇게 하려면 max_iter(반복 실행하는 최대 횟수)의 기본값을 늘려야 한다.

```

# max_iter 기본값을 증가시키지 않으면 max_iter 값을 늘리라는 경고가 발생한다.
lasso001 = Lasso(alpha=0.01, max_iter=50000).fit(X_train, y_train)
print("훈련 세트 점수: {:.2f}".format(lasso001.score(X_train, y_train)))
print("테스트 세트 점수: {:.2f}".format(lasso001.score(X_test, y_test)))
print("사용한 특성의 개수:", np.sum(lasso001.coef_ !=0))

훈련 세트 점수: 0.90
테스트 세트 점수: 0.77
사용한 특성의 개수: 33

```

15. alpha 값을 낮추면 모델의 복잡도는 증가하여 훈련 세트와 테스트 세트에서의 성능이 좋아진다.

16. 성능은 Ridge보다 조금 나은데 사용된 특성은 104개 중 33개뿐이어서, 아마도 모델을 분석하기가 조금 더 쉽다.

17. 그러나 alpha 값을 너무 낮추면 규제의 효과가 없어져 과대적합이 되므로 LinearRegression의 결과와 비슷해진다.

```

lasso00001 = Lasso(alpha=0.0001, max_iter=50000).fit(X_train, y_train)
print("훈련 세트 점수: {:.2f}".format(lasso00001.score(X_train, y_train)))
print("테스트 세트 점수: {:.2f}".format(lasso00001.score(X_test, y_test)))
print("사용한 특성의 개수:", np.sum(lasso00001.coef_ != 0))

훈련 세트 점수: 0.95
테스트 세트 점수: 0.64
사용한 특성의 개수: 96

```

18. [그림 2-12]와 비슷하게 alpha 값이 다른 모델들의 계수를 그래프로 그려보자

```

plt.plot(lasso.coef_, 's', label="Lasso alpha=1")
plt.plot(lasso001.coef_, '^', label = "Lasso alpha=0.01")
plt.plot(lasso00001.coef_, 'v', label = "Lasso alpha=0.0001")

plt.plot(ridge01.coef_, 'o', label = "Ridge alpha=0.1")
plt.legend(ncol=2, loc=(0, 1.05))
plt.ylim(-25, 25)

```

19. alpha=1일 때 (이미 알고 있듯) 계수 대부분이 0일 뿐만 아니라 나머지 계수들도 크기가 작다는 것을 알 수 있다.

20. alpha를 0.01로 줄이면 대부분의 특성이 0이 되는 분포를 얻게 된다.

21. alpha= 0.0001이 되면 계수 대부분이 0이 아니고 값도 커져 꽤 규제받지 않은 모델을 얻게 된다.

22. alpha=0.1인 Ridge 모델은 alpha=0.01인 라소 모델과 성능이 비슷하지만 Ridge를 사용하면 어떤 계수도 0이 되지 않는다.

23. 실제로 이 두 모델 중 보통은 리지 회귀를 선호한다.

24. 하지만 특성이 많고 그 중 일부만 중요하다면 Lasso가 더 좋은 선택이 될 수 있다.

25. 또한 분석하기 쉬운 모델을 원한다면 Lasso가 입력 특성 중 일부만 사용하므로 쉽게 해석할 수 있는 모델을 만들어줄 것이다.

26. scikit-leran은 Lasso와 Ridge의 페널티를 결합한 ElasticNet도 제공했다.

27. 실제로 이 조합은 최상의 성능을 내지만 L1 규제와 L2 규제를 위한 매개변수 두 개를 조정해야 한다.

# 분류용 선형 모델

1. 선형 모델은 분류에도 널리 사용한다.

2. 먼저 이진 분류(binary classification)를 살펴보자.

3. 이 경우 예측을 위한 방정식은 다음과 같다.

```

y^ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b > 0

```

4. 이 방정식은 선형 회귀와 아주 비슷하다.

5. 하지만 특성들의 가중치 합을 그냥 사용하는 대신 예측한 값을 임계치 0과 비교한다.

6. 함수에서 계산한 값이 0보다 작으면 클래스를 -1이라고 예측하고 0보다 크면 +1이라고 예측한다.

7. 이 규칙은 분류에 쓰이는 모든 선형모델에서 동일하다.

8. 여기에서도 계수(w)와 절편(b)을 찾기 위한 방법이 많이 있다.

9. 회귀용 선형 모델에서는 출력 y^이 특성의 선형 함수였다.

10. 즉 직선, 평면, 초평면(차원이 3 이상일 때) 이다.

11. 분류형 선형 모델에서는 결정 경계가 입력의 선형 함수다.

12. 다른 말로 하면 (이진) 선형 분류기는 선, 평면, 초평면을 사용해서 두 개의 클래스를 구분하는 분류기이다.

13. 선형 모델을 학습시키는 알고리즘은 다양한데, 다음의 두 방법으로 구분할 수 있다.

```

특정 계수와 절편의 조합이 훈련 데이터에 얼마나 잘 맞는지 측정하는 방법

사용할 수 있는 규제가 있는지, 있다면 어떤 방식인지

```

14. 알고리즘들은 훈련 세트를 잘 학습하는지 측정하는 방법이 각기 다르다.

15. 불행하게도 수학적이고 기술적인 이유로, 알고리즘들이 만드는 잘못된 분류의 수를 최소화하도록 w와 b를 조정하는 것은 불가능하다.

16. 많은 애플리케이션에서 앞 목록의 첫 번째 항목 (손실 항목(loss function)라 한다)에 대한 차이는 크게 중요하지 않다.

17. 가장 널리 알려진 두 개의 선형 분류 알고리즘은 linear_model.LogisticRegression에 구현된 로지스틱 회귀(logistic regression)와

18. svm.LinearSVC(SVC는 support vector classifier의 약자이다)에 구현된 선형 서포트 벡터 머신(support vector machine)이다. 

19. LogisticRegression은 이름에 "Regression (회귀)"이 들어가지만 회귀 알고리즘이 아니라 분류 알고리즘이므로 LinearRegression과 혼동하면 안 된다.

20. forge 데이터셋을 사용하여 LogisticRegression과 LinearSVC 모델을 만들고 이 선형 모델들이 만들어낸 결정 경계를 그림으로 나타내보자

```

from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC

X, y = mglearn.datasets.make_forge()

fig, axes = plt.subplots(1, 2, figsize=(10, 3))

for model, ax in zip([LinearSVC(max_iter=5000), LogisticRegression()], axes):
    clf = model.fit(X, y)
    mglearn.plots.plot_2d_separator(clf, X, fill=True, eps=0.5, ax=ax, alpha=.7)
    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)
    ax.set_title(clf.__class__.__name__)
    ax.set_xlabel("특성 0")
    ax.set_ylabel("특성 1")
axes[0].legend()

```

21. forge 데이터셋에 기본 매개변수를 사용해 만든 선형 SVM과 로지스틱 회귀 모델의 결정 경계

22. 이 그림은 이전처럼 forge 데이터셋의 첫 번째 특성을 x 축에 놓고 두 번째 특성을 y 축에 놓았다.

23. LinearSVC와 LogisticRegression으로 만든 결정 경계가 직선으로 표현되었고 위쪽은 클래스 1, 아래쪽은 클래스 0으로 나누고 있다.

24. 다르게 말하면 새로운 데이터가 이 직선 위쪽에 놓이면 클래스 1로 분류될 것이고 반대로 직선 아래쪽에 놓이면 클래스 0으로 분류될 것이다.

25. 이 두 모델은 비슷한 결정 경계를 만들었다.

26. 그리고 똑같이 포인트 두 개를 잘못 분류했다.

27. 회귀에서 본 Ridge와 마찬가지로 이 두 모델은 기본적으로 L2 규제를 사용한다.

28. LogisticRegression과 LinearSVC에서 규제의 강도를 결정하는 매개변수는 C이다.


# 요약 및 정리

1. 최근접 이웃 - 작은 데이터셋일 경우, 기본 모델로서 좋고 설명하기 쉬움

2. 선형 모델 - 첫 번째로 시도할 알고리즘. 대용량 데이터셋 가능. 고차원 데이터에 가능

3. 나이브 베이즈 - 분류만 가능. 선형 모델보다 훨씬 빠름. 대용량 데이터셋과 고차원 데이터에 가능. 선형 모델보다 덜 정확함.

4. 결정 트리 - 매우 빠름. 데이터 스케일 조정이 필요 없음. 시각화하기 좋고 설명하기 쉬움.

5. 랜덤 포레스트 - 결정 트리 하나보다 거의 항상 좋은 성능을 냄. 매우 안정적이고 강력함. 데이터 스케일 조정 필요 없음. 고차원 희소 데이터에는 잘 안맞음.

6. 그레이디언트 부스팅 결정 트리 - 랜덤 포레스트보다 조금 더 성능이 좋음. 랜덤 포레스트보다 학습은 느리나 예측은 빠르고 메모리를 조금 사용. 랜덤 포레스트보다 매개변수 튜닝이 많이 필요함.

7. 서포트 벡터 머신 - 비슷한 의미의 특성으로 이뤄진 중간 규모 데이터셋에 잘 맞음. 데이터 스케일 조정 필요. 매개변수에 민감.

8. 신경망 - 특별히 대용량 데이터셋에서 매우 복잡한 모델을 만들 수 있음. 매개변수 선택과 데이터 스케일에 민감. 큰 모델은 학습이 오래 걸림

# 비지도 학습

1. 비디조 학습이란 알고 있는 출력값이나 정보 없이 학습 알고리즘을 가르쳐야 하는 모든 종류의 머신러닝을 가리킨다.

2. 비지도 학습에서 학습 알고리즘은 입력 데이터만으로 데이터에서 지식을 추출할 수 있어야 한다.

# 비지도 학습의 종류

1. 이번 장에서는 두 가지 비지도 학습을 살펴본다.

2. 바로 데이터의 비지도 변환(unsupervised transformation)과 군집(clustering)이다.

3. 비지도 변환은 데이터를 새롭게 표현하여 사람이나 다른 머신러닝 알고리즘이 원래 데이터보다 쉽게 해석할 수 있도록 만드는 알고리즘이다.

4. 비지도 변환이 널리 사용되는 분야는 특성이 많은 고차원 데이터를 특성의 수를 줄이면서 꼭 필요한 특징을 포함한 데이터로 표현하는 방법인 차원 축소(dimensionally reduction)이다.

5. 차원 축소의 대표적 예는 시각화를 위해 데이터셋을 2차원으로 변경하는 경우이다.

6. 군집 알고리즘은 데이터를 비슷한 것끼리 그룹으로 묶는 것이다.

7. 소셜 미디어 사이트에 사진을 업로드하는 예를 생각해보자

8. 업로드한 사진을 분류하려면 같은 사람이 찍힌 사진을 같은 그룹으로 묶을 수 있다.

9. 그러나 사이트는 사진에 찍힌 사람이 누군지, 전체 사진 앨범에 얼마나 많은 사람이 있는지 알지 못한다.

10. 가능한 방법은 사진에 나타난 모든 얼굴을 추출해서 비슷한 얼굴로 그룹 짓는 것이다.

11. 이 얼굴들이 같은 사람의 얼굴이라면 이미지들을 그룹으로 잘 묶은 것이다.

# 데이터 표현과 특성 공학

1. 지금까지 우리는 데이터가 2차원 실수형 배열로 각 열이 데이터 포인트를 설명하는 연속형 특성(continuous feature)이라고 가정했다.

2. 하지만 많은 애플리케이션에서 이렇게 데이터가 수집되지는 않는다.

3. 일반적인 특성의 전형적인 형태는 범주형 특성(categorical feature)이다.

4. 또는 이산형 특성(discrete feature)이라고도 하는 이런 특성은 보통 숫자 값이 아니다.

5. 범주형 특성과 연속적인 특성 사이의 차이는 분류와 회귀의 차이와 비슷하지만, 출력이 아닌 입력에 대한 것이란 점이 다르다.

6. 특정 애플리케이션에 가장 적합한 데이터 표현을 찾는 것을 특성 공학(feature engineering)이라 하며, 데이터 과학자와 머신러닝 기술자가 실제 문제를 풀기 위해 당면하는 주요 작업 중 하나이다.

7. 올바른 데이터 표현은 지도 학습 모델에서 적절한 매개변수를 선택하는 것보다 성능에 더 큰 영향을 미친다.

