# 가설 검정

1. 가설 검정(Hypothesis Test) - 표본 통계량을 이용하여 모수에 대한 주장의 진위를 검정하는 과정

2. 귀무가설(Null Hypothesis) - 모집단의 모수에 대하여 어떤 조건을 가정하여 가설을 설정

- 귀무가설이란 거짓이 명확히 규명될 때까지 참인 것으로 인정되는 모수에 대한 주장, 즉 그 타당성을 입증해야 할 가설을 의미한다.

- 귀무가설은 일반적으로 H0로 나타나며 H0 가설을 기각할 수 없다는 것은 아무런 차이가 없다 또는 전혀 효과가 없다는 의미

3. 대립가설(Alternative Hypothesis) - 모수에 차이가 있다는 것을 나타내며, 효과가 있다는 것을 검증하기 위한 것

- 귀무가설과 반대되는 가설로서 H1으로 표기

- 즉, 대립가설은 귀무가설을 부정하는 가설로서 귀무가설이 거짓이라면 참이 되는 가설을 의미한다.

- 일반적으로 귀무가설은 등호(=)를 사용하고 대립가설에는 등호를 사용하지 않는다.

4. 가설 검정의 오차 - 가설 검정의 오류는 표본에서의 통계량을 이용해서 모수치를 추정하면 거의 오차가 발생하기 때문에 이 오차를 가설 검정의 오차라 한다.

5. 오류 - 다음과 같이 제1종 오류와 제2종 오류로 나눌 수 있다.

|구분|귀무가설(참)|귀무가설(거짓)|
|-|-|-|
|귀무가설(채택)|옳은 결정|제2종 오류|
|귀무가설(기각)|제1종 오류|옳은 결정|

6. 제1종 오류 - 귀무가설이 참인데 참인 귀무가설을 기각하면서 생기는 오류를 의미하며 제1종 오류를 발생시킬 확률은 a로 나타낸다.

7. 제2종 오류 - 대립가설이 참인데 참인 대립가설을 기각하면서 생기는 오류는, 제2종 오류라고 하고 일반적으로 b로 나타낸다.

8. 기각과 채택

- 가설을 기각 혹은 채택하는 기준은 유의수즌(a, 제1종 오류를 범할 확률)이고 귀무가설이 기각된 경우 '유의하다'라는 결론을 내린다.

- 세워진 귀무가설을 기각하였을 때 유의수준 몇 %에서 기각했는지를 보고 가설이 유의한지 그렇지 않은지를 결정할 수 있다.

9. 검정통계량

- 모집단의 부분집합인 표본으로부터 검정에 대한 결론을 내리고 H0를 기각하거나 H0를 기각하지 않고 유지하는 결정을 내리는 데 활용되는 표본의 함수

- 예를 들어서 '예전에 혈압 환자들은 기본적으로 평균 혈압이 150mmHg이었는데 이번에 새로 개발된 약을 한 달간 복용하고 나면 혈압이 150mmHg 미만이 될 것이다'

- 라는 것을 검정하려면 X- <= c일 때 임의의 c에 대해서 귀무가설 H0를 기각하게 된다.

- X-가 취하는 범위 중에서 H0 가설을 기각하는 영역을 기각역이라 한다.

- 즉 이 경우, 기각역 R은 X- <= c로 나타낼 수 있으며 표본평균 X-가 c이하이면 H0을 기각하게 된다.

- 표본을 이용해서 모수를 추정하기 때문에 오류를 범할 가능성이 존재하는데 이러한 가능성을 확률로 표현하면 제1종 오류를 범하게 될 확률은 a로 표기하고,

- 제2종 오류를 범하게 되는 확률은 일반적으로 b로 표기한다.

- 제1종 오류와 제2종 오류의 관계에 있어 a를 줄이려고 하면 b가 커지고, b를 줄이려고 하면 a가 커지게 된다.

10. 기각역

- H0가 참이라고 하면 (u=150일 때), a = P(x-<=c)

- H1가 참이라고 하면 (u<150일 때), b = P(X->c)

- 제1종 오류와 제2종 오류의 관계에 있어 a, b를 동시에 줄일 수 없기 때문에 제1종 오류 a는 0.01, 0.05, 또는 0.1 등 작은 값을 갖도록 상한선을 두고, 제2종 오류 b를 작게 해주는 기각역을 선택한다.

- 예를 들어서 표본의 크기가 클 때 모평균 u에 대한 검정통계량은 아래와 같이 구할 수 있다.

```

가설 검정

표본의 크기가 클 때 귀무가설 H0 : u = u0를 검정하기 위한 검정통계량은 다음 수식과 같다.

Z = X- - u / s / 루트n ~ N(0, 1)

검정통계량의 분포는 표준정규 분포를 따르며 각 가설에 대해서 기각역은 다음과 같다.

1. H1 : u > u0일 때 R : Z>za

2. H1 : u < u0일 때 R : Z<=-za

3. H1 : u != u0일 때 R : |Z|>=-z(a/2)

위의 수식에서 1,2의 경우는 단측 검정

3은 양측 검정이라 한다.

```

11. 유의수준과 유의확률

- 귀무가설이 참인데 참인 귀무가설을 기각하면서 생기는 오류는 제1종 오류로서 a로 나타내고 유의수준이라 한다.

- 주어진 통계량에서 귀무가설 H0를 기각할 수 있는 최소의 유의수준을 유의확률이라 하고 P값(P-value)이라 한다.

- 일반적으로 P 값이 주어진 경우 기각역은 다음과 같이 정의할 수 있다.

- P 값 < a 이 수식을 만족하면 귀무가설을 기각할 수 있다.

12. 가설 검정 절차

- 가설설정 - 데이터 분석을 위해서 모집단에 대한 통계적 가설을 설정한다.

- 표본 추출 - 모집단으로부터 표본을 추출한다.

- 가설의 진위 파악 

- 추출된 표본의 정보로 통계적 가설의 진위 여부를 파악한다.

- 표본의 크기가 고정되어져 있는 경우 제1종 오류의 확률이 커지면 제2종 오류의 확률이 작아지고, 제1종 오류의 확률이 작아지면 제2종 오류 확률이 커진다고 할 수 있다.

- 통계적가설 검정을 위해서는 귀무가설을 기각할 수 있는 최소의 유의확률 p를 기반으로 가설의 기각과 채택 여부를 결정할 수 있는데 유의확률 p 유의수준 a보다 작으면 귀무가설을 기각한다.

- 유의확률 p가 유의수준 a보다 크면 귀무가설을 기각할 통계적 근거가 없어서 귀무가설을 기각할 수 없다.

```

p <= a H0 기각
p > a H0 채택

```

## 분석 모형 설계

# 분석 절차 수립

# 분석 모형 선정

1. 데이터 유형 파악 - 데이터 분석모형을 선정하기 전에 먼저 데이터의 유형(정형, 반정형, 비정형)을 파악하여야 한다.

2. 그리고 분석하고자 하는 데이터가 독립변수, 종속변수인지 그리고 연속형인지, 범주형인지를 파악하고 분석모형을 선정한다.

3. 데이터 속성 파악 - 연구하고자 하는 목적과 데이터의 속성에 맞게 분석하고자 하는 모형을 선택해야 한다.

```

독립변수 - 연속형, 종속변수 - 연속형 : 회귀 분석, 상관 분석, 인공신경망 분석

독립변수 - 연속형, 종속변주 - 범주형 : 로지스틱 회귀 분석, 판별 분석

독립변수 - 범주형, 종속변주 - 연속형 : 회귀 분석, 인공신경망 분석

독립변수 - 범주형, 종속변주 - 범주형 : 로지스틱 회귀 분석, 분류트리기법


```

4. 통계 모형 분석

- 통계 분석은 객관적인 근거에 기반을 두고 데이터를 수집, 처리, 분류, 분석하여 의사결정을 하는 과정을 의미한다.

- 일반적으로 통계 분석에는 분석목적에 부합하는 통계기법을 선택해야 한다.

- 통계 분석을 하기 위해서는 통계패키지(SPSS, SAS) 및 프로그래밍 언어(R, Python 등)를 이용하여 다양한 통계적 방법을 통해 데이터를 처리-분석한다.

- 독립변수만을 가지고 통계 분석하는 경우와 독립변수, 종속변수를 이용해서 통계 분석하는 경우를 고려해서 통계 분석을 위한 모델을 정하고 데이터의 형태에 따라서 통계 분석을 실시한다.

- 통계적 기법을 활용해서 데이터를 분석하고 분석된 자료를 기반으로 추정과 예측을 하는 기법으로서 기술통계 분석, 상관관계 분석, 회귀 분석, 분산 분석, 주성분 분석 등의 방법을 이용한다.

5. 데이터 마이닝 기법

- 데이터 마이닝은 통계 및 수학적 기술들을 활용해서 빅데이터를 분석하고 새로운 상관관계나 추세를 파악하여 예측과 분류 등을 찾아내는 과정이라 할 수 있다.

- 데이터 마이닝은 기계학습이나 패턴인식과 관련한 빅데이터 분석에서 가장 기본적인 통계 기반 분석 기술이라 할 수 있다.

- 데이터 마이닝 분석을 위해 R, Python, 웨카(Weka) 등을 이용한다.

- 특히, 통계 기반 기법에서는 모집단에서 표본을 선출하고 표본을 기반으로 해서 가설 검정을 실시한다.

6. 빅데이터 분석

- 빅데이터 분석은 수집된 데이터로 특정 변수의 미래값을 예측한다는 것에 가장 큰 의미가 있다.

- 예를 들어 통계적인 방법론을 적용하여 인자에 대한 값을 추정해서 모델을 도출한다.

- 여기에서 모델에 대한 유의성 검정을 위해서 연구자가 주장하고 싶은 내용을 중심으로 귀무가설과 대립가설을 세우고 세워진 가설을 검정한다.

# 분석모형 정의

1. 분석모형 정의하기

- 빅데이터 분석을 위해서는 빅데이터 분석 목적에 부합하는 모형(모델)을 선택해야 하고 빅데이터 모형을 구축하기 위한 적합한 데이터를 선정하는 것이 중요하다.

- 그리고 선정된 데이터의 품질이 중요하다.

- 빅데이터 분석을 위해서 가용한 표본 데이터는 분석모델의 모수를 추정하기 위한 부분과 추정된 모수의 통계적 유의성 검증을 위한 부분으로 나눌 수 있다.

2. 데이터세트

- 일반적으로 데이터세트는 훈련 데이터세트와 검증 데이터세트나 평가 검증 데이터세트로 나눌 수 있다.

- 이와 같이 훈련 데이터(Traning), 검증 데이터(Validation), 평가 검증 데이터(Testing)는 일정한 비율에 의해서 무작위로 나누는 것이 일반적이다.

- 훈련 데이터세트는 가장 큰 데이터세트로서 빅데이터 통계 모형을 구축하기 위해서 사용된다.

- 검증 데이터세트는 훈련 데이터세트의 성과를 검증하기 위해서 사용되는 데이터세트라고 할 수 있다.

- 평가 검증 데이터세트는 새로운 데이터를 가지고 선택된 모형의 성능을 평가할 수 있도록 사용되는 데이터세트라고 할 수 있다.

- 훈련 데이터세트를 통해서 새로운 데이터세트를 적용하면 오차가 커지는 경우가 발생하는데 이것을 과적합 또는 미적합이라고 한다.

- 적합한 분석모형의 선택, 적용, 성능 및 적합성 평가, 안정성 평가를 하는 과정은 빅데이터 분석 프로세스의 핵심 단계라고 할 수 있다.

# 분석모형 구축 절차

1. 빅데이터 분석에 있어서 가장 중요한 것은 첫째로 데이터, 둘째로 분석을 수행할 수 있는 분석 모형(모델), 셋째로 데이터 분석을 수행하는 데이터 분석가라고 할 수 있다.

2. 이러한 세 가지 요소가 잘 구성되어야 가치 있는 결과를 창출할 수 있다.

3. 사전 업무

- 계획수립 - 빅데이터 분석모형 구축을 위한 계획(기획)에서 어떠한 목표를 달성하기 위해서 어떠한 데이터를 가지고 어떤 방식으로 수행할지에 대한 계획이 수립되어야 한다.

- 요건정의 - 정확한 요구사항 분석을 통해서 빅데이터 분석모형이 구축되어야 한다, 분석요건을 도출해서 분석과정을 설계하고 과제의 수행방안에 대한 설계와 요건 확정이 이루어져야 한다.

4. 빅데이터 분석 기획

- 빅데이터 분석을 위해서는 분석대상과 분석방법을 명확히 설정해서 프로젝트를 진행해야 한다.

- 빅데이터 분석 기획은 '문제발굴 - 문제정의 - 해결대안 설계 - 타당성 검토 - 과제선택'의 절차로 이루어지며

- 일반적으로 '요건정의 - 모델링 - 검증 및 테스트 - 적용'의 프로세스로 수행된다.

- 빅데이터 분석을 위한 방법은 네 가지로 나눌 수 있다

```

분석대상(WHAT) 알고, 분석방법(HOW) 알때 - 최적화(Optimization) : 목적함수 값을 최대화, 최소화하는 것을 목표로 하는 방법으로 제약조건 하에서 목푯값을 개선하는 방법. 목적함수와 제약조건을 정의하고 문제를 해결하는 것

분석대상(WHAT) 알고, 분석방법(HOW) 모를때 - 솔루션(Solution)

분석대상(WHAT) 모르고, 분석방법(HOW) 알때 - 통찰(Insight) : 기존 분석 방식을 통해서 새로운 지식과 통찰을 찾아낸다.

분석대상(WHAT) 모르고, 분석방법(HOW) 모를때 - 발견(Discovery) : 새로운 분석 대상 자체를 도출할 수 있다, 목표시점별 분석을 위한 기획 방안은 당면한 분석 주제의 해결을 위한 과제단위와 지속적 분석 문화 내재화를 위한 마스터플랜 단위로 나누어 볼 수 있다.

당면한 분석 주제 해결을 위한 과제단위의 1차 목표는 과제를 빠르게 해결하는 데 있으며, 과제의 유형은 Quick_Win 방식이고 접근방식은 Problem Solving이다.

마스터플랜 단위는 정확도에 1차 목표가 있으며, 과제 유형은 Long Term View에 있고 접근방식은 Problem Definition에 있다.

```

5. 분석 기획 시 고려사항

- 데이터 유형에 따라 적용 가능한 솔루션 및 분석 방법이 다르기 때문에 가용한 데이터가 있어야 한다.

- 유사한 분석 솔루션을 활용할 수 있도록 유스케이스가 필요하고 여러 가지 환경적 요인에 의해서 늘어날 수 있는 기간이나 투입 리소스를 고려해서 장애 요소들에 대한 사전 대응 방안을 수립해야 한다.

6. 분석모형 구축 시 고려사항

- 요구사항 분석이 정확하게 이루어져야 한다.

- 프로젝트를 진행함에 있어 가장 중요한 것은 요구사항 도출이므로 빅데이터 분석에 있어서도 가장 중요한 것은 정확한 요구사항 도출이라고 할 수 있다.

- 분석데이터를 준비해서 빅데이터 분석모형을 구축하여야 한다.

- 빅데이터 분석을 위해서 먼저 분석하고자 하는 데이터를 준비하고 준비된 데이터를 통해서 모형을 선정해야 한다.

- 분석모형에 대한 검증과 테스트를 진행한다. 선정된 모형은 모형의 적합성을 검토하기 위해서 검증 및 테스트를 진행해야 한다.

- 검증된 모형에 대해서 적용을 하게 된다. 모형이 검증되고 나면 실증 데이터를 적용하여 본다.

7. 데이터 분석모형 구축 절차 

- 요건정의 단계 - 빅데이터 분석을 위한 첫 번째 단계로 분석요건을 구체적으로 도출, 선별, 결정하는 단계라고 할 수 있다.

```

분석요건 도출단계 - 이 단계에서는 일반적으로 현업과 IT를 활용한 문제해결 방안 사이에 상호 이해하고 있는 업무 내용이 전혀 다를 위험이 있으므로 정확한 검토를 통해서 요건에 대한 파악이 이루어져야 한다.

수행방안 설계 단계 - 이 단계에서는 첫 번째 단계에서 수행했던 분석요건을 파악해서 문제를 해결할 수 있는 방법을 결정해야 하는데 빅데이터의 특성에 맞게 방법론을 수행해야 한다.

요건확정 단계 - 요건이 정확히 정리되면 이해관계자들의 동의를 구해서, 요건을 확정하는 단계이다.

```

- 모델링 단계 - 문제 해결을 위한 기법을 모델에 적용하는 단계라고 할 수 있는데 최적화 모델링이란 변수와 계수 값을 이용하여 목적함수와 제약조건을 정의하는 것을 의미한다.

- 모델링을 하기 위해서는 다음의 과정을 거친다.

```

모델링 마트 설계 및 구축 단계 - 요건정의 단계에서부터 데이터 소스를 파악하고 수집하여 모델링 기법을 위한 데이터를 준비하고 필요 데이터의 마트를 설계하는 단계라고 할 수 있다.

탐색적 데이터 분석 및 유의변수 도출 단계 - 업무에 대한 이해 및 분석요건에 대한 구체적인 사실을 발견하고 탐색적 데이터 분석 과정을 통해서 유의변수를 도출하는 단계이다.

모델링 단계 - 필요한 도구를 준비해서 수행하며 구현이 가능한 데이터 분석 모델링을 구축하는 단계라고 할 수 있다. 예를 들어 미래값을 예측하기 위한 데이터 분석 모델링 방법은 아래와 같다.

- 프로세스적인 측면이 없으면 데이터 마이닝 모델 기법을 적용한다.

- 프로세스 및 자원에 제약이 있고 입력값이 확률 분포를 가지게 되면 시뮬레이션 기법을 적용한다.

- 상황을 컴퓨터상에 모델로 재현해서 현상을 보다 잘 이해하도록 하고 미래의 변화에 따른 결과를 예측하기 위한 것을 시뮬레이션이라고 한다.

- 프로세스 및 자원에 제약이 있고 입력값이 상수값을 가질 때는 최적화 기법을 적용한다.

모델링 성능평가 단계 - 분석기법별로 성능평가의 기준은 다르다.

- 데이터 마이닝 성능 평가 항목 - 정확도, 정밀도, 디텍트 레이트, 리프트

- 시뮬레이션 성능평가 항목 - 처리율, 평균지연시간, 평균대기행렬길이, 시스템 작동 시간 

- 최적화의 성능평가 항목 - 목적 함수 값의 최적화 전의 값과 최적화 후의 값

```

- 검증 및 테스트 단계 - 이 단계에서는 모델링 성과를 확인하고 목표 성과값과 비교해서 성능평가를 수행한다.

- 분석용 데이터를 트레이닝용과 테스트용으로 나누어서 자체 검증을 한다.

- 본 단계에서는 테스트의 절차를 설계하고, 테스트를 수행하여 결과를 분석하고 분석 결과에 대해서 모형을 조정한다. 이러한 과정을 반복 시행해서 최적의 모형을 찾아낸다.

- 적용단계 - 적용단계에서는 실제 운영시스템에 실제 업무를 적용해서 성과를 확인하는 작업을 수행한다.

- 정확한 성과 측정을 위해서는 블라인드 테스트를 추전한다. 적용단계에서는 실제 업무환경에서 테스트를 진행해 봐야 하고 적용 후 유지보수적인 측면에서 계속적으로 관심이 필요하다.

- 적용단계에서는 주기적 리모델링이 필요한데 일반적으로 데이터 마이닝의 경우 분기별로 리모델링을, 시뮬레이션의 경우 반기별로 리모델링을, 최적화의 경우 연간으로 리모델링을 시행한다.

- 리모델링을 데이터 마이닝의 경우 재학습과 변수 추가 학습 방법으로 재조정을 시행하고, 시뮬레이션의 경우는 주요 시스템 원칙의 변경이나 발생 이벤트의 증가 시 평가와 재조정을 시행한다.

- 최적화의 경우는 조건의 변화나 가중치의 변화 시 재조정을 시행한다.

- 주기적 검토 - 한 번 만든 모형은 지속적으로 동일한 성과를 낼 수 없으므로 성과 모니터링을 주기적으로 수행하여 리모델링할 수 있도록 하여야 한다.

8. 데이터 분석 시 주요 고려사항

- 데이터 분석을 위해서는 데이터 처리, 시각화, 공간 분석, 탐색적 자료 분석, 통계 분석, 데이터 마이닝, 시뮬레이션, 최적화, WBS 수립, 결측치 및 이상치 처리 등을 고려한다

- 데이터 처리 단계 - 데이터 분석은 통계를 기반으로 하여 이루어진다고 볼 수 있으며, 데이터 처리를 위해서 최종 데이터 구조로 가공하게 되는데

- 예를 들어 시뮬레이션은 단계별 처리 시간에 대한 분포를 파악할 수 있는 속성을 만들고

- 최적화는 제약값에 대한 내용, 목적함수, 제약조건의 계수값을 프로세스별로 산출하고

- 데이터 마이닝은 분류값과 입력변수들의 연관관계를 파악한다.

- 시각화 - 시각화는 빅데이터 분석에 있어서 필수적인 단계라고 할 수 있으며 탐색적 데이터 분석 시에도 시각화는 필수적이라고 할 수 있다. 여러 차트나 그래프로 데이터를 표현해서 가시적 효과를 높일 수 있다.

- 공간 분석 - 공간적 차원과 관련된 속성들을 시각화하고 예를 들어 지도위에 관련 속성들을 생성하는 것을 공간 분석이라 한다.

- 탐색적 자료 분석 - 탐색적 자료 분석은 다양한 차원의 값을 조합하여 특이점을 찾아내고 데이터의 특징이나 내재하는 구조적 관계를 알아내기 위한 기법들을 통칭하는 것으로 탐새적 자료 분석의 4가지 주제는 다음과 같다.

```

데이터의 저항성(Resistance)을 강조한다. 즉 탐색적 자료 분석에서는 일부자료의 파손에 관한 저항성을 가져야 한다.

예를 들어 평균보다는 일부 자료의 파손에 저항적인 중위수가 바람직한 대푯값으로 선호된다.

잔차(Residual)를 계산한다. 잔차는 개별 관측값이 자료의 주경향으로부터 얼마나 벗어났는지를 나타낸다.

자료변수를 재표현(Re-expression)한다. 자료 분석을 단순화할 수 있도록 원래 변수를 다른 척도로 바꾸는 것을 의미하며 로그변환, 제곱근 변환 등을 이용한다.

그래프를 통해서 현시성(Revelation)을 높인다. 그래픽 표현이 데이터 내에 숨겨진 정보를 파악할 수 있는 효율적인 수단이 된다.

```

- 통계 분석 - 어떤 현상을 종합적으로 한눈에 알아보기 쉽게 숫자나 표, 그림으로 나타내는 것이다. 통계 분석은 기술통계와 추론통계로 나뉜다.

- 데이터 마이닝 - 대용량의 자료에 대해서 데이터 속에 숨겨져 있는 유용한 연관관계를 파악하고 이것을 이용해서 미래에 실행 가능한 정보를 추출하며 의사결정에 이용하는 과정을 의미한다.

- WBS(Work Breakdown Structure) 수립 - 데이터 분석 흐름에 맞추어서 일정에 대한 계획을 세워야 하는데 여기에서 데이터 흐름은 다음과 같다.

```

데이터 분석과제 정의의 일정을 세우는 단계 - 일정을 계획하는 단계로 프로젝트 전체 일정에 맞는 준비를 하는 단계라 할 수 있다.

프로젝트 참여 인원에 대한 관리와 하드웨어, 소프트웨어 등에 대한 것을 고려하여 프로젝트의 전체 일정을 정해야 한다.

데이터 준비 및 탐색의 일정을 계획하는 단계 - 엔지니어가 필요한 데이터 수집에 관련된 일정을 정리하고 데이터 분석가는 최종적으로 도출하는 과정이 포함된 일정을 계획하고 수립해야 한다.

데이터 분석 모델링과 검증단계의 일정 계획을 세우는 단계 - 실험방법이나 절차를 기획하고 검증하는 등의 자세한 일정을 세워야 한다. 모델링은 비교 평가가 가능하도록 해야 한다.

산출물 및 기타 일정을 계획하는 단계 - 프로젝트 단계별로 산출물을 구별해서 필수 산출물을 정의하고 단계별 산출물 및 최종산출물을 정리하는 단계가 일정에 포함되어야 한다.

```

- 결측치 처리 - 데이터 분석에서는 결측치(Missing Data)에 대한 처리가 필요하며, 결측치를 어떻게 처리할 것인지에 대한 계획이 세워져야 한다.

- 결측치를 대푯값으로 대체하는 경우는 예측력을 높여주지만 정확성이 낮아질 수 있다.

- 데이터가 적어서 결측치를 사용해야 하는 경우는 대푯값으로 대체해서 사용하지만 대체적으로 결측치로 정의하여 처리하는 경우가 많다.

- 이상치 처리 - 이상치 발견(Outlier Detection)은 데이터 분석에 있어 데이터 전처리를 어떻게 할지를 결정하는데 사용하며, 이상 감지(Fraud Detection)에서 규칙을 발견하기 위해서 사용된다.

9. 데이터마트 개발

- 데이터 분석을 위해서 데이터 가공을 SQL 등의 데이터 처리 프로그램을 이용해서 처리해야 한다.

- 일반적으로 R을 이용하게 되는데 R에서의 데이터마트 구축 패키지로는 reshpae, sqldf, plyr, data.table 등을 들 수 있다.

- 데이터마트란 작은 규모의 데이터웨어하우스로 소규무로 분할하여 구축하고 이용하는 것이 더 효율적일 수 있어 주로 고객관리, 상품관리, 재무회계관리 등의 단일 주제별로 의사결정 그룹을 구축하는 것을 의미한다.

- 변수 - 요약변수 : 기본 정보를 Aggregation한 변수로 세분화나 행동 예측이 가능하다. 

- 파생변수 - 특정의미를 갖는 작위적 의미의 변수를 의미한다.

- reshape - melt()와 cast()를 이용해서 데이터를 재구성하거나 재정렬하기 위한 기법으로 밀집화된 데이터를 유연하게 생성해준다.

- reshpae와 비교될 수 있는 밀집화는 엑셀의 피벗테이블 기능과 같은 것으로 밀집화 기능을 사용하면 복잡한 데이터를 단순하고 사용하기 편리한 상태로 축소하거나 재정렬할 수 있어 시각적인 효과를 줄 수 있지만,

- 기존의 정보를 손실한다는 단점이 있다.

- 그러나 reshape는 데이터를 재정렬하면서도 원본 데이터의 정보들을 유지할 수 있다.

- melt()는 선택한 id 변수를 이용해서 나머지 변수를 variable란 이름의 데이터로 만드는 것이다.

- cast()는 원하는 형태와 함수를 이용해서 데이터를 요약한다. cast()의 경우 그래프를 시각화할 때의 데이터 구조에 적합하고, melt()는 모델링 할 때의 데이터 구조에 적합하다고 할 수 있다.

- sqldf - SQL 명령이 주어지면 자동으로 스키마를 생성하고 데이터를 테이블로 로드한 뒤 SQL문을 수행하며 SQL 실행 결과를 다시 R로 로드하는 것을 말한다.

- sqldf() 함수를 이용해서 데이터 조회를 실행할 수 있다.

- SQL 문을 이용한 데이터 분석에 사용되며 데이터를 불러올 때 select()를 이용해서 데이터를 추출한다.

- plyr - 두 개 이상의 데이터 프레임을 병합하거나 분리해서 요약하고 집계할 때 사용되는 패키지라고 할 수 있다.

- 데이터를 분리하고 처리한 다음, 다시 결합하는 가장 필수적인 데이터 처리 기능을 제공하고 있으며 한꺼번에 여러 개의 통계치를 구할 수 있다.

10. 프로젝트 산출물

- 프로젝트 필수 산출물로는 데이터 분석과제 계획서가 있는데 여기에는 데이터 분석 목표를 정확하게 정의하고, 프로젝트 진행을 위한 일정 계획을 세우고, 자원을 배분할 계획을 세워야 한다.

- 데이터 탐색 보고서에는 데이터 수집 대상 내용을 포함해서 데이터 상세 내용을 기록해야 하고 프로젝트를 진행하기 위해서 사용할 데이터의 변수를 도출하고, 최종적으로 선택된 변수 목록을 기술해야 한다.

- 데이터 모델링 및 검증보고서에는 데이터 모델링을 위한 방안이 기술되어야 하고 데이터 모델링 개발을 위한 스크립트와 데이터 모델을 비교-검증하기 위한 검증 내용 및 결과를 기록해야 한다.

- 기타 그 외에도 데이터 분석 모델을 유지보수하기 위한 산출물과 프로젝트 관련 교육에 대한 일정과 산출물을 포함해야 한다.

11. 소프트웨어 분석 방법론

- 빅데이터를 분석하기 위한 소프트웨어 분석 방법론은 적용해야 하는 업무에 따라서 다양한 모델을 적용할 수 있다.

- 폭포수 모델

```

과정 - 개발 전 과정을 나누어서 체계적으로 순차적으로 접근하는 방식인 폭포수 모델은 요구사항 분석 -> 설계 -> 구현 -> 테스트 -> 유지보수의 단계로 분석이 이루어진다.

장점 - 체계적 문서화가 가능해서 프로젝트 진행을 명확하게 알 수 있다.

단점 - 단계적으로 이루어지므로 앞 단계가 완료되어야 다음 단계로 넘어갈 수 있다.

```

- 나선형 모델

```

목적 - 반복을 통해서 점증적으로 개발하는 방법으로 프로젝트를 수행할 때 위험을 관리하고 위험을 최소화하는 것이다.

과정 - 나선형 모델은 목표설정 -> 위험 분석 -> 구현 및 테스트 -> 고객평가 및 다음단계 수립의 과정으로 이루어진다.

단점 - 복잡성으로 인한 프로젝트 관리가 어렵고 개발 장기화의 가능성이 존재한다.

```

- 프로토타입 모델

```

목적 - 사용자의 요구사항을 충분히 분석하고자 하는 목적으로 시스템 일부분을 구현한 후 다음 요구사항을 반영하는 방법으로 점진적 개발 방법이다.

과정 - 계획수립 -> 요구사항 분석 및 정의 -> 프로토타입 개발 -> 프로토타입 평가 -> 구현 -> 인수의 과정을 거친다.

장점 - 사용자 요구사항을 도출하는 것이 용이하다.

```

12. 데이터 분석기법 요약

- 빅데이터 분석을 위한 데이터 처리는 통계적 기법, 정형 데이터 마이닝과 비정형 데이터 마이닝으로 나눌 수 있다.

- 여기에서 통계기법과 정형 데이터 마이닝, 비정형 데이터 마이닝에 대해서 간단히 소개한다.

- 통계 분석기법

```

모수 검정 - 검정하고자 하는 모집단의 분포를 가정하고 검정통계량을 구하여 검정을 실시하는 것이다.

비모수 검정 - 모집단의 분포에 제약을 가하지 않고 검정을 실시하는 검정방법이라고 할 수 있다.

기술통계 분석 - 자료를 분석하기 위해 간단한 기술통계 요약표를 만들어서 검토한 후 기초 통계량을 구해서 분석하는 것이다.

고급통계 분석 - 회귀 분석, 시계열 분석, 분산 분석, 판별 분석, 군집 분석, 요인 분석, 로지스틱 회귀 분석 등 프로젝트 기획에 맞추어서 용도에 맞는 분석을 실시하고 결과를 도출하는 것이다.

```

- 정형 데이터 마이닝

```

분류(Classification) - 훈련 데이터군을 학습시켜 만들어진 모델에 새롭게 추가되는 데이터가 어느 군에 속할지를 검증하여 속할 만한 데이터 군을 찾는 지도 학습 방법이다.

추정(Estimation) - 분류가 결과물을 분리하는 데 사용된다면 추정은 입력 데이터를 사용해서 알려지지 않은 결과의 값을 추정하는 데 사용된다.

- 추정의 정확도를 높이기 위해서 모델에 대한 검증이 필요하다.

예측(Prediction) - 장바구니 분석, 의사결정 나무, 신경망 모두 예측에 사용되는 기술들로서 입력데이터의 유형, 특성 및 성격에 따라서 방법론을 결정하게 된다.

연관 분석(Association Analysis) - 아이템 사이의 연관성을 파악하는 분석이다. 연관 분석의 한 기법인 장바구니 분석의 결과는 연관규칙으로 패턴을 발견하여 연관성을 찾아낸다.

군집화(Clustering) - 데이터의 특성을 고려하여 비슷한 특성이 있는 데이터들을 합쳐가면서 유사군으로 분류하는 기법으로 분류와 달리 훈련 데이터군이 이용되지 않기 때문에 군집화는 비지도 학습이다.

요약기술(Description) - 데이터가 가지고 있는 의미를 단순하게 기술하는 것으로 데이터가 암시하는 바에 대한 설명을 의미한다.

- 여기에서 데이터 마이닝 기능을 추진하기 위해서는 목적정의, 데이터준비, 데이터가공, 데이터 마이닝 기법의 적용과 검증 단계가 필요하다.

```

- 비정형 데이터 마이닝(텍스트 마이닝)

```

문서요역 문서분류, 문서군집, 특성 추출의 기능을 가지고 있다.

데이터를 수집해서 데이터에 있는 문장부호나 의미 없는 숫자와 단어를 제거하거나 변형하여 분석에 용이하게 텍스트를 가공하는 과정을 의미한다.

Corpus(말뭉치, 코퍼스)는 데이터 마이닝(텍스트 마이닝)에서 데이터에 대한 정제, 통합, 선택, 변환의 과정을 거친 구조화된 단계이다. 즉 더 이상 추가적인 절차 없이 데이터 마이닝 알고리즘 실험에 활용될 수 있는 상태를 의미한다.

```

- 비정형 데이터 마이닝(사회연결망 분석)

```

개인과 집단들 간의 관계를 노드와 링크로써 모델링을 하고 위상구조와 확산 및 진화과정을 계량적으로 분석하는 방법론이라고 할 수 있다.

즉, 개인적인 인간관계가 확산되어 형성된 사람들 사이에 네트워크를 분석하는 것을 의미한다.

```

# 분석기법

1. 회귀 분석(Regression Analysis)

2. 회귀 분석은 변수들 사이에 함수적인 관계를 알아보기 위해서 하는 통계적 기법으로 독립변수와 종속변수 간의 함수식을 유도하게 된다.

3. 회귀 분석을 통하여 독립변수와 종속변수 간의 상관관계에 따른 함수식을 도출하며, 독립변수들의 값을 이용해서 종속변수의 값을 예측한다.

4. 종속변수 예측

- 독립변수와 종속변수 간에 함수식이 성립되면 함수식을 통해서 독립변숫값을 대입하여 종속변수를 예측하게 된다.

- 회귀 분석은 독립변수의 수가 몇 개인지, 종속변수의 수가 몇 개인지에 따라서 회귀모델을 설정할 수 있다.

5. 단순회귀 분석모형 - 독립변수와 종속변수가 각각1개인 경우로 가장 간단한 회귀 분석모형이다.

6. 다중회귀 분석모형 - 독립변수의 수가 두 개 이상이고 종속변수가 한 개인 경우의 회귀 분석에 해당하며 주어진 독립변수와 종속변수 간에 회귀식을 구해서 종속변수와의 관계를 알아보는 회귀 분석모형이다.

7.  적합성 평가 - 회귀 분석의 모형에 대한 적합성 검정이 필요한데 회귀 분석의 적합성 검정은 다음의 방법을 이용한다.

- 결정계수 확인 - 회귀 분석에서는 회귀모형의 설명력을 알아보기 위한 결정계수(R^2: Coefficient of Determination)를 구해서 회귀 분석의 적합성을 검정할 수 있는데

- 결정계수 값은 0<=R^2<=1로써 1에 가까운 값일수록 회귀식의 설명력이 높다고 할 수 있다.

- 단순 회귀 분석모형의 경우 결정계수(R^2) 산출방법과 그 의미를 설명하면 다음과 같다.

- 종속변수의 값을 yi, 평균을 y-, 회귀 분석모형을 이용한 예측값을 yi^라 한다.

- 여기서 (yi - yi^)는 실제 데이터와 회귀모형(회귀직선)을 통한 추정값의 차이로서 잔차(Residual)라고 하며, 이 값이 작을수록 회귀직선이 실제 데이터를 잘 설명해준다고 할 수 있다.

- 그리고 (yi^ - y-)는 회귀직선에 의해 설명될 수 있는 편차를 의미한다.

- 총편차 분해식을 제곱하여 합하면 다음이 성립한다.

- SST = SSE + SSR

- 즉, 총편차의 제곱합(SST : Total Sum of Squares)은 회귀에 의하여 설명되는 편차의 제곱합(SSR : Regression Sum of Squares)과 잔차들의 제곱합(SSE, Error Sum of Squares)의 합으로 나타낼 수 있다.

- 여기서 자료 전체의 흩어진 정도를 나타내는 SST 중에서 회귀에 의하여 설명디는 부분인 SSR이 클수록 회귀모형이 관측결과를 잘 설명해준다고 할 수 있다.

- 이러한 의미에서 SST 중에서 SSR이 차지하는 비율을 결정계수(R^2) 또는 회귀직선의 기여율이라고 한다.

- R^2 = SSR/SST = 1 - SSE/SST

- 결졍계수(0<=R^2<=1)가 1에 가까울수록 산점에서 점들이 직선 주위에 밀집되어 나타나게 되어 회귀에 의한 설명이 잘 됨을 의미한다.

- 즉, 만약 모든 측정값들이 회귀직선상에 위치한다면 SSE = 0 이고 SSR = SST 이므로 R^2 = 1이 된다.

- 분산 분석 테이블 확인 - 분산 분석(ANOVA : Analysis of Variance) 테이블의 검정통계량 값을 보고 가설의 유의성 검정을 할 수 있다.

- 이때 분석 결과 출력된 F 값을 보고 회귀식의 유의성 검정을 할 수 있으며 유의확률 p가 유의수준 a 보다 작으면 회귀식이 유의하다고 할 수 있다.

8. 회귀 분석의 가정 - 회귀 분석 결과를 활용하기 위해서는 먼저 기본적인 가정이 필요하다. 이러한 가정을 만족하지 않으면 회귀식을 변환하여야 한다.

- 선형성 - 독립변수와 종속변수 간에는 선형적인 관계가 존재(단순회귀 분석의 경우)해야 한다.

- 등분산성 - 회귀식의 잔차는 등분산성(분산이 일정)을 만족해야 한다.

- 독립성 - 회귀식의 잔차는 독립성을 만족해야 한다. 독립변수들 간에 독립이 보장되어야 한다.

- 정규성 - 회귀식의 잔차는 평균이 0(불편성)이고 표준편차가 6인 정규 분포를 따라야 한다.

9. 독립변수 선택 방법 - 다중회귀 분석을 위해서 사용된 독립변수는 유의성 검정이 필요하다.

- 회귀 분석에서 독립변수로 사용된 k개의 독립변수 중에서 유의성 검정을 통해서 변수를 선택하게 된다.

- 다중회귀 분석에서 독립변수의 선택방법으로는 네 가지가 있으며 각각의 방법은 다음과 같다.

- 변수 모두 선택 - 회귀 분석을 실시할 경우 독립변수로 사용된 변수 모두를 이용해서 회귀식을 세우는 것이 첫 번째 방법이다.

- 전진선택법(Forward Selection) - 사용된 독립변수 중에서 가장 유의한 변수를 선택해서 하나씩 회귀식에 추가하는 방법이다.

- 즉, 종속변수에 가장 영향이 큰 독립변수라고 판단되는 변수를 하나씩 추가해 가면서 회귀방정식을 만들어 가는 방법이다.

- 후진선택법(Backward Selection) - 회귀 분석을 위해서 모든 독립변수들을 이용해서 회귀식을 세우고 가장 유의하지 않은 독립변수를 제거하면서 적합한 회귀식을 찾는 방법이다.

- 즉, 회귀방정식에 포함된 모든 독립변수를 이용해서 회귀방정식에 가장 중요하지 않다고 판단되는 변수를 하나씩 제거해 가면서 회귀방정식을 세우는 방법이다.

- 단계별 선택법(Stepwise Selection) - 전진선택법과 후진제거법을 동시에 이용하는 통계적 기법으로 단계별로 변수를 선택하는 방법이다.

- 즉, 단계별 선택법은 전진선택법과 후진제거법을 절충해서 만든 방법으로 기준 통계치에 영향이 적은 변수를 삭제하거나 모델에서 빠진 변수 중 모델을 위해서 추가해야 하는 변수를 검증하는 작업을 반복 수행해서 변수를 선택하는 방법이다.

10. 다중공선성 - 회귀 분석을 실시할 경우 독립변수들 사이에 강한 상관관계가 나타나는 문제를 의미한다.

- 독립변수 간에 상관관계가 존재해서 회귀 분석을 할 경우 회귀계수의 분산을 크게 하여 회귀 계수에 대한 추정에 문제가 발생하는 경우가 생긴다.

- 분산팽창지수(VIF : Variance Inflation Factor) - 다중공선성을 측정하기 위해서는 분산팽창지수를 계산하거나, 공차한계(VIF의 역수) 등을 통해서 확인하고,

- 다중공선성 문제를 해결하기 위해서는 다중공선성 문제를 일으키는 설명변수를 제거한다. 

- 분산팽창지수 VIF = 1 / 1 - R^2 (R^2는 결정계수)

11. 회귀 분석 절차 - 회귀식을 세우기 위한 독립변수와 종속변수 결정

- 종속변수와 독립변수가 선형을 만족하는지 확인

- 최소제곱법을 이용해서 회귀계수 추정

- 회귀모형의 유의성 검정 실시

- 회귀모형이 유의하면 독립변수별 회귀계수의 유의성을 검정

