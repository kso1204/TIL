# 기술통계 vs 추론통계

1. 기술통계(Descriptive Statistics) 묘사, 요약, 시각화

- 집단을 묘사, 요약, 정리

- Location parameter

- Dispersion parameter

- Frequency tables

- graphics

- 평균, 분산, 최빈값, 표준편차, 히스토그램, 박스플랏...

- 말 그대로 데이터를 묘사하는 통계

- 데이터를 요약해서 설명

- 중심경향성(평균, 중앙값, 최빈값)

- 산포도 (분산, 표준편차, 사분위편차, 변동계수)

- 분포 (왜도, 첨도, 분포, 시각화)

- 백분위수

2. 추론통계(Inferential Statistics) 추측, 예측

- 표본으로 결론 도출, 추론

- 신뢰구간(Confidenct interval), 가설검정(Hypothes Testing)

- Simple test procedures

- Regression Analysis

- Correlation analysis (상관계수 - 피어슨 상관계수, 스피어만 상관계수)

- ANOVA

- 추정 (점푸정, 구간추정, 최대우도점추정, 평균제곱오차, 편향모평균 구간추정, 모분산 신뢰구간, 모비율 신뢰구간)

- 가설검정 (귀무가설, 유의수준, 1종오류, 2종오류, 검정통계량, 표본의 평균 검정, 평균차이 검정)

# 중심경향 기초통계량 (Central Tendency)

1. 기초통계량 3가지

- 중심경향성 (평균(Mean), 중앙값(Median), 최빈값(Mode)) - 하나의 값으로 전체 데이터의 중앙을 대표할 수 있도록 표현함

- 산포도(Dispersion) - 퍼진정도 (분산(variance), 표준편차(std), 사분위편차, 변동계수)

- 분포(Distribution)의 모양 (왜도(Skewness), 첨도(Kurtosis))

2. 중심경향성

- 평균(Mean) - 산술 평균 값들을 모두 더해서 개수로 나눔

- 중앙값(Median) - 주어진 값들을 오름차순으로 정렬했을 때 중앙에 있는 값. 극단치 제거시 사용

- 최빈값(Mode) - 가장 많이 나온 값(도수가 가장 높은 값) 단, 각 데이터의 도수가 1일때는 값 없음

# 산포도 기초통계량 (Degree of Dispersion)

1. 산포도(Dispersion) - 퍼진정도 (분산(variance), 표준편차(std), 사분위편차, 변동계수(Coefficient of Variance))

2. 데이터의 중심에서 얼마나 퍼져있는가를 측정하는 기초 통계량

3. 중심경향 기초통계량으로 중심 측정 후, 자료가 얼마나 멀리 분포 되어있는가를 측정함

4. 분산(Variance) - 편차의 제곱의 평균

5. 범위(Range) - 데이터 구간의 길이(최대값 - 최소값)

6. 사분위편차(Inter Quartile Range) - 오름차순 25%, 50%, 75%, 100% => 75% 숫자 - 25% 숫자

7. 변동계수(CV, Coefficient Variance) - 표준편차/평균의 비율(%), 두 집단의 상대적 산포의 크기 비교

- 변동계수는 상대적이라는 말이 등장

- A집단 : 평균 = 10, 표준편차 = 2 , 2/10 * 100 = 20%

- B집단 : 평균 = 20, 표준편차 = 10 , 10/20 * 100 = 50%

- B집단의 변동계수는 50%, A집단의 변동계수는 20%이다. B집단의 변동계수가 더 크므로 B집단이 상대적으로 평균에서 멀리 분포해 있다.

# 분포의 모양 기초통계량 (Shape of Distribution)

1. 분포(Distribution)의 모양 (왜도(Skewness), 첨도(Kurtosis))

2. 분포의 좌우 비대칭성, 분포의 뾰족한 정도 등 분포의 모양을 측정

3. 편차제곱의 평균으로는 왼쪽 좌우쪽 좌우 비대칭성으르 표현할 수 없음

4. 왜도(Skewness) = 분포의 좌우 비대칭성

- Skewness > 0 = 평균에 비해 많이 큰 값 존재 (Positive) mean > median > mode

- Skewness < 0 = 평균에 비해 많이 작은 값 존재 (Negative) mean < median < mode

- Skewness = 0 = (Zero) mean = median = mode

5. 첨도(Kurtosis) = 분포의 뾰족한 정도

- Kurtosis > 3 = 정규분포보다 뾰족

- Kurtosis = 3 = 정규분포

- Kurtosis < 3 = 정규분포보다 완만함(뾰족 x)

# 시각적 데이터 탐색 - 차트 (Box Plot, Stem - Leaf)

1. 히스토그램(Histogram) - 각 데이터의 구간별 도수 또는 상대도수

2. 막대그래프(Bar Chart) - 각 데이터 변수에 대한 값들 비교

3. 파이차트(Pie chart) - 각 데이터의 상대도수를 시각화(비율%)

4. 산점도(Scatter plot) - 두 변수를(x, y) 좌표로 나타냄. 상관관계

5. 줄기-잎(Stem-Leaf Diagram) - 줄기-잎으로 데이터 정리 및 구분

6. 상자 수염(Box Plot) - 이상치를 확인하기 위해 5가지를 요약(Low, Q1(25%), Q2(50%), Q3(75%), Up)

# 데이터 전처리(Preprocessing) vs EDA(탐색적 데이터 분석, Explortary Data Analysis)

1. EDA(Data Spread, Data Quality, Variable Relationship의 교집합)

2. pandas_profiling.ProfileReport(df)

3. 데이터 분석(모델링 전) 데이터의 이상치 발견, 데이터 전처리(Preprocessing)에서 결측, 이상치 등 확인 및 변수 변환

4. Data Preparation 과정

- Raw Data -> Structure Data -> Data Preprocessing -> Exploration Data Analysis(EDA) -> Insight, Reports, Visual Graphs

5. EDA 

- 데이터 모델링에 들이가기전에, 수집된 데이터의 분포, 관계를 파악하는 과정 (히스토그램, 산포도, 기초통계량 등으로 분포 파악)하고 모델링, 알고리즘에 사용될 수 있는 Dataset인지 파악함

- 모델링 input에 사용하기 부족하다면 다시 정제한다.

6. 데이터 전처리

- 수집된 데이터에는 이상한, 극단적인 데이터가 섞여 있을 수 있음. 데이터의 가공 없이 데이터 분석 및 모델링을 하면 결과가 이상하게 나올 수 있다.

- 데이터를 분석에 사용할 수 있도록 정제하고 가공하고 변환하는 과정을 거쳐 모델링에 필요한 변수로 만드는 과정

- 결측치 제거, 데이터 노이즈 제거, 데이터 이상치 제거, 데이터의 불균형

- 데이터 실수화, 데이터 정제, 데이터 통합, 데이터 축소, 데이터 변환, 데이터 조정

# 결측치(Missing Value) vs 이상치 NaN (Outlier)

1. 결측치 - 값이 없이 비어있는 데이터, NA(Not Available), NaN(Not a Number), Null 상태

2. 결측유형 : MCAR, MAR, NMAR

3. 결측치 대체 방법 - 단순 대체, 최대우도, 다중대체

4. 이상치 - 일반적인 데이터의 값에서 벗어난 값, 추세(경향)에서 벗어난 값

5. 이상치 탐지(모수적/비모수적), 시각화(Box Plot, Stem-Leaf), Z-Score

# 결측치 종류? 대체 방법 (MCAR, MAR, MNAR)

1. 삭제, 단순대치, 다중대치

2. MCAR(Missing Completely At Random) - 완전 무작위 결측

- 결측값 발생이 다른 변수와 상관이 없는 경우(전산 오류로 데이터 누락, 네트워크 문제로 누락)

3. MAR(Missing At Random) - 무작위 결측 

- 결측값 발생이 특정 변수와 상관이 있으나, 얻고자 하는 결과와는 상관이 없는 경우

- 30대 남성이 용돈 설문 결측 자주 발생, 얻고자 하는 결과(소득수준)와 30대 남성 용돈 설문 결측이 무관할 경우

4. MNAR(Missing Not At Random) - 비무작위 결측

- 결측값 발생이 다른 변수와 상관이 있는 경우

- 용돈 설문 -> 소득이 일정금액 이하인 사람이 용돈 설문 결측, 소득 수준과 용돈 설문 상관성이 있는 경우

5. 삭제(주로 MCAR) - 단일값(Pairwise) 삭제, 특정값(Listwise) 삭제

6. 대체 - 특정값,단일값(Simple) 대체(Imputation), 다중값(Multiple) 대체(Imputation)

7. 단순대체 

- 평균값 대체(Mean, Median, Mode 등으로 대체)

- 회귀 대체 - 관측된 데이터로1차 회귀선을 구함, 결측치를 회귀선의 Y값으로 대체

- Single Stochastic Imputation - 결측값을 어떤 확률로 대체하겠다?

8. 핫덱(Hot-Deck)

- 사회조사적 방법(연구적인 자료), 표본을 바탕으로 비슷한 성향 추출

9. 콜드덱(Cold-Deck)

- 사회조사적 방법(외부 자료), 외부출처, 비슷한 연구로부터 

# 이상치(Outlier) 탐지

1. 통계적 기법(기초통계량 활용)

- ESD, 기하평균, 사분위수 => 평균, 표준편차, IQR

- 이상치? 평균에서 멀다 => 산포도를 활용해서 기초통계량 측정

- ESD(Estreme Studentized Deviation) 평균에서 좌우로 3시그마 떨어진 것(표준편차) 약 0.2%의 데이터 z=3이라는 것은 표준편차 3칸 떨어졌다는 것을 의미 => z-Score 변환이라고 한다.

- 기하평균(Geometric Mean) 활용 (곱하기의 평균(1/n루트)) - 기하평균에서 좌우로 2.5시그마

- 사분위편차(IQR) 활용 - 좌우 양쪽 1.5IQR Q1(25%)-1.5*IQR, Q3(75%)+1.5*IQR

2. 시각화(차트)

- 히스토그램, 확률밀도함수, Box Plot

3. 모델링(분석기법 활용)

- K-군집, 마할라노비스, LOF(Local Outlier Factor), isolation Forest(의사결정나무)

# Feature Vs Target, Feature Engineering

1. X : 독립변수 (원인)

- Independent, Feature, Input, Attribute, Predict, Explanatory, Risk Factor, Control, Manuplate 등 ...

2. Y : 종속변수 (결과)

- Dependent, Label, Class, Target, Response, Outcome 등 ...

3. 결과 예측을 위해 Feature를 몇 개 선택할 것인가? -> Feature Selection, Feature Extraction(Dimension)

4. Feature Engineering? => Feature VS Target

# Feature Selection(필터, 랩퍼, 임베디드)

1. Filter

- Corrleation, Proxy Measure

- Correlation-based feature selection

- EDA를 통해서 데이터의 상관성 탐색, 원인과 결과의 상관성 측정

- 상관관계 등으로 예측하고자 하는 결과와 얼마나 관련이 있는지 파악한다.

- 즉 Filter 는 상관성이 낮은 변수를 필터로 걸러낸다.

2. Wrapper 

- Predictvie Model, Subset 추출

- All Features -> Subset of Features -> ML Alogorithm -> Performance

- 정확성은 높지만 비용 + 시간이 많이 든다, 계속 반복하기 때문에

- Stepwise regression, Best subset regression

- 머신러닝 예측모델에서 좋은 성능?(AIC?)을 보이는 Subset 추출

- 머신러닝 알고리즘을 반복해서 변수를 찾아가기 때문에 비용, 시간이 많이 든다.

- 즉 모델링에 적합한 Feature를 찾을 수 있으나, Cost(시간, 비용)를 포기

- Feature의 Subsetd을 예측 모델의 성능 관점에서 찾는 방법 (여러번 반복 Iterably)

- Forward Method - 상관관계가 높은 것부터 하나씩 추가
 
- Backward Method - 모든 변수를 넣고 하나씩 제거

- Stepwise Method - Forward, Backward 결합해서 추가/제거

3. Embedded or Intrinsic

- During Model Builiding, Select Feature

- Actual Feature Set -> Subset of Features (Lasso Regression, Ridge Regression, Elastic net)

- 모델안에 변수선택이 포함

- 회귀분석처럼 변수를 선택하고 최적화된 해(변수)를 찾아내는 과정이 모델링안에 포함

- 즉, 모델의 정확도에 영향을 주는 변수를 학습함

# 공분산(Covariance)

1. Cov(x, y)??

2. 공분산, 직선관계(Linear), X와 Y의 상관성을 찾는 것 

3. Corrleation

- 1 <= e <= 1, 두 변수 간에 직선관계?

- 직선은 같이 증가 or 같이 감소

4. 공분산의 한계?

- 극단치들로 인해 값을 왜곡할 수 있어서, 상관계수의 표준화가 필요하다

# Correlation

1. 직선관계, 1차식, y = ax + b

2. 상관계수가 Positive(양수), 상관계수가 Negative(음수)

3. 강한 양의 상관관계? 약한 양의 상관관계

4. 상관계수가 0이면?

5. 두 변수 x,y 간의 공분산 Cov(x,y)을 각각의 x표준편차, y표준편차로 나눈 수치

6. -1 <= 상관계수 <= 1

7. 상관계수 = 0 직선관계가 없다

8. 1이면 증가직선, -1이면 감소직선

9. 0에 가까울수록 넓게 분포하는 그림이 그려진다. (즉 약하다)

10. 직선의 기울기랑 상관계수랑은 상관 없음, 직선 관계가 어느 정도 있는지를 측정하는 것 !! 

11. 직선을 어떻게 찾을 것인가? 회귀분석!!

12. 피어슨 상관계수 VS 스피어만 상관계수

# 선형회귀분석 (Simple Linear Regression)

1. 직선의 식을 찾아내는 과정 -> 선형회귀분석

2. y = ax + b => x(원인, Feature), y(결과, Target), a, b를 찾는 과정

3. 관측된 Data를 좌표(x,y) 산점도로 나타내면 점으로 표현 가능

4. 여기서 관측된 점 좌표 (x,y)는 x 독립변수, y 종속변수라고 가정

5. 직선 y = ax + b를 만족하는 a와 b를 찾아내고 y = ax + b 식을 찾아냄 (선형회귀분석)

6. 회귀분석의 가정??(선형성, 독립성, 동분산성, 정규성)

7. 직선 y = ax + b를 알아낸다면 임의의 x로부터 y를 예측할 수 있다.

8. 관측된 점과 예측한 회귀선과의 거리(Residual, 잔차)

9. 어떤직선? 잔차의 합이 최소가 되는 직선 찾기 => 최소 자승법(List square Method)

10. y = ax + b + (잔차)

# 다중선형회귀분석 (Multiple Linear Regression)

1. 빅데이터를 분석하려면 원인이 1개가 아니라 여러개..

2. 복잡한 Dataset의 직선관계를 찾아 n개의 원인변수와 결과 y를 찾음, 중요한의미는 원인변수로 결과 y 예측

3. 원인이 여러개라는 것은 차원이 늘어나는 것

4. 정규성, 등분산성, 선형성, 다중공선성(독립성)

5. y = 기울기랑 y절편을 이용 (x1, x2, x3, x4, ... ,xn)으로 y를 찾는 것(즉 여러개의 원인!!), 단순은? x1밖에 없음

6. 각각의 독립변수와 y와의 직선관계

7. 변수들간의 다중공선성(독립?)

8. 다중공선성 만족하는 X의 개수(차원)

# 다중공선성(Multi-Collinearity)

1. 회귀분석 가정

- 선형성(Linearity)

- 잔차의 등분산성(Equal Variance)

- 잔차의 정규성(Normality)

- 다중공선성 만족하는 x (변수의 독립성)

