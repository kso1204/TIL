# 분석 변수 점검항목 정의

1. 데이터 수집

- 데이터 적정성, 데이터 가용성, 대체 분석 데이터 유무

2. 데이터 정합성

- 데이터 중복, 분석 변수별 범위, 분석 변수별 연관성, 데이터 내구성

3. 특징 변수

- 특징 변수 사용, 변수 간 결합 가능 여부

4. 타당성

- 편익/비용 검증, 기술적 타당성

# 데이터 전처리 수행

1. 데이터 정제, 통합, 축소, 변환을 반복적으로 수행하여 분석 변수로 활용하는 방안을 수립할 수 있다.

# 분석 절차

1. 문제 인식 -> 연구조사 -> 모형화 -> 데이터 수집 -> 데이터 분석 -> 분석 결과 제시

# 작업분할구조(WBS, Work Breakdown Structure)

1. 프로젝트의 범위와 최종 산출물을 세부요소로 분할한 계층적 구조도

2. 데이터 분석과제 정의 -> 데이터 준비 및 탐색 -> 데이터 분석 모델링 및 검증 -> 산출물 정리

# 분석목표정의서

1. 문제의 개선방향에 맞는 현실적인 분석목표를 수립하여 필요한 데이터에 대한 정보나 분석 타당성 검토 및 성과측정 방법 등을 정리한 정의서

2. 구성요소

- 원천 데이터 조사

- 분석 방안 및 적용 가능성 판단

- 성과평가

# 분석 프로젝트

1. 분석 프로젝트는 과제 형태로 도출된 분석 기회를 프로젝트화하여 그 가치를 증명하기 위한 수단이다.

2. 도출된 결과의 재해석을 통한 지속적인 반복과 정교화가 수행되는 경우가 대부분이다.

3. 관리 영역

- 데이터 크기

- 데이터 복잡도

- 속도

- 분석 모형의 복잡도(Analytic Model Complexity) - 분석 모형의 정확도와 복잡도는 Trade off 관계에 있다.

- 정확도(Accuracy)와 정밀도(Precision) - 분석 결과를 활용하는 측면에서는 정확도가 중요하다, 분석 모형의 안정성 측면에서는 정밀도가 중요하다. 정확도와 정밀도는 Trade off인 경우가 많다.

- 정확도와 정밀도는 편향(Bias)과 분산(Variance)과 관련이 있다. 반대의 속성

3. 분석 프로젝트의 영역별 주요 관리 항목

- 범위, 일정, 원가, 품질, 통합, 조달, 인적자원, 위험, 의사소통, 이해관계자 관리

# 비즈니스 도메인과 원천 데이터 정보 수집

1. 비즈니스 도메인 정보

- 비즈니스 모델, 비즈니스 용어집, 비즈니스 프로세스로부터 관련 정보를 습득한다.

- 도메인 전문가 인터뷰를 통해 데이터의 종류, 유형, 특징 정보를 습득한다.

2. 원천 데이터 정보

- 데이터 분석에 필요한 대상 원천 데이터의 수집 가능성, 데이터의 보안, 정확성을 탐색하고, 데이터 수집의 난이도, 수집 비용 등 기초 자료를 수집할 수 있다.

# 내, 외부 데이터 수집

1. 데이터의 종류

- 내부 데이터는 조직 내부의 서비스 시스템, 네트워크 및 서버 장비, 마케팅 관련 시스템 등으로부터 생성되는 데이터를 말한다.

- 외부 데이터는 다양한 소셜 데이터, 특정 기관 데이터, M2M 데이터, LOD(LinkedOpen Data) 등으로 나눌 수 있다.

# 데이터 유형별 수집 기술

1. 정형 데이터

- ETL, FTP, API, DBtoDB, 스쿱(Sqoop) - 관계형 데이터베이스(RDBMS)와 하둡(Hadoop)간 데이터를 전송하는 방법

2. 비정형 데이터

- 크롤링, RSS(Rich Site Summary), Open API, 척와(Chukwa), 카프카

3. 반정형 데이터

- 플럼(Flume), 스크라이브(Scribe), 센싱(Sensing), 스트리밍(Streaming) - TCP, UDP, Bluetooth, RFID

# 스쿱

1. Sqoop(SQL + Hadoop)

2. 관계형 데이터 스토어 간에 대량 데이터를 효과적으로 전송하기 위해 구현된 도구이다.

3. 커넥터를 사용하여 MySql, Orcle, MS SQL 등 관계형 데이터베이스의 데이터를 하둡 파일시스템(HDFS, Hive, Hbase)으로 데이터를 수집한다.

4. 관계형 데이터베이스에서 가져온 데이터들을 하둡 맵리듀스(Hadoop MapReduce)로 변환하고, 변환된 데이터들을 다시 관계형 데이터베이스로 내보낼 수 있다.

5. 데이터 가져오기/내보내기 과정을 맵리듀스를 통해 처리하기 때문에 병렬처리가 가능하고 장애에도 강한 특징을 갖는다.

6. 스쿱은 모든 적재 과정을 자동화하고 병렬처리 방식으로 작업한다.

7. Bulk import 지원 - 전체 데이터베이스 또는 테이블을 HDFS로 전송 가능하다.

8. 데이터 전송 병렬화 - 시스템 사용률과 성능을 고려한 병렬 데이터를 전송한다.

9. Direct Input 제공 - RDB에 매핑하여 Hbase와 Hive에 직접적 import를 제공한다.

10. 프로그래밍 방식의 데이터 인터랙션 - 자바 클래스 생성을 통한 데이터 상호작용을 지원한다.

# 플럼

1. 아파치 플럼은 대요량의 로그 데이터를 효과적으로 수집, 집계, 이동시키는 신뢰성 있는 분산 서비스를 제공하는 솔루션이다.

2. 스트리밍 데이터 흐름에 기반을 둔 간단하고 유연한 구조를 가진다.

3. 플럼에서 하나의 에이전트는 소스, 채널, 싱크로 구성된다.

4. 소스는 웹서버, 로그데이터서버 등 원시데이터소스와 연결되며, 소스로부터 들어오는 데이터는 큐의 구조를 갖는 채널로 들어간 후, 싱크를 통해 목표 시스템으로 전달된다.

5. 로그 데이터 수집과 네트워크 트래픽 데이터, 소셜 미디어 데이터, 이메일 메시지 등 대량의 이벤트 데이터 전송을 위해 사용된다.

6. 특징으로는 신뢰성, 확장성, 효율성을 가지고 있다.

# 데이터 적절성 검증

1. 데이터 누락 점검 - 수집 데이터 세트의 누락, 결측 여부를 판단하여 누락 발생 시 재수집한다.

2. 소스 데이터와 비교 - 수집 데이터와 소스 데이터의 사이즈 및 개수를 비교 검증한다.

3. 데이터의 정확성 점검 - 유효하지 않는 데이터 존재여부를 점검한다.

4. 보안 사항 점검 - 수집 데이터의 개인정보 유무 등 보안 사항의 점검이 필요하다.

5. 저작권 점검 - 데이터의 저작권 등 법률적 검토를 수행한다.

6. 대량 트래픽 발생 여부 - 네트워크 및 시스템에 트래픽을 발생시키는 데이터 여부를 검증한다.

# 데이터 변환 방식의 종류

1. 비정형 데이터를 정형 데이터 형태로 저장하는 방식(관계형 데이터베이스)

2. 수집 데이터를 분산파일시스템으로 저장하는 방식(HDFS 등)

3. 주제별, 시계열적으로 저장하는 방식(데이터 웨어하우스)

4. 키-값 형태로 저장하는 방식(NoSQL)

```

관계형 데이터 베이스 - MySQL, Oracle, DB2, PostgreSQL

분산데이터 저장 - HDFS(Hadoop Distributed File System)

데이터 웨어하우스 - 네티자, 테라데이타, 그린플럼의 DW 솔루션

NoSQL - Hbase, Cassandra, MongoDB

```

# 비식별 조치 방법

1. 가명처리 - 개인 식별이 가능한 데이터를 직접적으로 식별할 수 없는 다른 값으로 대체하는 기법

```

장점 - 데이터의 변형 또는 변질 수준이 적다.

단점 - 대체 값 부여 시에도 식별 가능한 고유 속성이 계속 유지된다.

```

2. 가명처리 세부기술 

```

휴리스틱 가명화 - 식별자에 해당하는 값들을 몇 가지 정해진 규칙으로 대체하거나 사람의 판단에 따라 가공하여 자세한 개인정보를 숨기는 방법

암호화 - 정보 가공시 일정한 규칙의 알고리즘을 적용하여 암호화함으로써 개인정보를 대체하는 방법

교환방법 - 기존의 데이터베이스 레코드를 사전에 정해진 외부의 변수(항목)값과 연계하여 교환한다.

```

3. 총계처리 - 통계값을 적용하여 특정 개인을 식별할 수 없도록 한다.

```

장점 - 민감한 수치 정보에 대하여 비식별 조치가 가능하며, 통계분석용 데이터셋 작성에 유리하다.

단점 - 정밀 분석이 어려우며, 집계 수량이 적을 경우 추론에 의한 식별 가능성이 있다.

```

4. 총계처리 세부기술

```

부분총계 - 데이터셋 내 일정부분 레코드만 총계 처리하며, 다른 데이터 값에 비하여 오차 범위가 큰 항목을 통계값(평균 등)으로 변환한다.

라운딩 - 집계 처리된 값에 대하여 라운딩(올림, 내림, 반올림) 기준을 적용하여 최종 집계 처리하는 방법이다.

재배열 - 기존 정보 값은 유지하면서 개인이 식별되지 않도록 데이터를 재배열 하는 방법이다.

```

5. 데이터 삭제 - 개인 식별이 가능한 데이터를 삭제 처리한다.

```

장점 - 개인 식별요소의 전부 및 일부 삭제 처리가 가능하다.

단점 - 분석의 다양성과 분석 결과의 유효성, 신뢰성이 저하된다.

```

6. 데이터 삭제 세부기술

```

식별자 (부분)삭제 - 원본 데이터에서 식별자를 단순 삭제하는 방법과 일부만 삭제하는 방법이 있다.

레코드 삭제 - 다른 정보와 뚜렷하게 구별되는 레코드 전체를 삭제하는 방법이다. (소득)

식별요소 전부삭제 - 식별자뿐만 아니라 잠재적으로 개인을 식별할 수 있는 속성자까지 전부 삭제하여 프라이버시 침해 위험을 줄이는 방법이다. (연예인, 정치인 정보)

```

7. 데이터 범주화 - 특정 정보를 해당 그룹의 대푯값 또는 구간값으로 변환(범주화)하여 개인 식별을 방지한다.

```

장점 - 통계형 데이터 형식이므로 다양한 분석 및 가공 가능하다.

단점 - 정확한 분석결과 도출이 어려우며, 데이터 범위 구간이 좁혀질 경우 추론 가능성이 있다.

```

8. 데이터 범주와 세부기술

```

감추기 - 명확한 값을 숨기기 위하여 데이터의 평균 또는 범주 값으로 변환하는 방식이다.

랜덤 라운딩 - 수치 데이터를 임의의 수 기준으로 올림 또는 내림하는 기법으로 수치 데이터 이외의 경우에도 확장 적용 가능하다. (42세, 45세 -> 40대)

범위 방법 - 수치 데이터를 임의의 수 기준으로 범위(range)로 설정하는 기법이다. (3,300만원 -> 3,000 ~ 4,000만 원)

제어 라운딩 - 랜덤 라운딩 방법에서 어떠한 특정 값을 변경할 경우 행과 열의 합이 일치하지 않는 단점을 해결하기 위해 행과 열이 맞지 않는 것을 제어하여 일치시키는 기법이다.

```

9. 데이터 마스킹 - 데이터의 전부 또는 일부분을 대체 값(공백, 노이즈)으로 변환한다.

```

장점 - 개인 식별 요소를 제거하는 것이 가능하며, 원 데이터 구조에 대한 변형이 적다.

단점 - 마스킹을 과도하게 적용할 경우 데이터 필요 목적에 활용하기 어려우며 마스킹 수준이 낮을 경우 특정한 값에 대한 추론이 가능하다.

```

10. 데이터 마스킹 세부기술

```

임의 잡음 추가 - 개인 식별이 가능한 정보에 임의의 숫자 등 잡음을 추가(더하기 또는 곱하기)하는 방법이다.

공백과 대체 - 특정 항목의 일부 또는 전부를 공백 또는 대체문자('*','_' 등이나 전각 기호)로 바꾸는 기법이다. (1990-12-04를 19**-**-**)

```

# 적정성 평가

1. 적정성 평가 시 프라이버시 보호 모델 중 최소한의 수단으로 k-익명성을 활용하며, 필요시 추가적인 평가모델(l-다양성, t-근접성)을 활용한다.

2. k-익명성 - 특정인임을 추론할 수 있지 여부를 검토, 주어진 데이터 집합에서 같은 값이 적어도 k개 이상 존재하도록 하여 쉽게 다른 정보로 결합할 수 없도록 한다.

3. k-익명성의 취약점

```

동질성 공격 - k-익명성에 의해 레코드들이 범주화되었더라도 일부 정보들이 모두 같은 값을 가질 수 있기 때문에 데이터 집합에서 동일한 정보를 이용하여 공격 대상의 정보를 알아내는 공격이다.

배경지식에 의한 공격 - 주어진 데이터 이외의 공격자의 배경 지식을 통해 공격 대상의 민감한 정보를 알아내는 공격이다 (여자는 전립선염에 걸릴 수 없다.)

```

4. l-다양성 - k-익명성에 대한 두 가지 공격, 즉 동질성 공격 및 배경지식에 의한 공격을 방어하기 위한 모델, 비식별 조치 과정에서 충분히 다양한(l개 이상) 서로 다른 정보를 갖도록 동질 집합을 구성함

5. l-다양성의 취약점

```

쏠림공격 - 정보가 특정한 값에 쏠려 있을 경우 l-다양성 모델이 프라이버시를 보호하지 못한다. (임의의 집단이 99개의 위암 양성, 1개의 위암 음성이면 공격자는 공격 대상이 99%확률로 위암 양성이라는 것을 알 수 있음)

유사성 공격 - 비식별 조치된 레코드의 정보가 서로 비슷하다면 l-다양성 모델을 통해 비식별된다 할지라도 프라이버시가 노출될 수 있다.

```

6. t-근접성 - l-다양성의 취약점(쏠림 공격, 유사성 공격)을 보완하기 위한 모델로 값의 의미를 고려하는 모델, t-근접성은 동질 집합에서 특정 정보의 분포와 전체 데이터 집합에서 정보의 분포가 t 이하의 차이를 보여야 한다.

# 정형 데이터 품질기준

1. 완전성, 유일성, 유효성, 일관성, 확장성

2. 완전성 - 필수항목에 누락이 없어야 한다. (개별 완전성, 조건 완전성)

3. 유일성 - 데이터 항목은 유일해야 하며 중복되어서는 안된다. (단독 유일성, 조건 유일성)

4. 유효성 - 데이터 항목은 정해진 데이터 유효범위 및 도메인을 충족해야 한다. (범위 유효성, 날짜 유효성, 형식 유효성)

5. 일관성 - 데이터가 지켜야할 구조, 값, 표현되는 형태가 일관되게 정의되고, 서로 일치해야 한다. (기준코드 일관성, 참조 무결성, 데이터 흐름 일관성, 칼럼 일관성)

6. 정확성 - 실세계에 존재하는 객체의 표현 값이 정확히 반영되어야 한다. (선후 관계 정확성, 계산/집계 정확성, 최신성, 업무규칙 정확성)

# 비정형 데이터 품질 기준

1. 기능성, 신뢰성, 사용성, 효율성, 이식성

2. 기능성 - 명시된 요구와 내재된 요구를 만족하는 기능을 제공하는 정도 (적절성, 정확성, 상호 운용성, 기능 순응성)

3. 신뢰성 - 규정된 신뢰 수준을 유지하거나 사용자로 하여금 오류를 방지할 수 있도록 하는 정도 (성숙성, 신뢰 순응성)

4. 사용성 - 사용자에 의해 이해되고, 선호될 수 있게 하는 정도 (이해성, 친밀성, 사용 순응성)

5. 효율성 - 자원의 양에 따라 요구된 성능을 제공하는 정도 (시간 효율성, 자원 효율성, 효율 순응성)

6. 이식성 - 다양한 환경과 상황에서 실행될 가능성 (적응성, 공존성, 이식 순응성)

# 빅데이터 저장 시스템

1. 대용량 데이터 집합을 저장하고 관리하는 시스템으로 사용자에게 데이터 제공 신뢰성과 가용성을 보장하는 시스템이다.

2. 파일 시스템 저장방식 - 빅데이터를 확장 가능한 분산 파일의 형태로 저장하는 방식의 대표적인 예는 HDFS, 구글의 GFS 등을 들 수 있다. 

# 분산 파일 시스템

1. 하둡 분산파일 시스템(HDFS) 

2. 하둡은 아파치 진영에서 분산 환경 컴퓨팅을 목표로 시작한 프로젝트로 분산 처리를 위한 파일 시스템이다.

3. HDFS는 대용량 파일을 클러스터에 여러 블록으로 분산하여 저장하며, 블록은 마지막 블록을 제외하고 모두 크기가 동일하다(기본 크기 64MB)

4. HDFS는 마스터(Master) 하나와 여러 개의 슬레이브(Slave)로 클러스트링 되어 구성된다.

- 마스터노드(Master Node)는 네임 노드(Name Node)라고 하며 슬레이브를 관리하는 메타데이터와 모니터링 시스템을 운영한다.

- 슬레이브노드(Slave Node)는 데이터 노드(Data Node)라고 하며 데이터 블록을 분산처리한다.

5. 데이터 손상을 방지하기 위해서 데이터 복제 기법을 사용한다.

6. 하둡의 장점

```

하둡의 DFS는 대용량의 빙정형 데이터 저장 및 분석에도 효율적이다.

클러스터 구성을 통해 멀티 노드로 부하를 분산시켜 처리한다.

개별적인 서버에서 진행되는 병렬처리 결과를 하나로 묶어 시스템의 과부하나 병목 현상을 줄여준다.

하둡을 장비를 증가시킬 수록 성능이 향상된다.

오픈소스 하둡은 무료로 사용할 수 있다.

```

7. 분산 데이터 처리기술 - 맵리듀스(MapReduce)

```

구글에서 발표한 MapReduce 방법을 하둡 오픈소스 프로젝트에서 구현하였다.

MapReduce는 주어진 입력에 대해, 이를 여러 개의 부분으로 분할하고 각각의 부분에 대해 필요한 함수를 적용하여 결과값을 저장한다(Map함수와 Reduce함수로 구성)

분산 병렬처리가 가능하다.

Input -> Spliting -> Mapping -> Shuffling -> Reducing -> Final result

```

8. 구글 파일 시스템(GFS)

9. 구글 파일 시스템은 엄청나게 많은 데이터를 보유하는 구글의 핵심 데이터 스토리지와 구글 검색 엔진을 위해 최적화된 분산 파일 시스템이다.

10. 마스터(Master), 청크 서버(Chunk Server), 클라이언트로 구성된다.

- 마스터는 GFS 전체의 상태를 관리하고 통제한다.

- 청크서버는 물리적인 하드디스크의 실제 입출력을 처리한다.

- 클라이언트는 파일을 읽고 쓰는 동작을 요청하는 애플리케이션이다.

11. 파일들은 일반적인 파일 시스템에서의 클러스터들과 섹터들과 비슷하게 64MB로 고정된 크기의 청크들로 나누어서 저장된다.

12. 가격이 저렴한 서버에서도 사용되도록 설계되었기 때문에 하드웨어 안전성이나 자료들의 유실에 대해서 고려하여 설계되었고 응답 시간이 조금 길더라도 데이터의 높은 처리성능에 중점을 두었다.

# CAP 이론 - 기존 데이터 저장 구조의 한계

1. 2002년 버클리 대학의 에릭 브루어 교수가 발표한 이론

2. 분산 컴퓨팅 환경의 특징을 일관성(Consistency), 가용성(Availability), 지속성(Partition Tolerance) 세 가지로 정의 할 수 있는데, 어떤 시스템이든 이 세 가지 특성을 동시에 만족하기 어렵다.

3. 일관성 - 분산 환경에서 모든 노드가 같은 시점에 같은 데이터를 보여줘야 한다.

4. 가용성 - 일부 노드가 다운되어도 다른 노드에 영향을 주지 않아야 한다.

5. 지속성 - 데이터 전송 중에 일부 데이터를 손실하더라도 시스템은 정상 동작해야 한다.

6. RDBMS - 일관성 + 가용성 (트랜잭션 ACID 보장, 금융 서비스)

7. NoSql - 일관성 + 지속성 (대용량 분산 파일 시스템 - 성능 보장), 가용성 + 지속성 (비동기식 서비스 - 아마존, 트위터)

# NoSQL의 기술적 특성

1. 스키마 X(데이터 저장 방식은 크게 열, 값, 문서, 그래프 등의 네 가지를 기반으로 구분한다.), 탄력성(시스템 일부에 장애가 발생해도 클라이언트가 시스템에 접근 가능하다.), 질의 기능, 캐싱

# NoSQL의 데이터 모델

1. NoSQL의 데이터 저장 방식에 따라 키-값 구조, 칼럼기반 구조, 문서기반 구조로 구분할 수 있다.

2. 키-값 데이터베이스 - 데이터를 키와 그에 해당하는 값의 쌍으로 저장하는 데이터 모델에  기반을 둔다. (DynamoDB, Redis)

3. 열기반(컬럼기반) 데이터베이스 - 데이터를 로우가 아닌 컬럼기반으로 저장하고 처리하는 데이터베이스를 말한다. (Cassandra, BigTable, Hbase)

4. 문서기반 데이터베이스 - 문서 형식의 정보를 저장, 검색, 관리하기 위한 데이터베이스 (MongoDB, CouchDB)

# 데이터 관련 정의

1. 데이터 - 관심의 대상이 되는 사물이나 사건의 속성을 일정한 규칙에 의해 측정, 조사, 관찰하여 습득

2. 단위 - 관찰되는 항목 또는 대상을 지칭한다.

3. 관측값 - 각 조사 단위별 기록정보 또는 특성을 말한다.

4. 변수 - 각 단위에서 측정된 특성 결과이다.

5. 원자료 - 표본에서 조사된 최초의 자료를 이야기한다.

# 데이터의 종류

1. 단변량자료 - 자료의 특성을 대표하는 특성 변수가 하나인 자료이다.

2. 다변량자료 - 자료의 특성을 대표하는 특성 변수가 두 가지 이상인 자료이다.

3. 질적자료 - 정성적 또는 범주형 자료라고도 하며 자료를 범주의 형태로 분류한다.

- 분류의 편의상 부여된 수치의 크기자체에는 의미를 부여하지 않는 자료이며 명목자료, 서열자료 등 이질적자료로 분류된다.

- 명목자료 - 측정대상이 범주나 종류에 대해 구분되어지는 것을 수치 또는 기호로 분류되는 자료이다(국번)

- 서열자료 - 명목자료와 비슷하나 수치나 기호가 서열을 나타내는 자료이다(기록경기의 순위)

4. 수치자료 - 정량적 또는 연속형 자료라고도 한다. 숫자의 크기에 의미를 부여할 수 있는 자료를 나타내며 구간자료, 비율자료가 여기에 속한다.

- 구간자료 - 명목자료, 서열자료의 의미를 포함하면서 숫자로 표현된 변수에 대해서 변수 간의 관계가 산술적인 의미를 가지는 자료(온도)

- 비율자료 - 명목자료, 서열자료, 구간자료의 의미를 가지는 자료로서 수치화된 변수에 비율의 개념을 도입할 수 있는 자료(무게)

5. 시계열자료 - 일정한 시간간격 동안에 수집된, 시간개념이 포함되어 있는 자료(일별 주식 가격)

6. 횡적자료 - 횡단면자료라고도 하며 특정 단일 시점에서 여러 대상으로부터 수집된 자료이다. 즉 한 개의 시점에서 여러 대상으로부터 취합하는 자료

7. 종적자료 - 시계열자료와 횡적자료의 결합으로 여러 개체를 여러 시점에서 수집한 자료이다.

8. 데이터의 종류는 앞의 내용에서 변수들의 집합인 자료의 종류와 그 특성을 동일하게 가지므로 데이터의 종류에 따라서 적용방법론이 다양하게 변화할 수 있다.

# 데이터의 정제

1. 수집된 데이터를 대상으로 분석에 필요한 데이터를 추출하고 통합하는 과정이다.

2. 다양한 매체로부터 데이터를 수집, 원하는 형태로 변환, 원하는 장소에 저장, 저장된 데이터의 활용가능성을 타진하기 위한 품질확인, 필요한 시기와 목적에 따라 사용이 원활하도록 관리의 과정이 필요하다.

3. 수집 - 데이터의 입수 방법 및 정책 결정, 입수경로의 구조화, 집계, 저장소 결정

4. 변환 - 데이터 유형의 변환 및 분석 가능한 형태로 가공, ETL, 일반화, 정규화

5. 교정 - 결측치의 처리, 이상치 처리, 노이즈 처리, 비정형데이터 수집 시 필수사항

6. 통합 - 데이터분석이 용이하도록 기존 또는 유사데이터와의 연계 통합, 레거시 데이터와 함께 분석이 필요할 경우 수행

# 데이터 결측값 처리

1. 데이터 분석에서 결측치(결측값)는 데이터가 없음을 의미한다.

- 결측치를 임의로 제거 시 - 분석 데이터의 직접손실로 분석에 필요한 유의수준 데이터 수집에 실패할 가능성이 발생한다.

- 결측치를 임의로 대체 시 - 데이터의 편향(bias)이 발생하여 분석 결과의 신뢰성 저하 가능성이 있다.

2. 결측 데이터의 종류

- 완전 무작위 결측(MACR, Missing Completely At Random) - 어떤 변수상에서 결측 데이터가 관측된 혹은 관측되지 않는 다른 변수와 아무런 연관이 없는 경우이다.

- 무작위 결측(MAR, Missing At Random) - 변수상의 결측데이터가 관측된 다른 변수와 연관되어 있지만 그 자체가 비관측값들과는 연관되지 않은 경우이다.

- 비 무작위 결측(NMAR, Not Missing At Random) - 어떤 변수의 결측 데이터가 완전 무작위 결측 또는 무작위 결측이 아닌 결측데이터로 정의하는 즉, 결측변수값이 결측여부(이유)와 관련이 있는 경우이다.

```

나이대별(X), 성별(Y)과 체중(Z) 분석에 대한 모델링을 가정해보면

X, Y, Z와 관계없이 Z가 없는 경우 - 데이터의 누락(응답 없음) - 완전 무작위 결측(MACR)

여성(Y)은 체중 공개를 꺼려하는 경향 - Z가 누락될 가능성이 Y에만 의존 - 무작위 결측(MAR)

젊은(X) 여성(Y)의 경우는 체중 공개를 꺼리는 경우가 더 높음 - 무작위 결측(MAR)

무거운 or 가벼운 사람들은 체중 공개 가능성이 적음 - Z가 누락될 가능성이 Z값 자체에 관찰되지 않는 값에 달려 있음 - 비 무작위 결측(NMAR)

```

3. 결측값 유형의 분석 및 대치

- 결측치가 존재하는 데이터를 이용한 분석은 다음 세 가지 고려사항이 발생하는데 효율성문제, 자료처리의 복잡성, 편향 문제이다.

4. 단순 대치법

- 기본적으로 결측치에 대하여 MCAR or MAR로 판단하고 이에 대한 처리를 하는 방법

- 완전 분석 - 불완전 자료는 완전하게 무시하고 분석을 수행한다. 분석의 용이성을 보장하나 효율성 상실과 통계적 추론의 타당성에 문제 발생 가능성이 있다.

- 평균 대치법 - 관측 또는 실험으로 얻어진 데이터의 평균으로 결측치를 대치해서 사용한다. 평균에 의한 대치는 효율성의 향상 측면에서 장점이 있으나 통계량의 표준오차가 과소 추정되는 단점이 있다. 비조건부 평균대치법이라고도 한다.

- 회귀 대치법 - 회귀분석(regression)에 의한 예측치로 결측치를 대치하는 방법으로 조건부 평균 대치법이라고도 한다.

- 단순확률 대치법 - 평균 대치법에서 추정량 표준오차의 과소 추정을 보완하는 대치법으로 Hot-deck 방법이라고도 한다. 확률 추출에 의해서 전체 데이터 중 무작위로 대치하는 방법이다.

- 최근접 대치법 - 전체표본을 몇 개의 대체군으로 분류하여 각 층에서의 응답자료를 순서대로 정리한 후 결측값 바로 이전의 응답을 결측치로 대치한다. 응답값이 여러 번 사용될 가능성이 단점이다.

5. 다중 대치법

- 단순 대치법을 복수로 시행하여 통계적 효율성 및 일치성 문제를 보완하기 위하여 만들어진 방법이다.

- 1단계 - 대치단계 - 복수의 대치에 의한 결측을 대치한 데이터를 생성한다.

- 2단계 - 분석단계 - 복수 개의 데이터셋에 대한 분석을 시행한다.

- 3단계 - 결합단계 - 복수 개의 분석결과에 대한 통계적 결합을 통해 결과를 도출한다.

# 데이터 이상치 처리

1. 이상치(이상값, outlier)란 데이터의 전처리 과정에 발생 가능한 문제로 정상의 범주(데이터의 전체적 패턴)에서 벗어난 값을 의미한다.

2. 이상치의 종류

- 단변수 이상치 - 하나의 데이터 분포에서 발생하는 이상치를 말한다.

- 다변수 이상치 - 복수의 연결된 데이터 분포공간에서 발생하는 이상치를 의미한다.

3. 이상치의 발생 원인

- 비자연적 이상치 발생(Non-Natural Outlier)

- 입력실수 - 데이터의 수집과정에서 발생하는 에러로 입력 실수 등을 지칭한다.

- 측정오류 - 데이터의 측정중에 발생하는 에러로 측정기 고장으로 발생되는 문제이다.

- 실험오류 - 실험과정 중 발생하는 에러로 실험환경에서 야기된 모든 문제점을 지칭한다.

- 의도적 이상치 - 자기 보고 측정(Self-reported Measure)에서 발생되는 이상치를 지칭한다. 의도가 포함된 이상치로 예를 들어 남성의 키를 조사 시 의도적으로 키를 높게 기입하는 경우 등

- 자료처리오류 - 복수 개의 데이터셋에서 데이터를 추출,조합하여 분석 시 분석 전의 전처리에서 발생하는 에러를 말한다.

- 표본오류 - 모집단에서 표본을 추출하는 과정에서 편향이 발생하는 경우를 지칭한다.

- 상기 경우 이외에 발생하는 이상치들은 자연적 이상치(Natural Outlier)라고 한다.

4. 이상치의 탐지

- 종속변수가 단변량인지 다변량인지 데이터의 분포를 고려하여 모수적 or 비모수적인지에 따라 다양한 방법으로 고려해야 한다.

- 시각화를 통한 방법(비모수적, 단변량의 경우) - Box Plot, 줄기-잎 그림, 산점도 그림

- Z-Score 통한 방법(모수적 단변량 또는 저변량의 경우) - 정규화를 통해 threshold를 벗어난 경우를 이상치로 판별한다.

- 밀도기반 클러스터링 방법(DBSCAN, Density Based Spatial Clustering of Application with Noise) - 비모수적 다변랴으이 경우 군집간의 밀도를 이용하여 특정 거리 내의 데이터 수가 지정 개수 이상이면

군집으로 정의하는 방법이다. 정의된 군집에서 먼거리에 있는 데이터는 이상치로 간주한다.

- 고립 의사나무 방법(Isolation Forest) - 비모수적 다변량의 경우 의사결정나무(Decision Tree) 기반으로 정상치의 단말 노드(Terminal Node)보다 이상치의 노드에 이르는 길이(Path Length)가 더 짧은 성질을 이용하는 방법을 의미한다.

# 변수별 모형의 분류

1. 전체 모형(FM, Full Model) - 모든 독립변수를 사용한 모형

2. 축소 모형(RM, Reduced Model) - 전체 모형에서 사용된 변수의 개수를 줄여서 얻은 모형

3. 영 모형(NM, Null Model) - 독립변수가 하나도 없는 모형

# 변수의 선택 방법

1. 전진 선택법(Forward Selection) 

- 영 모형에서 시작, 모든 독립변수 중 종속변수와 단순상관계수의 절댓값이 가장 큰 변수를 분석모형에 포함시키는 것을 말한다.

- 부분 F 검정(F test)을 통해 유의성 검증을 시행, 유의한 경우는 가장 큰 F 통계량을 가지는 모형을 선택하고 유의하지 않은 경우는 변수선택 없이 과정을 중단한다.

- 한번 추가된 변수는 제거하지 않는다.

2. 후진 선택법(Backward Selection)

- 전체모델에서 시작, 모든 독립변수 중 종속변수와 단순상관계수의 절댓값이 가장 작은 변수를 분석모형에서 제외시킨다.

- 부분 F 검정(F test)을 통해 유의성 검증을 시행, 유의하지 않은 경우는 변수를 제거하고 유의한 경우는 변수제거 없이 과정을 중단한다.

- 한번 제거된 변수는 추가하지 않는다.

3. 단계적 선택법(Stepwise Selection)

- 전진 선택법과 후진 선택법의 보완방법이다.

- 전진 선택법을 통해 가장 유의한 변수를 모형에 포함 후 나머지 변수들에 대해 후진 선택법을 적용하여 새롭게 유의하지 않은 변수들을 제거한다.

- 제거된 변수는 다시 모형에 포함하지 않으며 유의한 설명변수가 존재하지 않을때까지 과정을 반복한다.

# 요인 분석

1. 다수의 변수들 간의 관계(상관관계)를 분석하여 공통차원을 축약하는 통계분석 과정이다.

2. 독립변수, 종속변수 개념이 없다. 주로 기술 통계에 의한 방법을 이용한다.

3. 주성분 분석(PCA), 공통요인 분석 특이값 분해(SVD), 음수미포함 행렬분해(NMF) 등이 있다.

4. 주성분 분석(PCA, Principal Component Analysis)

- 분포된 데이터들의 특성을 설명할 수 있는 하나 또는 복수 개의 특징(주성분, Principal Component)을 찾는 것을 의미한다.

- 서로 연관성이 있는 고차원공간의 데이터를 선형연관성이 없는 저차원(주성분)으로 변환하는 과정을 거친다(직교번환을 사용)

- 기존의 기본변수들을 새로운 변수의 세트로 변환하여 차원을 줄이되 기존 변수들의 분포특성을 최대한 보존하여 이를 통한 분석결과의 신뢰성을 확보한다.

- 데이터 하나하나에 대한 성분을 분석하는 것이 아니라, 여러 데이터들이 모여 하나의 분포를 이룰 때, 이 분포의 주성분을 분석해 주는 방법이라고 할 수 있다.

- v1의 방향과 크기, 그리고 v2의 방향과 크기를 알면 이 데이터분포가 어떤 형태인지를 가장 단순하면서도 효과적으로 파악할 수 있다.

- 스케일에 대한 영향이 크다. 즉 PCA 수행을 위해선 변수들 간의 스케일링이 필수이다.

5. 특이값 분해(SVD, Singular Value Decomposition)

- 데이터공간을 나타내는 m*n 크기의 행렬 M에 대해, 다음과 같이 분해 가능하다.

- M = U * 시그마 * V^t 

- 여기서 U는 m * m 크기의 직교행렬(Orthogonal Matrix)이고 시그마는 m*n 크기의 대각행렬(Diagonal Matrix), V^t는 n*n 크기의 전치행렬이다.

- 직교행렬 - 행렬의 열벡터가 독립이라는 의미..?!

- 대각행렬 - 행렬의 대각성분을 제외한 나머지행렬의 원소의 값이 모두 0인 행렬

6. 음수 미포함 행렬분해(NMF, Non-negative Matrix Factorization)

- 음수를 포함하지 않은 행렬 V를 음수를 포함하지 않은 두 행렬의 곱으로 분해하는 알고리즘

- 일반적으로 W의 열 개수와 H의 행 개수가 WH=V가 되도록 결정한다.

- 기존 행렬 V와 분해한 음수 미포함 행렬 W와 H의 곱과의 차이를 오차 U라고 이야기한다. V = WH + U

- 행렬 곱셈에서 곱해지는 행렬은 결과행렬보다 훨씬 적은 차원을 가지기 때문에 NMF가 차원을 축소할 수 있다.

# 파생변수

1. 데이터 마트(Data Mart)는 데이터 웨어하우스(Data Warehouse)로부터 복제 또는 자체 수집된 데이터모임의 중간층이지만 분석을 위한 기본단계변수가 모여지는 단계로 요약변수와 파생변수들의 모임이라고 볼 수 있다.

2. 기존의 변수를 조합하여 새로운 변수를 만들어 내는 것을 의미한다.

# 요약변수

1. 수집된 정보를 분석에 맞게 종합한 변수이다.

2. 데이터 마트에서 가장 기본적인 변수이다. 

3. 많은 분석 모델에서 공통으로 사용될 수 있어 재활용성이 높다.

# 변수 변환

1. 데이터를 분석하기 좋은 형태로 바꾸는 작업을 말한다.

2. 범주형 변환

- 연속형 변수 중에서 변수자체로의 분석보다는 분석결과의 명료성 및 정확성을 배가시키기 위해 범주형으로 바꾸는 것이 좋은 경우가 있다.

3. 정규화

- 분석을 정확히 하려면 원래 주어진 연속형(이산형) 데이터 값을 바로 사용하기 보다는 정규화를 이용하는 경우가 타당할 수 있다.

- 일반 정규화 - 수치로 된 값들을 여러 개 사용할 때 각 수치의 범위가 다르면 이를 같은 범위로 변환해서 사용하는데 이를 일반 정규화라고 한다.

- 최소-최대 정규화 - 데이터를 정규화하는 가장 일반적인 방법이다. 모든 feature에 대해 최소값 0, 최대값 1로 그리고 다른 값들은 0과 1 사이의 값으로 변환하는 것이다.

- 분포형태별 정규분포 변환 - 모집단의 분포형태별로 사용가능한 변수 변환이 다르다.

```

좌로 치우침 - X^3 

좌로 약간 치우침 - X^2

우로 약간 치우침 - 루트x

우로 치우침 - ln(x)

극단적 우로 치우침 - 1/X

```

# 불균형 데이터

1. 데이터의 양에 차이가 큰 경우, 클래스 불균형이 있다고 말한다.

# 불균형 데이터 처리 방법

1. 가중치 균형 방법 - 데이터에서 손실을 계산할 때 특정 클래스의 데이터에 더 큰 loss 값을 갖도록 하는 방법이다.

- 고정 비율 이용 - 클래스의 비율에 따라 가중치를 두는 방법

- 최적 비율 이용 - 분야와 최종 성능을 고려해 가중치 비율의 최적 세팅을 찾으면서 가중치를 찾아가는 방법

2. 언더샘플링과 오버샘플링 - 비대칭 데이터는 다수 클래스 데이터에서 일부만 사용하는 언더샘플링이나 소수 클래스 데이터를 증가시키는 오버샘플링을 사용하여 데이터 비율을 맞추면 정밀도가 향상된다.

- 언더샘플링 - 언더샘플링은 대표클래스의 일부만을 선택하고, 소수클래스는 최대한 많은 데이터를 사용하는 방법이다.

- 오버샘플링 - 소수클래스의 복사본을 만들어, 대표클래스의 수만큼 데이터를 만들어 주는 것이다.

# 탐색적 데이터 분석 (EDA, Exploratory Data Analysis)

1. 수집한 데이터가 들어왔을 때, 다양한 방법을 통해서 자료를 관찰하고 이해하는 과정을 의미하는 것으로 본격적인 데이터 분석 전에 자료를 직관적인 방법으로 통찰하는 과정이다.

# 탐색적 데이터 분석의 필요성

1. 데이터의 분포 및 값을 검토함으로써 데이터가 표현하는 현상을 이해하며 내재된 잠재적 문제에 대해 인식하고 해결안을 도출할 수 있다.

- 문제점 발견 시 본 분석 전 데이터의 수집 의사를 결정할 수 있다.

2. 다양한 각도에서 데이터를 살펴보는 과정을 통해 문제정의 단계에서 인지 못한 새로운 양상, 패턴을 발견할 수 있다.

- 새로운 양상을 발견 시 초기설정 문제의 가설을 수정하거나 또는 새로운 가설을 수립할 수 있다.

# 분석과정 및 절차

1. 분석의 목적과 변수가 무엇인지, 개별변수의 이름이나 설명을 가지는지 확인한다.

2. 데이터의 문제성을 확인한다. 즉, 데이터의 결측치의 유무, 이상치의 유무 등을 확인하고 추가적으로 분포상의 이상 형태 Head 또는 Tail 부분을 확인한다.

3. 데이터의 개별 속성값이 예상한 범위 분포를 가지는지 확인한다(기초통계산출을 통한 확인과정을 거친다.)

4. 관계속성 확인 절차를 가진다. 즉, 개별 데이터 간의 속성 관찰에서 보지 못한 데이터 간의 속성(상관관계 등)을 확인한다.

# 이상치

1. 이상치를 발견하는 기법 네 가지(개별 데이터 관찰, 통계값 활용, 시각화 활용, 머신러닝 기법 활용)

2. 개별 데이터 관찰

- 데이터 값을 눈으로 살펴보면서 전체적인 추세와 특이사항을 관찰할 수 있다.

3. 통계값 활용

- 적절한 요약 통계지표를 사용할 수 있다.

- 데이터의 중심을 알기 위해서는 평균(mean), 중앙값(median), 최빈값(mode)을 사용할 수 있다.

- 데이터의 분산도를 알기 위해서는 범위(range), 분산(variance)을 사용할 수 있다.

- 통계 지표를 이용할 때는 데이터의 특성에 주의해야 한다. 평균 = 이상값이 있으면 영향을 받음, 중앙값 = 이상값의 존재에도 대표성이 있는 결과를 얻을 수 있음

- 통계값 활용 방법 두 가지

```

1. IQR(Inter Quantile Range) 방법

전체 데이터들을 오름차순으로 정렬하고, 정확히 4등분으로 나눈다. 여기서 75% 지점의 값(3사분의 수)과  25% 지점의 값(1사분의 수)의 값의 차이를 IQR이라고 한다.

최댓값 = 3사분위수 + 1.5 * IQR

최솟값 = 1사분위수 - 1.5 * IQR

최댓값보다 크거나 최솟값보다 작은 값을 이상치로 간주한다.

2. 정규분포를 활용(평균과 분산을 이용한 이상치 제거)

평균과 표준편차의 차를 통해 정규분포의 평균과 표준편차 내의 데이터 분포 관계를 알 수 있다. 

```

4. 시각화 활용

- 시작적인 표현은 분석에 많은 도움을 준다. 시각화를 통해 주어진 데이터의 개별 속성에 어떤 통계 지표가 적절한지 결정할 수 있다.

- 시각화 방법에는 확률밀도 함수, 히스토그램, 점플롯(dot plot), 워드 클라우드, 시계열 차트, 지도 등이 있다.

5. 머신러닝 기법 활용

- 대표적인 머신러닝 기법으로 K-means를 통해 이상치를 확인할 수 있다.

# 변수 간의 상관성 분석

1. 두 변수 간에 어떤 선형적 관계를 갖고 있는지를 분석하는 방법이다. 두 변수는 서로 독립적인 관계이거나 상관된 관계일 수 있으며 이때 두 변수 간의 관계의 정도를 상관관계(correlation)라 한다.

2. 상관분석 - 2개 이상의 양적 변수간의 관계가 유의한지 확인하는 분석을 말한다.

3. 단순상관분석 - 단순히 두 개의 변수가 어느 정도 강한 관계에 있는가를 측정한다.

4. 다중상관분석 - 3개 이상의 변수 간의 관계강도를 측정한다.

5. 편상관관계분석 - 다중상관분석에서 다른 변수와의 관계를 고정하고 두 변수의 관계강도를 측정하는 것을 말한다.

# 상관분석의 기본가정

1. 선형성 - 두 변인 X와 Y의 관계까 직선적인지를 알아보는 것으로 이 가정은 분포를 나타내는 산점도를 통하여 확인할 수 있다.

2. 동변량성(등분산성, Homoscedasticity) - X의 값에 관계없이 Y의 흩어진 정도가 같은 것을 의미한다. 반의어는 이분산성(Heteroscedasticity)이다.

- 산포도가 특정 구간에 상관없이 퍼진 정도가 일정할 때 자료가 동변량성을 띤다고 얘기하며, 반대로 그 정도가 일정하지 않으면 이분산성을 보인다고 말한다.

3. 두 변인의 정규분포성 - 두 변인의 측정치 분포가 모집단에서 모두 정규분포를 이루는 것이다.

4. 무선독립표본 - 모집단에서 표본을 뽑을 때 표본대상이 확률적으로 선정된다는 것이다.

# 상관분석 방법

1. 피어슨 상관계수

- 두 변수 X와 Y간의 상관관계를 계량화한 수치이다.

- 피어슨 상관계쑤는 +1과 -1사이의 값을 가지며, +1은 완벽한 양의 선형 산관관계, 0은 선형 상관관계 없음, -1은 완벽한 음의 선형 상관관계를 의미한다.

2. 스피어만 상관계수

- 데이터가 서열자료인 경우, 즉 자료의 값 대신 순위를 이용하는 경우의 상관계수로서, 데이터를 작은 것부터 차례로 순위를 매겨 서열 순서로 바꾼 뒤 순위를 이용해 상관계수를 구한다.

- 두 변수 간의 연관 관계가 있는지 없는지를 밝혀 주며 자료에 이상점이 있거나 표본크기가 작을 때 유용하다.

- 크기 순으로 정한 두 변수의 차이가 클수록 스피어만 상관계수의 값은 커진다. 즉 스피어만 상관계수는 한 변수의 값이 커지면 다른 변수의 값도 단조적으로 커지는지를 알아볼 수 있다.

# 기초통계량의 추출 및 이해

1. 자료를 수집하여 요약, 정리하는 기초통계(또는 기술통계)는 자료의 ㅌ그성을 정량적인 수치에 의해서 나타내는 방법이다.

2. 자료의 특성을 수치적 결과로 나타내는 방법인 중심화 경향, 퍼짐 정도(산포도, 분포도), 자료의 분포형태(왜도, 첨도) 등으로 나타낼 수 있다.

3. 중심화 경향 기초 통계량(산술 평균, 기하 평균, 조화 평균, 중앙값, 최빈값, 분위수)

- 산술 평균 - 모든 자료들을 합한 후 전체 자료수로 나누어 계산하는 일반적인 평균

- 기하 평균 - N개의 자료에 대해서 관측치를 곱한 후 n 제곱근으로 표현한다. 다기간의 수익률에 대한 평균 수익률, 평균 물가상승률을 구할 때 사용한다.

- 조화평균 - 각 요소의 역수의 산술평균을 구한 후 다시 역수를 취하는 형태로 표현한다. 변화율 등의 평균을 구할 때 사용한다.

- 중앙값 - 자료를 크기 순으로 나열할 때 가운데에 위치한 값

- 최빈값 - 가장 노출 빈도가 높은 자료

- 분위수 - 분위수는 자료의 위치를 표현하는 수치. 자료를 크기순서대로 배열을 한 후 그 자료를 분할하는 역할을 하는 위치의 수치를 계산한 것 

4. 산포도(분산도, Degree Dispersion)

- 자료의 퍼짐 정도를 나타내는 기초 통계량(분산, 표준편차, 범위, 평균 절대편차, 사분위범위)

- 분산(Variance), 표준편차(Standard Deviation)

- 분산은 평균을 중심으로 밀집되거나 퍼짐 정도를 나타내는 척도이고, 표준편차는 분산의 제곱근으로 표현한다.

- 분산은 제곱한 단위를 사용하게 되는데, 분산으로 얻은 수치를 해석하기가 곤란하다는 단점을 보완하기 위하여 제곱근을 취한 척도가 표준편차이다. 수리적으로 다루기 쉽다.

- 범위 - 데이터 간의 최댓값과 최솟값의 차이를 나타내는 것으로 동일한 범위를 갖더라도 자료의 분포모양은 다를 수가 있음을 유의해야 한다.

- 평균 절대 편차(MAD, Mean Abosolute Deviation) - 각 자료값과 표본평균과의 편차의 절댓값에 대한 산술평균을 의미한다. 절댓값을 사용하여 수리적으로 다루기 부적절하다.

- 사분위범위(Inter Qunatile Range) - 자료를 크기 순으로 배열 후 1/4에 해당하는 1사분위수(Q1)를 구하고 3/4에 해당하는 3사분위수(Q3)를 구한다. 사분위범위는 Q3-Q1로 정의되며 자료의 50% 범위 내에 위치하게 됨을 의미한다.

- 변동계수(CV, Coefficient of Variance) - 평균을 중심으로 한 상대적인 산포의 척도를 나타내는 수치이다. 변동계수가 클수록 상대적으로 넓게 분포한다. CV = 표준편차 / 모평균 * 100%

5. 자료의 분포형태(Shape of Distribution)

- 왜도(Skewness) - 분포가 어느 하쪽으로 치우친 정도를 나타내는 통계적 척도이다. 오른쪽으로 더 길면 양의 값, 왼쪽으로 더 길면 음의 값이 된다.

- 피어슨의 비대칭 계수 - Cs = 3*(평균 - 중앙값) / 표준편차, Cs값이 양수면 오른쪽으로 긴 꼬리, Cs값이 음수면 왼쪽으로 긴 꼬리 

- 첨도(Kurtosis) - 분포의 뾰족한 정도를 나타내는 통계적 척도이다. 첨도의 값이 3미만이면 평평한 분포, 3이면 정규분포, 3이 넘는 경우는 뾰족한 분포

# 통계적 시각화 도구

1. 도수분포표 - 수집된 자료를 적절한 계급에 의해 분류하여 정리한 표로 질적자료의 경우는 각 자료값(범주)에 대하여 도수나 상대도수로 표현한다.

2. 히스토그램 - 도수분포표를 이용하여 표본의 자료분포를 나타낸 그래프이다.

3. 막대그래프 - 각 자료값에 대한 도수 또는 상대도수를 그림을 표현한 것이다.

4. 파이차트 - 각 자료값의 상대도수로 기입하여 원의 면적에 각 상대 크기별로 나타낸 그래프이다.

5. 산점도 - 직교 좌표계를 이용해 두 개 변수 간의 관계를 나타내는 방법이다.

6. 줄기 잎 그림(Stem-and-Leaf Diagram) - 통계적 자료를 표 형태와 그래프 형태의 혼합된 방법으로 나타내는 것을 말한다.

7. 상자 수염 그림(Box Plot) - 수치적 자료를 표현하는 그래프이다. 5가지 요약 수치를 가지고 그린다.(최솟값, 제1사분위, 제2사분위, 제3사분위, 최댓값)

# 시공간 데이터의 개념

1. 기본적으로 공간적 정보(데이터)에 시간의 흐름(이력정보 등)이 결합된 다차원 데이터를 다루는 것을 지칭한다.

2. 시간 데이터

- 기존 데이터는 어느 한 시점에 대한 스냅샷 정보이다. 그래서 데이터에 유효 시간, 거래 시간, 사용자 정의 시간과 같은 연관된 시간 표현을 정의한다.

- 유효 시간 - 데이터가 발생하거나 소멸된 시간

- 거래 시간 - 관리 시스템을 통해 처리된 시간

- 사용자 정의 시간 - 유효 시간이나 거래 시간이 없는 경우 사용자가 정의

- 스냅샷 데이터 - 시간 개념이 필요하지 않아 거래, 유효시간 미지원

- 유효 시간 데이터 - 유효 시간 지원

- 거래 시간 데이터 - 거래 시간 지원

- 이원 시간 데이터 - 거래 시간, 유효 시간 지원

3. 공간 데이터

- 기존 데이터베이스보다 복잡하고 다양한 유형의 값을 갖는 공간 데이터를 효율적으로 관리, 저장, 이용하는 데 초점을 맞춘다.

- 비공간 타입 - 기본적인 데이터 유형을 가진 속성

- 래스터 공간 타입 - 실세계에 존재하는 객체의 이미지

- 벡터 공간 타입 - 점, 선, 면 등의 요소로 구성

- 기하학적 타입 - 벡터 타입의 요소로부터 거리, 면적, 길이 등과 같은 유클리드 기하학 계산 값으로 표현

- 위상적 타입 - 공간 객체 간의 관계를 표현하며, 방위, 공간, 공간 객체 간의 중첩, 포함, 교차, 분리 등과 같은 위치적 관계로 일반적으로 처리 시, 대량의 공간을 필요로 해서 일반적으로 저장되지 않고 보통 공간객체로부터 동적으로 계산

4. 공간 데이터 모델

- 관계형 모델 - 데이터의 표현이 유연하지 못하며 실세계 공간의 객체의 특징을 적절히 표현하지 못하는 문제점이 있다.

- 객체지향 모델 

```

비 구조적이고 복잡한 데이터를 자연스럽게 표현한다.

데이터 계층 구조를 이용한 연산이 쉽다.

새로운 함수의 확장이 쉽다.

데이터 무결성 검사가 쉽다.

설계 단계 모델-구현 단계 모델 사이의 불일치 문제를 줄인다.

```

- 시공간 데이터 - 시간과 공간 데이터의 결합 형태를 지칭한다.

```

실제 객체들은 공간적 정보뿐만 아니라 시간적 정보와도 연관이 있다. 

기본적으로 위치, 영역과 같은 공간 정보는 시간의 흐름에 따라서 변화를 하기 때문이다.

```

# 시공간 데이터 분석

1. 시공간 데이터에 대한 질의어

- 시공간자료 정의언어 - 시공간 테이블 인덱스 및 뷰(view)의 정의문, 변경문 등이 포함되어 있다. 이 자료는 공간적 속성과 시간적 속성을 동시에 포함하며 시공간 테이블의 정의문은 점, 선, 면 등의 공간속성 타입이 추가되어 있다.

- 시공간자료 조작언어 - 객체의 삽입, 삭제, 변경 등의 검색문이 있다. 이 문장들은 시간지원 연산자와 공간 연산자를 포함하며 이를 통해 객체에 대한 공간관리와 이력정보를 제공한다.

2. 시공간 데이터의 연산

- 시공간위상 관계연산 - 공간위상 연산자는 두 객체 간 공간영역상의 관계에 대해서 참, 거짓을 반환하는 연산으로 대표적으로 교차 연산자는 선과 선의 교차, 선과 면의 교차 여부를 반환하며 

시간 관계의 경우는 두 객체의 유효시간 정보를 기반으로 선후관계를 평가하여 참, 거짓을 반환하는 연산자이다.

- 시공간기하 연산 - 공간기하 연산자 + 시간구성 연산자 결합으로 표현한다. 공간기하 연사자는 두 객체 간의 거리 연산을 지칭하며 시간 구성 연산자는 주어진 객체의 유효시간값에 대하여

지정된 시간 혹은 다른 객체의 유효시간값과의 계산을 통해 객체의 유효시간값을 변경하는 연산이다.

3. 적용 및 응용분야

- 시공간 데이터 기술은 지리정보 시스템, 위치 기반 서비스, 차량 위치추적 서비스 등에 활용된다.

# 종속변수와 독립변수 사이의 인과 관계

1. 다중회귀

- 독립변수가 2개 이상인 회귀모형을 지칭하며 각 독립변수는 종속변수와 선형관계에 있음을 가정한다.

```

장점 

- 변수를 추가하여 분석내용의 질적 향상을 도모할 수 있다. (단순회귀분석의 단점을 극복할 수 있다)

- 종속변수를 설명하는 독립변수가 두 개일 때 단순회귀모형을 설정한다면 모형설정이 부정확할 뿐 아니라 종속변수에 대한 중요한 독립변수를 누락함으로써

계수 추정량에 대해 편이(bias)를 야기시킬 수 있다. 이때 다중회귀분석을 통해 편이를 제거할 수 있다.

기본가정

- 회귀모형은 모수에 대해 선형인 모형이다.

- 오차항(관측치와 모예측치 간 편차 차이)의 평균은 0이다.

- 오차항의 분산은 모든 관찰치에 대해 6^의 일정한 분산을 갖는다.

- 서로 다른 관찰치 간의 오차항은 상관이 없다 (오차항은 서로 독립이며 공분산은 0)

- 오차항의 각 독립변수 역시 독립인 관계이다.

- 오차항은 정규분포를 따르며 N(0, 6^)이다.

분석 방법

- 최소자승법을 이용하여 결과를 도출할 수 있다.

```

2. 로지스틱 회귀(Logistic Regression)

- 독립 변수의 선형 결합을 이용하여 사건의 발생 가능성을 예측하는 데 사용되는 통계 기법이다.

- 종속변수가 이항형 문제(유효한 범주의 개수가 두 개인 경우)를 지칭할 때 사용된다.

```

특징

로지스틱 회귀의 모델은 종속 변수와 독립 변수 사이의 관계에 있어서 선형모델과 차이점을 지니고 있다.

첫 번째 차이점은 이항형인 데이터에 적용하였을 때 종속 변수 y의 결과가 범위[0, 1]로 제한된다는 것이고

두 번째 차이점은 종속 변수가 이진적이기 때문에 조건부 확률 P(Y|X)의 분포가 정규분포 대신 이항 분포를 따른다는 것이다.

독립 변수는 실제 값, 이진 값, 카테고리 등 어떤 형태든 될 수 있다.

종속 변수의 형태는 연속 변수(수입, 나이, 혈압) 또는 이산 변수(성별, 인종)로 구분된다.

만약, 특정 이산 변수 값의 후보가 2개 이상 존재한다면 일반적으로 해당 후보들을 임시 변수로 변환하여 로지스틱 회귀를 수행한다.

```

3. 분산분석(ANOVA, Analysis of Variance)

- 분산분석은 3개 이상의 표본들의 차이를 표본평균 간의 분산과 표본 내의 관측치간 분석을 비교하여 가설을 검증하는 것이다.

- 일원분산분석(One-Way ANOVA) - 단 하나의 인자에 근거하여 여러 수준으로 나누어지는 분석이다.

```

일원분산분석의 특징

일원분산분석은 단일용인변수(독립변수)에 의해 종속변수에 대한 평균치의 차이를 검정하는 데 이용한다.

일원분산분석을 위해서는 종속변수(등간 척도)와 정수값을 갖는 요인변수가 각 하나여야 하고 요인변수가 정의되어야 한다.

EX) 3학급(A반, B반, C반) 간 성적의 평균 차이가 존재할 것이다.

EX) 판매방법이나 지역에 따라 자사 매출액 평균에 차이가 존재하는가?

```

4. 다변량 분산분석(Multi Variate ANOVA)

- 측정형 변수, 종속 변수가 2개 이상인 분산분석이다.

- 이원분산분석(Two-way ANOVA) - 두 개 이상의 인자에 근거하여 여러 수준으로 나누어지는 분석이다.

- 이원분산분석의 특징

```

이원분산분석은 일원분산분석과는 달리 독립변인의 수가 둘이다.

만약 연구자의 관심이 한 변수에 따른 종속변수의 영향이 아니라 두 개 이상의 변수,

예를 들어 성별변수와 연령변수에 따라 직무만족도가 어떻게 차이나는가를 알아보고자 한다면 이원분산분석을 해야 한다. <== 독립변수가 두 개인 것 아닌가..? 종속 변수가 두 개 인거..?

```

# 변수축약

1. 변수들간의 상관관계를 이용하여 변수를 줄이는 방법으로 변수유도기법이라고도 한다.

2. 주성분분석(PCA, Principal Component Analysis)

- 다변량자료에서 존재하는 비정규성(abnormality)이나 이상치(outlier)를 발견하기 위하여 변수들의 상관관계(또는 공분산)가 존재하지 않는 새로운 변수(주성분)를 구하는 것을 지칭한다.

- 공분산(covariance) - 두 개의 확률 변수의 상관정도를 나타내는 값

- 주성분 분선은 N개의 변수로부터 서로 독립인 K( < N)개의 주성분을 구해 원 변수의 차원을 줄이는 방법이다.

3. 요인분석(Factor Analysis)

- 다수의 변수들의 상관관계를 분석하여 공통차원들을 통해 축약해 나가는 방법으로 이해하면 된다.

- 즉, 다수의 변수들 간 정보손실을 최소화하면서 소수의 요인(Factor)으로 축약하는 것이다.

```

요인분석의 특징

독립변수와 종속변수의 개념이 없다.

추론통계가 아닌 기술통계기법에 의해 수행할 수 있다(상관분석 등)

요인분석의 목적

변수축소 - 여러 개의 관련변수가 하나의 요인으로 묶인다

변수제거 - 요인에 포함되지 않거나 포함되더라도 중요도가 낮은 변수를 찾을 수 있다.

변수특성파악 - 관련된 변수들의 묶음으로 상호독립특성을 파악하기 용이해진다.

측정항목의 타당성 평가 - 그룹이 되지 않은 변수의 특성을 구분할 수 있게 된다.

요인점수를 통한 변수생성 - 회귀분석, 군집분석, 판별분석 등에 적용 가능한 변수를 생성할 수 있다.

```

4. 정준상관분석(Canocical Analysis)

- 두 변수집단 간의 연관성(Association)을 각 변수집단에 속한 변수들의 선형결합(Linear Combination)의 상관계수를 이용하여 분석하는 방법이다(일반화된 상관계수)

- 정준변수(Canonical Variable) - 새로 만들어진 선형결합이다.

- 정준상관계수(Canonical Correlation Coefficient) - 정준변수들 사이의 상관계수이다.

- 두 집단에 속하는 변수들의 개수 중에서 변수의 개수가 적은 집단에 속하는 변수의 개수만큼의 정준변수가 만들어질 수 있다.

- 정준분석과 회귀분석의 차이 - 회귀분석의 경우 하나의 반응변수를 여러 개의 설명변수로 설명하고자 할 때, 가장 설명력이 높은 변수들의 선형결합을 찾아 이들 사이의 인과관계를 생각하는 반면에 정준분석에는 이와 같은 인과성이 없다.

# 개체유도

1. 개체들의 특성을 측정한 변수들의 상관관계를 이용하여 유사한 개체를 분류하는 방법이다.

2. 군집분석(Cluster Analysis)

- 변수 또는 개체(item)들이 속한 모집단 또는 범주에 대한 사전정보가 없는 경우에 관측값들 사이의 거리(또는 유사성)를 이용하여 변수 또는 개체들을 자연스럽게 몇 개의 그룹 또는 군집(cluster)으로 나누는 분석기법이다.

- 군집 간의 거리에 대한 정의가 가장 중요한 부분으로 거리의 정의에 따라서 유사성에 대한 척도가 형성된다.

- 계층적(hierarchical) 방법 - 가까운 개체끼리 차례로 묶거나 멀리 떨어진 개체를 차례로 분리해 가는 군집방법으로 한 번 병합된 개체는 다시 분리되지 않는 것이 특징이다.

- 비계층적 방법(nonhirerarchical) 방법 또는 최적분화(partitioning) 방법 - 다변량 자료의 산포를 나타내는 여러가지 측도를 이용하여 이들 판정기준을 최적화시키는 방법으로 군집을 나누는 방법이다.

한 번 분리된 개체도 반복적으로 시행하는 과정에서 재분류될 수 있는 것이 특징이다.

- 조밀도에 의한 방법 - 데이터가 분포한 특성에 따라 군집을 나누는 방법이다.

- 그래프를 이용하는 방법 - 다차원 자료들을 2차원 또는 3차원으로 축소할 수 있다면 시각적 차원에서 자연스러운 군집을 형성할 수 있다.

3. 다차원 척도법(MDS, Multi Demensional Scaling)

- 다차원 척도법은 다차원 관측값 또는 개체들 간의 거리(distance) 또는 비유사성(dissimilarity)을 이용하여 개체들을 원래의 차원보다 낮은 차원(보통 2차원)의 공간상에 위치시켜(spatial configuration) 개체들 사이의 구조 또는 관계를 쉽게 파악하고자 하는 데 목적이 있다.

- 차원의 축소와 개체들의 상대적 위치 등을 통해 개체들 사이의 관계를 쉽게 파악하고, 공간적 배열에 대한 주관적인 해석에 중점을 두고 있다.

4. 판별 분석(Discriminant Analysis)

- 2개 이상의 그룹으로 나누어진 개체에 대해 분류에 영향을 미칠 것 같은 특성(변수)을 측정하고 이를 이용하여 새로운 개체를 분류하는 방법이다.

- 로지스틱 판별분석(Logistic Discriminant Analysis) - 분류를 하는 도구(판별식)를 로지스틱 회귀분석을 이용하여 분류하는 방법이다.

# 비정형 데이터의 분석

## 데이터 마이닝(Data Mining), 텍스트 마이닝(Text Mining), 오피니언 마이닝(Opnion Mining), 웹 마이닝(Web Mining)

1. 데이터 마이닝(Data Mining)

- 대규모로 저장된 데이터 안에서 체계적이고 자동적으로 통계적 규칙이나 패턴을 분석하여 가치 있는 정보를 추출하는 과정이다.

- 데이터 마이닝은 통계학에서 패턴 인식에 이르는 다양한 계량 기법을 사용한다.

- 데이터 마이닝 기법은 통계학 쪽에서 발전한 탐색적 자료분석, 가설 검정, 다변량 분석, 시계열 분석, 일반선형모형 등의 방법론이 쓰인다.

- 데이터베이스 쪽에서 발전한 OLAP, 인공지능 진영에서 발전한 SOM(Self-Organizing Map, 자기조직화 지도), 전문가 시스템 등의 기술적인 방법론이 쓰인다.

2. 데이터 마이닝 적용 분야

- 신용평점 시스템의 신용평가모형 개발, 사기탐지 시스템, 장바구니 분석, 최적 포트폴리오 구축과 같이 다양한 산업 분야에서 광범위하게 사용되고 있다.

- 분류(Classification) - 일정한 집단에 대한 특정 정의를 통해 분류 및 구분을 추론한다. (경쟁자에게로 이탈한 고객)

- 군집화(Clustering) - 구체적인 특성을 공유하는 군집을 찾는다. 군집화는 미리 정의된 특성에 대한 정보를 가지지 않는다는 점에서 분류와 다르다 (유사 행동 집단의 구분)

- 연관성(Association) - 동시에 발생한 사건 간의 관계를 정의한다. (장바구니에 동시에 들어가는 상품들의 관계 규명)

- 연속성(Sequencing) - 특정 기간에 걸쳐 발생하는 관계를 규명한다. 기간의 특성을 제외하면 연관성 분석과 유사하다 (슈퍼마켓과 금융상품 사용에 대한 반복 방문)

- 예측(Forecasting) - 대용량 데이터집합 내의 패턴을 기반으로 미래를 예측한다.

3. 데이터 마이닝의 단점

- 자료에 의존하여 현상을 해석하고 개선하려고 하기 때문에 자료가 현실을 충분히 반영하지 못한 상태에서 정보를 추출한 모형을 개발할 경우 잘못된 모형을 구축하는 오류를 범할 수가 있다.

4. 텍스트 마이닝(Text Mining)

- 전통적인 데이터 마이닝의 한계를 벗어난 방법으로 인간의 언어로 이루어진 비정형 텍스트 데이터들을 자연어 처리(Natural Language Processing) 방식을 이용하여 대규모 문서에서 정보 추출, 연계썽 파악, 분류 및 군집화, 요약 등을 통해 데이터의 숨겨진 의미를 발견하는 기법이다.

- 자연어 처리(NLP, Natural Language Process) - 인간의 언어 현상을 컴퓨터와 같은 기계를 이용해서 모사할 수 있도록 연구하고 이를 구현하는 인공지능의 주요 분야 중 하나다.

## 오피니언 마이닝

1. 오피니언 마이닝은 텍스트 마이닝의 한 분류로서, 특정 주제에 대한 사람들의 주관적 의견을 통계, 수치화해 객관적 정보로 바꾸는 빅데이터 분석기술이다.

- 텍스트 마이닝과 같이 문장을 분석하기 때문에 자연어 처리 방법(NLP)을 사용하지만, 텍스트 마이닝은 문장 내 주제를 파악하고 오피니언 마이닝은 감정, 뉘앙스, 태도 등을 판별한다는 차이가 있다. 이 때문에 감정 분석이라고도 불린다.

### 적용

- 텍스트 내 정보를 파악하기 위해 문장 구조, 문장 간의 관계, 어휘 등을 분석해 키워드와 연관된 감성 어휘의 빈도를 중립, 긍정, 부정으로 분류하고 그 강도를 평가한다.

## 웹 마이닝(Web Mining)

1. 웹 마이닝 또는 웹 데이터 마이닝은 일반적으로 웹 자원으로부터 의미있는 패턴, 추세 등을 도출해 내는 것을 지칭한다.

- 기기 내 쌓이는 로그, 사용자 행동 및 작성 콘텐츠 등 모든 것을 포함한다. 이러한 데이터를 분석하여 유용한 정보를 추출, 통찰(insight)을 얻어 내는 것이 핵심이다.

### 웹 마이닝의 특징

- 웹 환경에서 얻어지는 고객의 정보, 특정 행위, 패턴 등의 정보를 이용하여 다양한 활동(마케팅 등)에 활용할 수 있다.

### 웹 마이닝의 유형

- 웹구조 마이닝(Web Structure Mining) - 웹 사이트로부터 구조적 요약정보를 추출하는 것이다.

- 웹내용 마이닝(Web Contents Mining) - 웹사이트 또는 페이지로부터 의미 있는 내용을 추출하는 것을 말한다.

- 웹사용 마이닝(Web Usage Mining) - 웹상의 사용자의 행동 등 패턴으로부터 통찰을 이끌어 내는 방법을 말한다.

# 기술통계

1. 기술통계(Descriptive Statistics)는 분석에 필요한 데이터를 요약하여 묘사, 설명하는 통계기법을 말한다.

2. 분석을 위해서 단순히 데이터를 정리하는 행위 자체는 의미가 없다.

3. 분석 전 데이터의 특성을 찾아내서 그 특성의 정량화를 통한 체계적 요약이 필요하다.

## 데이터 요약

1. 분석대상이 되는 데이터의 단순 정리가 아닌 데이터의 분포가 가지는 특성을 찾아내서 본격적인 분석 이전에 기본적인 특징을 수치적으로 정량화하여 기술한다.

2. 주로 기초 통계량(중심화 경향, 분산도 경향, 자료의 분포형태)을 산출하여 결과를 도출한다.

# 표본추출

1. 모집단(Population) - 연구, 실험의 결과가 일반화된 큰 집단, 정보를 얻고자 하는 관심 대상의 전체집합으로 정의한다.

2. 표본(Sample) - 여러 자료를 포함하는 모집단 속에서 그 일부를 끄집어 내어 조사한 결과로 원래 집단의 성질을 추측할 수 있는 자료로 정의한다.

3. 표본추출(Sampling) - 모집단으로부터 표본을 선택하는 행위(과정)를 말한다.

# 표본추출 오차(Sampling Bias, Sampling Error)

1. 표본에서 선택된 대상이 모집단의 특성을 과잉 대표하거나 최소 대표할 때 발생한다.

2. 과잉 대표 - 중복선택 등의 원인으로 모집단이 반복, 중복된 데이터만으로 규정되는 현상을 지칭한다.

3. 최소 대표 - 실제모집단의 대표성을 나타낼 표본이 아닌 다른 데이터가 표본이 되는 현상이다.

4. 표본추출 시 표본의 크기보다는 **대표성**을 가지는 표본을 추출하는 것이 중요하다.

## 확률 표본추출 기법

1. 모집단에 속하는 모든 추출단위에 대해 사전에 일정한 추출확률이 주어지는 표본추출법이다.

2. 모든 표본들의 추출확률을 사전에 알 수 있다.

3. 표본자료로부터 얻어지는 추정량의 통계적 정확도를 확률적으로 나타낼 수 있다.

4. 단순무작위 추출(Simple Random Sampling) - 통계조사에서 가장 기본이 되는 표본추출법이다.

- 모집단으로부터 무작위(randomly) 추출하고 독립적 선택으로 편향성(bias)을 제거하여 난수(Random Number)를 이용하는 것이 기본이다.

- 모집단내의 조사단위수(N)를 파악한 다음 원하는 표본수(n) 만큼 난수를 발생시키고 그 수에 해당되는 조사단위를 표본으로 선택하는 방법이다.

5. 계통추출(Systematic Sampling)

- 모집단에서 추출간격(Sampling Interval)을 설정하여 간격 사이에서 무작위로 추출하는 방법이다.

- 만일 전체 모집단이 N개인 집단에서 K(단, K < N)라는 추출간격으로 뽑는다면 N/K 수만큼 표본이 선택될 수 있다.(1/K 계통추출법).

6. 층화추출(Stratified Sampling)

- 모집단을 서로 겹치지 않게 여러 층(strata)으로 나누어 분할된 층(stratum)별로 배정된 표본을 단순 임의 추출법에 따라 추출하는 방법이다.

- 층 - 관심을 갖고 있는 집단, 각 집단 내에 있는 추출단위들이 유사하도록 구성

- 각 집단별 분석이 필요한 분석의 경우나 모집단 전체에 대한 특성치의 효율적 추정(추론)이 필요한 경우 시행한다 (EX - 모집단 남녀 성비 3:2, 표본의 성비 3:2)

### 특징

- 단순임의추출법에 비해 추정의 정도를 높일 수 있다.

- 전체 모집단에 대한 추정뿐만 아니라 각 층별 추정결과도 얻을 수 있다.

- 모집단을 효과적으로 층화할 경우 임의표본에서 구한 추정량보다 오차가 적게 되어 추정의 정도를 높일 수 있다.

- 표본의 대표성 제고 및 조사관리가 편리하고, 조사비용이 절감된다.

### 층화변수(Stratification Variable)

- 모집단을 몇 개의 층으로 나누려고 할 때 각 추출단위가 어느 층에 속하는지를 구분하기 위해 기준으로 사용되는 변수이다.

- 사전에 모집단 단위들의 정보를 쉽게 알 수 있으면서도 조사하고자 하는 주 변수와 밀접한 관련이 있는 보조 변수가 되어야한다.

- 질적 층화변수 - 변수값에 따라 층 구분

- 양적 층화변수 - 층의 경계점을 나누는 방법 필요

- 층화변수가 양적 층화변수인 경우 층의 최적경계점(optimum point of stratification)

- 모집단을 n 개의 층으로 나누려면 n-1개의 경계점을 결정해야 함

- 추정값의 분산을 최소화시킬 수 있도록 경계점 결정 (여론조사에서 층화변수의 선택시 성별, 지역, 연령, 학령 등을 기준으로 할 수 있다.)

### 표본의 배분

- 각 층 내의 추출단위들의 수 - 많으면 크게 늘림 <==  왜? (비례배분, 양을 맞추기 위해서)

- 각 층 내에서 변동의 정도 - 변동의 정도가 커지면 크게 늘림 <== 왜? (네이만배분, 변동이 큰 층에 대해서 변동양을 줄이기 위해?)

- 각 층에서 추출단위를 조사하는데 드는 비용 - 비용증가 시 줄임

### 표본 배분의 방법 예시

- 비례배분법 - 각 층 내의 추출단위 수에 비례하여 표본크기를 배분하는 방법

- 네이만배분법 - 각 층의 크기와 층별 변동의 정도를 동시에 고려한 표본배정 방법, 변동이 큰 층에 대해서는 상대적으로 많은 표본을 배정

- 최적배분법 - 추정량의 분산을 최소화 시키거나 주어진 분산위 범위 하에서 비용을 최소화 시키는 방법

7. 군집추출(Cluster Sampling)

- 모집단을 차이가 없는 여러 개 군집으로 나누어 군집의 단위의 일부 또는 전체에 대한 분석을 시행한다.

- 모집단에 대한 구체적인 추출 방법론을 정하기 어려운 경우 사용하면 편리하다.

# 비확률 표본추출 기법

1. 각 추출단위들이 표본에 추출될 확률을 객관적으로 나타낼 수 없는 표본추출법이다.

2. 비용, 시간, 조사의 편리함 때문에 자주 사용한다.

3. 간편추출법(편의추출법, Convenience Sampling)

- 응답자를 선정하는 데 있어서 조사원 개인의 자의적인 판단에 따라 간편한 방법으로 표본을 추출하는 방법이다. (어떤 특정장소를 지나가는 사람들을 대상으로 여론조사를 하는 경우)

4. 판단추출법(Judgement Sampling)

- 조사자가 나름의 지식과 경험에 의해 모집단을 가장 잘 대표한다고 여겨지는 표본을 주관적으로 선정하는 방법이다.

5. 할당추출법(Quota Sampling)

- 조사목적과 밀접하게 관련되어 있는 조사대상자의 연령이나 성별과 같은 변수값에 따라 모집단을 부분집단으로 구분하고, 모집단의 부분집단별 구성비율과 표본의 부분집단별 구성비율이 유사하도록 표본을 선정하는 방법이다.

- (어느 대학에서 학생 서비스 만족도를 조사하고자 한다면 기존의 자료에 의거하여 각 학과별, 학년별, 성별 구성비율을 알아본 다음, 그 비율에 따라 표본을 학과별, 학년별, 성별로 할당)

6. 눈덩이추출법(Snowball Sampling)

- 접근이 어렵거나 추출틀(Sampling Frame)의 작성이 곤란한 특정한 집단에 대한 조사에서 사용되는 방법이다. (폭력조직원의 약물 실태 조사)

# 확률분포

1. 기술통계 - 분석에 필요한 데이터를 요약하고 묘사, 설명하는 통계기법이다.

2. 추론통계 - 표본에 내포되어 있는 정보를 이용하여 모집단에 과학적인 추론을 하는 통계기법이다.

# 표본공간

1. 통계적 실험에서 모든 발생 가능한 실험결과들의 집합을 의미한다.

2. 표본공간이 S인 확률 실험에서 사건은 S의 부분집합이 된다.

# 총확률정리

1. 총확률정리는 임의의 사건B의 확률을 k개의 조건부 확률을 이용해서 구하는 것이다.

2. 사전에 표본공간은 상호 배타적인 사건으로 분할적인 사건으로 분할되었다고 하면 임의의 사건 P(B)는 아래와 같이 표현 가능하다.

```

표본공간이 상호 배타적인 사건 A1, A2, ..., Ak로 분할될 때

P(B) = 시그마 i=1, k까지 P(B|Ai)P(Ai)

```

# 베이지안 정리

1. 총확률정리를 이용하여 임의의 사건 B의 확률을 k개의 조건부 확률을 이용해 계산하면 베이지안 법칙(Bayes' Theory)을 이용하여 표본공간을 분할하는 k개의 상호 배타적인 사건 A1, A2, ..., Ak에 대한 사후확률(Posterior Probablility)을 구할 수 있다.

2. P(Aj)는 미리 주어진 사전확률(Prior Probablilty)이지만 사건 B라는 사건이 발생 시 P(Aj|B)의 확률을 구할 수 있고 이 확률이 사후확률이 된다.

3. 베이지안 법칙은 사전에 어떤 사건 A에 대한 사전확률이 부여된 상태에서 어떤 사건 B에 관한 정보가 알려진 후, 그 사건 A에 대한 사후 확률을 다음 아래와 같이 정리할 수 있다.

```

표본공간이 상호 배타적인 A1, A2, ..., Ak로 분할될 때,

P(Aj|B) = P(B|Aj)P(Aj) / P(B) = P(B|Aj)P(A) / 시그마 i=1, k까지 P(B|Ai)P(Ai)

해당 내용은 사건에 대한 정의가 중요하다!!!

```

# 확률변수의 종류

1. 이산확률변수(Discrete Random Variable) - 확률변수가 취할 수 있는 값의 수가 유한한 변수(동전을 던지거나 주사위를 던지는 사건 등)

2. 연속확률변수(Continuous Random Variable) - 확률변수가 취할 수 있는 값의 수가 무한한 변수(확률변수 X가 키, 몸무게, 시간과 같이 셀 수 없고 연속적인 값을 취할 때)

# 확률분포

1. 확률분포는 수치로 대응된 확률변수의 개별 값들이 가지는 확률값의 분포이다.

2. 이산확률분포(Discrete Probability Distribution) - 확률변수가 취할 수 있는 값의 수가 유한한 확률분포이다.

- 확률질량함수(Probability Mass Function) - 이산확률변수에서 특정 값에 대한 확률을 나타내는 함수 f(x) = P(X=x)이다.

3. 연속확률분포(Continuous Probability Distribution) - 확률변수가 취할 수 있는 값의 수가 무한한 확률분포이다.

- 확률밀도함수(Probability Density Function) - 확률 변수의 분포를 나타내는 함수이다.

4. 확률분포함수(Probability Distribution Function, 확률 함수)

- 확률변수가 취할 수 있는 구체적인 값 하나하나를 확률공간상의 확률값으로 할당해주는 함수를 의미한다.

- 이산확률분포함수(Discrete Probability Distribution Function) - 확률변수가 이산적인 확률분포를 가지는 함수다.

- 연속확률분포함수(Continuous Probability Distribution Function) - 확률변수가 연속적인 확률분포를 가지는 함수다.

# 이산확률분포의 종류

1. 베르누이 분포(Bernoulli Distribution) - 결과가 성공 아니면 실패, 두 가지로 귀결되어 나오는 이산확률 분포이다.

2. 이항분포(Binormial Distribution) - 베르누이 시행을 n번 독립적으로 시행할 때 성공횟수를 X로 정의한 이산확률분포이다.

3. 다항분포(Multinomial Distribution) - 여러 개의 값을 가질 수 있는 독립 확률변수들에 대한 확률분포로, 여러 번의 독립적 시행에서 각각의 값이 특정 횟수가 나타날 확률을 정의하는 분포이다.

4. 포아송분포(Poisson Distribution) - 단위 시간 안에 어떤 사건이 몇 번 발생할 것인지를 표현하는 이산확률분포이다.

- X를 단위시간당 발생건수라고 하면 포아송분포는 평균 사건 발생수 람다에 의해 유도된다. 그러므로 포아송분포는 기댓값과 분산이 동일하게 정의된다.

5. 기하분포(Geometric Distributuin) - 베르누이 시행에서 처음 성공까지 시도한 횟수를 분포화한 이산확률분포의 한 종류이다.

6. 음이항분포(Negative Binomial Distribution) - x번의 베르누이 시행에서 k번째 성공할 때까지 계속 시행하는 실험에서의 확률을 나타내는 이산확률분포이다.

7. 초기하분포(Hypergeometric Distribution) - 비복원 추출에서 N개 중에 n개를 추출했을 때, 원하는 것 k개가 뽑힐 확률을 나타내는 이산확률분포이다.(로또)

# 연속확률분포

1. 연속균등분포(Continuous Uniform Distribution) - 분포가 특정 범위 내에서 균등하게 나타나 있을 경우를 가리킨다. 이 분포는 두 개의 매개변수 a, b를 받으며, 이때 [a, b]범위에서 균등한 확률을 가진다. 보통 기호로 U(a, b)

2. 지수분포(Exponential Distribution) - 사건이 서로 독립적일 때, 일정 시간 동안 발생하는 사건의 횟수가 포아송분포를 따른다면, 다음 사건이 일어날 때까지의 대기시간(B)에 대한 확률이 따르는 분포이다.

- 즉 포아송과정에서 한 개의 사건이 발생할 때까지의 대기 시간을 의미한다. 지수분포는 대기시간, 포아송분포는 횟수이다.

- 지수분포의 무기억성질(Memoryless Property) - P(X > a + b|X > a) = P (X > b) 가 성립

3. 정규분포(Normal Distribution) - 정규분포는 표본을 통한 통계적 추정 및 가설검정이론의 핵심이 되며, 실제로 우리가 사회적, 자연적 현상에서 접하는 여러 자료들의 분포가 정규분포를 띠게 된다.

4. 표준정규분포(Standard Normal Distribution) - 정규확률변수가 어떤 범위의 값을 취할 확률을 계산할 때 매번 확률밀도함수 그래프의 밑부분에서 그 범위에 해당하는 넓이를 구하는 일은 매우 번거로우며, 정규분포의 위치는 평균과 표준편차에 따라 달라지게 된다.

- 표준정규분포는 평균 u=0, 표준편차 6=1이 되도록 한 정규분포이다.

5. 감마분포(Gamma Distribution) - 연속 확률분포로, 두 개의 매개변수를 받으며 양의 실수를 가질 수 있다.

- 감마분포는 지수분포나 포아송분포 등의 매개변수와 연관이 있는 분포로 포아송과정(1/람다 = 세타)에서 k개의 사건이 발생할 때까지의 대기시간으로 확률변수 X를 정의할 수 있다.

6. 카이제곱분포(Chi-Squared Distribution) - k개의 서로 독립적인 표준정규확률 변수를 각각 제곱한 다음 합해서 얻어지는 분포로 정의한다.

7. 스튜던트 t 분포(Student t-Distribution) - 정규분포의 평균 측정 시 주로 사용하는 분포이다. 종 모양으로서 t=0에 대하여 대칭을 이루는데 t-곡선의 모양을 결정하는 것은 자유도이다.

- 자유도 - 표본 크기 n에서 1을 뺀 것이다. 

8. F 분포(F Distribution) - 두 개의 확률변수 V1, V2의 자유도가 각각 k1, k2이고 서로 카이제곱분포를 따른다고 할 때, 다음 아래와 같이 정의된 확률변수(검정통계량)

- F = V1/k1 / V2/k2는 자유도가 k1,k2인 F-분포를 따른다고 한다.

- F 분포는 F 검정이나 분산분석 등에 주로 사용되는 분포함수이다.



