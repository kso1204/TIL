# 분석 변수 점검항목 정의

1. 데이터 수집

- 데이터 적정성, 데이터 가용성, 대체 분석 데이터 유무

2. 데이터 정합성

- 데이터 중복, 분석 변수별 범위, 분석 변수별 연관성, 데이터 내구성

3. 특징 변수

- 특징 변수 사용, 변수 간 결합 가능 여부

4. 타당성

- 편익/비용 검증, 기술적 타당성

# 데이터 전처리 수행

1. 데이터 정제, 통합, 축소, 변환을 반복적으로 수행하여 분석 변수로 활용하는 방안을 수립할 수 있다.

# 분석 절차

1. 문제 인식 -> 연구조사 -> 모형화 -> 데이터 수집 -> 데이터 분석 -> 분석 결과 제시

# 작업분할구조(WBS, Work Breakdown Structure)

1. 프로젝트의 범위와 최종 산출물을 세부요소로 분할한 계층적 구조도

2. 데이터 분석과제 정의 -> 데이터 준비 및 탐색 -> 데이터 분석 모델링 및 검증 -> 산출물 정리

# 분석목표정의서

1. 문제의 개선방향에 맞는 현실적인 분석목표를 수립하여 필요한 데이터에 대한 정보나 분석 타당성 검토 및 성과측정 방법 등을 정리한 정의서

2. 구성요소

- 원천 데이터 조사

- 분석 방안 및 적용 가능성 판단

- 성과평가

# 분석 프로젝트

1. 분석 프로젝트는 과제 형태로 도출된 분석 기회를 프로젝트화하여 그 가치를 증명하기 위한 수단이다.

2. 도출된 결과의 재해석을 통한 지속적인 반복과 정교화가 수행되는 경우가 대부분이다.

3. 관리 영역

- 데이터 크기

- 데이터 복잡도

- 속도

- 분석 모형의 복잡도(Analytic Model Complexity) - 분석 모형의 정확도와 복잡도는 Trade off 관계에 있다.

- 정확도(Accuracy)와 정밀도(Precision) - 분석 결과를 활용하는 측면에서는 정확도가 중요하다, 분석 모형의 안정성 측면에서는 정밀도가 중요하다. 정확도와 정밀도는 Trade off인 경우가 많다.

- 정확도와 정밀도는 편향(Bias)과 분산(Variance)과 관련이 있다. 반대의 속성

3. 분석 프로젝트의 영역별 주요 관리 항목

- 범위, 일정, 원가, 품질, 통합, 조달, 인적자원, 위험, 의사소통, 이해관계자 관리

# 비즈니스 도메인과 원천 데이터 정보 수집

1. 비즈니스 도메인 정보

- 비즈니스 모델, 비즈니스 용어집, 비즈니스 프로세스로부터 관련 정보를 습득한다.

- 도메인 전문가 인터뷰를 통해 데이터의 종류, 유형, 특징 정보를 습득한다.

2. 원천 데이터 정보

- 데이터 분석에 필요한 대상 원천 데이터의 수집 가능성, 데이터의 보안, 정확성을 탐색하고, 데이터 수집의 난이도, 수집 비용 등 기초 자료를 수집할 수 있다.

# 내, 외부 데이터 수집

1. 데이터의 종류

- 내부 데이터는 조직 내부의 서비스 시스템, 네트워크 및 서버 장비, 마케팅 관련 시스템 등으로부터 생성되는 데이터를 말한다.

- 외부 데이터는 다양한 소셜 데이터, 특정 기관 데이터, M2M 데이터, LOD(LinkedOpen Data) 등으로 나눌 수 있다.

# 데이터 유형별 수집 기술

1. 정형 데이터

- ETL, FTP, API, DBtoDB, 스쿱(Sqoop) - 관계형 데이터베이스(RDBMS)와 하둡(Hadoop)간 데이터를 전송하는 방법

2. 비정형 데이터

- 크롤링, RSS(Rich Site Summary), Open API, 척와(Chukwa), 카프카

3. 반정형 데이터

- 플럼(Flume), 스크라이브(Scribe), 센싱(Sensing), 스트리밍(Streaming) - TCP, UDP, Bluetooth, RFID

# 스쿱

1. Sqoop(SQL + Hadoop)

2. 관계형 데이터 스토어 간에 대량 데이터를 효과적으로 전송하기 위해 구현된 도구이다.

3. 커넥터를 사용하여 MySql, Orcle, MS SQL 등 관계형 데이터베이스의 데이터를 하둡 파일시스템(HDFS, Hive, Hbase)으로 데이터를 수집한다.

4. 관계형 데이터베이스에서 가져온 데이터들을 하둡 맵리듀스(Hadoop MapReduce)로 변환하고, 변환된 데이터들을 다시 관계형 데이터베이스로 내보낼 수 있다.

5. 데이터 가져오기/내보내기 과정을 맵리듀스를 통해 처리하기 때문에 병렬처리가 가능하고 장애에도 강한 특징을 갖는다.

6. 스쿱은 모든 적재 과정을 자동화하고 병렬처리 방식으로 작업한다.

7. Bulk import 지원 - 전체 데이터베이스 또는 테이블을 HDFS로 전송 가능하다.

8. 데이터 전송 병렬화 - 시스템 사용률과 성능을 고려한 병렬 데이터를 전송한다.

9. Direct Input 제공 - RDB에 매핑하여 Hbase와 Hive에 직접적 import를 제공한다.

10. 프로그래밍 방식의 데이터 인터랙션 - 자바 클래스 생성을 통한 데이터 상호작용을 지원한다.

# 플럼

1. 아파치 플럼은 