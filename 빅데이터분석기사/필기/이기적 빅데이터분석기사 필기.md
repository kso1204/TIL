# 분석 변수 점검항목 정의

1. 데이터 수집

- 데이터 적정성, 데이터 가용성, 대체 분석 데이터 유무

2. 데이터 정합성

- 데이터 중복, 분석 변수별 범위, 분석 변수별 연관성, 데이터 내구성

3. 특징 변수

- 특징 변수 사용, 변수 간 결합 가능 여부

4. 타당성

- 편익/비용 검증, 기술적 타당성

# 데이터 전처리 수행

1. 데이터 정제, 통합, 축소, 변환을 반복적으로 수행하여 분석 변수로 활용하는 방안을 수립할 수 있다.

# 분석 절차

1. 문제 인식 -> 연구조사 -> 모형화 -> 데이터 수집 -> 데이터 분석 -> 분석 결과 제시

# 작업분할구조(WBS, Work Breakdown Structure)

1. 프로젝트의 범위와 최종 산출물을 세부요소로 분할한 계층적 구조도

2. 데이터 분석과제 정의 -> 데이터 준비 및 탐색 -> 데이터 분석 모델링 및 검증 -> 산출물 정리

# 분석목표정의서

1. 문제의 개선방향에 맞는 현실적인 분석목표를 수립하여 필요한 데이터에 대한 정보나 분석 타당성 검토 및 성과측정 방법 등을 정리한 정의서

2. 구성요소

- 원천 데이터 조사

- 분석 방안 및 적용 가능성 판단

- 성과평가

# 분석 프로젝트

1. 분석 프로젝트는 과제 형태로 도출된 분석 기회를 프로젝트화하여 그 가치를 증명하기 위한 수단이다.

2. 도출된 결과의 재해석을 통한 지속적인 반복과 정교화가 수행되는 경우가 대부분이다.

3. 관리 영역

- 데이터 크기

- 데이터 복잡도

- 속도

- 분석 모형의 복잡도(Analytic Model Complexity) - 분석 모형의 정확도와 복잡도는 Trade off 관계에 있다.

- 정확도(Accuracy)와 정밀도(Precision) - 분석 결과를 활용하는 측면에서는 정확도가 중요하다, 분석 모형의 안정성 측면에서는 정밀도가 중요하다. 정확도와 정밀도는 Trade off인 경우가 많다.

- 정확도와 정밀도는 편향(Bias)과 분산(Variance)과 관련이 있다. 반대의 속성

3. 분석 프로젝트의 영역별 주요 관리 항목

- 범위, 일정, 원가, 품질, 통합, 조달, 인적자원, 위험, 의사소통, 이해관계자 관리

# 비즈니스 도메인과 원천 데이터 정보 수집

1. 비즈니스 도메인 정보

- 비즈니스 모델, 비즈니스 용어집, 비즈니스 프로세스로부터 관련 정보를 습득한다.

- 도메인 전문가 인터뷰를 통해 데이터의 종류, 유형, 특징 정보를 습득한다.

2. 원천 데이터 정보

- 데이터 분석에 필요한 대상 원천 데이터의 수집 가능성, 데이터의 보안, 정확성을 탐색하고, 데이터 수집의 난이도, 수집 비용 등 기초 자료를 수집할 수 있다.

# 내, 외부 데이터 수집

1. 데이터의 종류

- 내부 데이터는 조직 내부의 서비스 시스템, 네트워크 및 서버 장비, 마케팅 관련 시스템 등으로부터 생성되는 데이터를 말한다.

- 외부 데이터는 다양한 소셜 데이터, 특정 기관 데이터, M2M 데이터, LOD(LinkedOpen Data) 등으로 나눌 수 있다.

# 데이터 유형별 수집 기술

1. 정형 데이터

- ETL, FTP, API, DBtoDB, 스쿱(Sqoop) - 관계형 데이터베이스(RDBMS)와 하둡(Hadoop)간 데이터를 전송하는 방법

2. 비정형 데이터

- 크롤링, RSS(Rich Site Summary), Open API, 척와(Chukwa), 카프카

3. 반정형 데이터

- 플럼(Flume), 스크라이브(Scribe), 센싱(Sensing), 스트리밍(Streaming) - TCP, UDP, Bluetooth, RFID

# 스쿱

1. Sqoop(SQL + Hadoop)

2. 관계형 데이터 스토어 간에 대량 데이터를 효과적으로 전송하기 위해 구현된 도구이다.

3. 커넥터를 사용하여 MySql, Orcle, MS SQL 등 관계형 데이터베이스의 데이터를 하둡 파일시스템(HDFS, Hive, Hbase)으로 데이터를 수집한다.

4. 관계형 데이터베이스에서 가져온 데이터들을 하둡 맵리듀스(Hadoop MapReduce)로 변환하고, 변환된 데이터들을 다시 관계형 데이터베이스로 내보낼 수 있다.

5. 데이터 가져오기/내보내기 과정을 맵리듀스를 통해 처리하기 때문에 병렬처리가 가능하고 장애에도 강한 특징을 갖는다.

6. 스쿱은 모든 적재 과정을 자동화하고 병렬처리 방식으로 작업한다.

7. Bulk import 지원 - 전체 데이터베이스 또는 테이블을 HDFS로 전송 가능하다.

8. 데이터 전송 병렬화 - 시스템 사용률과 성능을 고려한 병렬 데이터를 전송한다.

9. Direct Input 제공 - RDB에 매핑하여 Hbase와 Hive에 직접적 import를 제공한다.

10. 프로그래밍 방식의 데이터 인터랙션 - 자바 클래스 생성을 통한 데이터 상호작용을 지원한다.

# 플럼

1. 아파치 플럼은 대요량의 로그 데이터를 효과적으로 수집, 집계, 이동시키는 신뢰성 있는 분산 서비스를 제공하는 솔루션이다.

2. 스트리밍 데이터 흐름에 기반을 둔 간단하고 유연한 구조를 가진다.

3. 플럼에서 하나의 에이전트는 소스, 채널, 싱크로 구성된다.

4. 소스는 웹서버, 로그데이터서버 등 원시데이터소스와 연결되며, 소스로부터 들어오는 데이터는 큐의 구조를 갖는 채널로 들어간 후, 싱크를 통해 목표 시스템으로 전달된다.

5. 로그 데이터 수집과 네트워크 트래픽 데이터, 소셜 미디어 데이터, 이메일 메시지 등 대량의 이벤트 데이터 전송을 위해 사용된다.

6. 특징으로는 신뢰성, 확장성, 효율성을 가지고 있다.

# 데이터 적절성 검증

1. 데이터 누락 점검 - 수집 데이터 세트의 누락, 결측 여부를 판단하여 누락 발생 시 재수집한다.

2. 소스 데이터와 비교 - 수집 데이터와 소스 데이터의 사이즈 및 개수를 비교 검증한다.

3. 데이터의 정확성 점검 - 유효하지 않는 데이터 존재여부를 점검한다.

4. 보안 사항 점검 - 수집 데이터의 개인정보 유무 등 보안 사항의 점검이 필요하다.

5. 저작권 점검 - 데이터의 저작권 등 법률적 검토를 수행한다.

6. 대량 트래픽 발생 여부 - 네트워크 및 시스템에 트래픽을 발생시키는 데이터 여부를 검증한다.

# 데이터 변환 방식의 종류

1. 비정형 데이터를 정형 데이터 형태로 저장하는 방식(관계형 데이터베이스)

2. 수집 데이터를 분산파일시스템으로 저장하는 방식(HDFS 등)

3. 주제별, 시계열적으로 저장하는 방식(데이터 웨어하우스)

4. 키-값 형태로 저장하는 방식(NoSQL)

```

관계형 데이터 베이스 - MySQL, Oracle, DB2, PostgreSQL

분산데이터 저장 - HDFS(Hadoop Distributed File System)

데이터 웨어하우스 - 네티자, 테라데이타, 그린플럼의 DW 솔루션

NoSQL - Hbase, Cassandra, MongoDB

```

# 비식별 조치 방법

1. 가명처리 - 개인 식별이 가능한 데이터를 직접적으로 식별할 수 없는 다른 값으로 대체하는 기법

```

장점 - 데이터의 변형 또는 변질 수준이 적다.

단점 - 대체 값 부여 시에도 식별 가능한 고유 속성이 계속 유지된다.

```

2. 가명처리 세부기술 

```

휴리스틱 가명화 - 식별자에 해당하는 값들을 몇 가지 정해진 규칙으로 대체하거나 사람의 판단에 따라 가공하여 자세한 개인정보를 숨기는 방법

암호화 - 정보 가공시 일정한 규칙의 알고리즘을 적용하여 암호화함으로써 개인정보를 대체하는 방법

교환방법 - 기존의 데이터베이스 레코드를 사전에 정해진 외부의 변수(항목)값과 연계하여 교환한다.

```

3. 총계처리 - 통계값을 적용하여 특정 개인을 식별할 수 없도록 한다.

```

장점 - 민감한 수치 정보에 대하여 비식별 조치가 가능하며, 통계분석용 데이터셋 작성에 유리하다.

단점 - 정밀 분석이 어려우며, 집계 수량이 적을 경우 추론에 의한 식별 가능성이 있다.

```

4. 총계처리 세부기술

```

부분총계 - 데이터셋 내 일정부분 레코드만 총계 처리하며, 다른 데이터 값에 비하여 오차 범위가 큰 항목을 통계값(평균 등)으로 변환한다.

라운딩 - 집계 처리된 값에 대하여 라운딩(올림, 내림, 반올림) 기준을 적용하여 최종 집계 처리하는 방법이다.

재배열 - 기존 정보 값은 유지하면서 개인이 식별되지 않도록 데이터를 재배열 하는 방법이다.

```

5. 데이터 삭제 - 개인 식별이 가능한 데이터를 삭제 처리한다.

```

장점 - 개인 식별요소의 전부 및 일부 삭제 처리가 가능하다.

단점 - 분석의 다양성과 분석 결과의 유효성, 신뢰성이 저하된다.

```

6. 데이터 삭제 세부기술

```

식별자 (부분)삭제 - 원본 데이터에서 식별자를 단순 삭제하는 방법과 일부만 삭제하는 방법이 있다.

레코드 삭제 - 다른 정보와 뚜렷하게 구별되는 레코드 전체를 삭제하는 방법이다. (소득)

식별요소 전부삭제 - 식별자뿐만 아니라 잠재적으로 개인을 식별할 수 있는 속성자까지 전부 삭제하여 프라이버시 침해 위험을 줄이는 방법이다. (연예인, 정치인 정보)

```

7. 데이터 범주화 - 특정 정보를 해당 그룹의 대푯값 또는 구간값으로 변환(범주화)하여 개인 식별을 방지한다.

```

장점 - 통계형 데이터 형식이므로 다양한 분석 및 가공 가능하다.

단점 - 정확한 분석결과 도출이 어려우며, 데이터 범위 구간이 좁혀질 경우 추론 가능성이 있다.

```

8. 데이터 범주와 세부기술

```

감추기 - 명확한 값을 숨기기 위하여 데이터의 평균 또는 범주 값으로 변환하는 방식이다.

랜덤 라운딩 - 수치 데이터를 임의의 수 기준으로 올림 또는 내림하는 기법으로 수치 데이터 이외의 경우에도 확장 적용 가능하다. (42세, 45세 -> 40대)

범위 방법 - 수치 데이터를 임의의 수 기준으로 범위(range)로 설정하는 기법이다. (3,300만원 -> 3,000 ~ 4,000만 원)

제어 라운딩 - 랜덤 라운딩 방법에서 어떠한 특정 값을 변경할 경우 행과 열의 합이 일치하지 않는 단점을 해결하기 위해 행과 열이 맞지 않는 것을 제어하여 일치시키는 기법이다.

```

9. 데이터 마스킹 - 데이터의 전부 또는 일부분을 대체 값(공백, 노이즈)으로 변환한다.

```

장점 - 개인 식별 요소를 제거하는 것이 가능하며, 원 데이터 구조에 대한 변형이 적다.

단점 - 마스킹을 과도하게 적용할 경우 데이터 필요 목적에 활용하기 어려우며 마스킹 수준이 낮을 경우 특정한 값에 대한 추론이 가능하다.

```

10. 데이터 마스킹 세부기술

```

임의 잡음 추가 - 개인 식별이 가능한 정보에 임의의 숫자 등 잡음을 추가(더하기 또는 곱하기)하는 방법이다.

공백과 대체 - 특정 항목의 일부 또는 전부를 공백 또는 대체문자('*','_' 등이나 전각 기호)로 바꾸는 기법이다. (1990-12-04를 19**-**-**)

```

# 적정성 평가

1. 적정성 평가 시 프라이버시 보호 모델 중 최소한의 수단으로 k-익명성을 활용하며, 필요시 추가적인 평가모델(l-다양성, t-근접성)을 활용한다.

2. k-익명성 - 특정인임을 추론할 수 있지 여부를 검토, 주어진 데이터 집합에서 같은 값이 적어도 k개 이상 존재하도록 하여 쉽게 다른 정보로 결합할 수 없도록 한다.

3. k-익명성의 취약점

```

동질성 공격 - k-익명성에 의해 레코드들이 범주화되었더라도 일부 정보들이 모두 같은 값을 가질 수 있기 때문에 데이터 집합에서 동일한 정보를 이용하여 공격 대상의 정보를 알아내는 공격이다.

배경지식에 의한 공격 - 주어진 데이터 이외의 공격자의 배경 지식을 통해 공격 대상의 민감한 정보를 알아내는 공격이다 (여자는 전립선염에 걸릴 수 없다.)

```

4. l-다양성 - k-익명성에 대한 두 가지 공격, 즉 동질성 공격 및 배경지식에 의한 공격을 방어하기 위한 모델, 비식별 조치 과정에서 충분히 다양한(l개 이상) 서로 다른 정보를 갖도록 동질 집합을 구성함

5. l-다양성의 취약점

```

쏠림공격 - 정보가 특정한 값에 쏠려 있을 경우 l-다양성 모델이 프라이버시를 보호하지 못한다. (임의의 집단이 99개의 위암 양성, 1개의 위암 음성이면 공격자는 공격 대상이 99%확률로 위암 양성이라는 것을 알 수 있음)

유사성 공격 - 비식별 조치된 레코드의 정보가 서로 비슷하다면 l-다양성 모델을 통해 비식별된다 할지라도 프라이버시가 노출될 수 있다.

```

6. t-근접성 - l-다양성의 취약점(쏠림 공격, 유사성 공격)을 보완하기 위한 모델로 값의 의미를 고려하는 모델, t-근접성은 동질 집합에서 특정 정보의 분포와 전체 데이터 집합에서 정보의 분포가 t 이하의 차이를 보여야 한다.

# 정형 데이터 품질기준

1. 완전성, 유일성, 유효성, 일관성, 확장성

2. 완전성 - 필수항목에 누락이 없어야 한다. (개별 완전성, 조건 완전성)

3. 유일성 - 데이터 항목은 유일해야 하며 중복되어서는 안된다. (단독 유일성, 조건 유일성)

4. 유효성 - 데이터 항목은 정해진 데이터 유효범위 및 도메인을 충족해야 한다. (범위 유효성, 날짜 유효성, 형식 유효성)

5. 일관성 - 데이터가 지켜야할 구조, 값, 표현되는 형태가 일관되게 정의되고, 서로 일치해야 한다. (기준코드 일관성, 참조 무결성, 데이터 흐름 일관성, 칼럼 일관성)

6. 정확성 - 실세계에 존재하는 객체의 표현 값이 정확히 반영되어야 한다. (선후 관계 정확성, 계산/집계 정확성, 최신성, 업무규칙 정확성)

# 비정형 데이터 품질 기준

1. 기능성, 신뢰성, 사용성, 효율성, 이식성

2. 기능성 - 명시된 요구와 내재된 요구를 만족하는 기능을 제공하는 정도 (적절성, 정확성, 상호 운용성, 기능 순응성)

3. 신뢰성 - 규정된 신뢰 수준을 유지하거나 사용자로 하여금 오류를 방지할 수 있도록 하는 정도 (성숙성, 신뢰 순응성)

4. 사용성 - 사용자에 의해 이해되고, 선호될 수 있게 하는 정도 (이해성, 친밀성, 사용 순응성)

5. 효율성 - 자원의 양에 따라 요구된 성능을 제공하는 정도 (시간 효율성, 자원 효율성, 효율 순응성)

6. 이식성 - 다양한 환경과 상황에서 실행될 가능성 (적응성, 공존성, 이식 순응성)

# 빅데이터 저장 시스템

1. 대용량 데이터 집합을 저장하고 관리하는 시스템으로 사용자에게 데이터 제공 신뢰성과 가용성을 보장하는 시스템이다.

2. 파일 시스템 저장방식 - 빅데이터를 확장 가능한 분산 파일의 형태로 저장하는 방식의 대표적인 예는 HDFS, 구글의 GFS 등을 들 수 있다. 

# 분산 파일 시스템

1. 하둡 분산파일 시스템(HDFS) 

2. 하둡은 아파치 진영에서 분산 환경 컴퓨팅을 목표로 시작한 프로젝트로 분산 처리를 위한 파일 시스템이다.

3. HDFS는 대용량 파일을 클러스터에 여러 블록으로 분산하여 저장하며, 블록은 마지막 블록을 제외하고 모두 크기가 동일하다(기본 크기 64MB)

4. HDFS는 마스터(Master) 하나와 여러 개의 슬레이브(Slave)로 클러스트링 되어 구성된다.

- 마스터노드(Master Node)는 네임 노드(Name Node)라고 하며 슬레이브를 관리하는 메타데이터와 모니터링 시스템을 운영한다.

- 슬레이브노드(Slave Node)는 데이터 노드(Data Node)라고 하며 데이터 블록을 분산처리한다.

5. 데이터 손상을 방지하기 위해서 데이터 복제 기법을 사용한다.

6. 하둡의 장점

```

하둡의 DFS는 대용량의 빙정형 데이터 저장 및 분석에도 효율적이다.

클러스터 구성을 통해 멀티 노드로 부하를 분산시켜 처리한다.

개별적인 서버에서 진행되는 병렬처리 결과를 하나로 묶어 시스템의 과부하나 병목 현상을 줄여준다.

하둡을 장비를 증가시킬 수록 성능이 향상된다.

오픈소스 하둡은 무료로 사용할 수 있다.

```

7. 분산 데이터 처리기술 - 맵리듀스(MapReduce)

```

구글에서 발표한 MapReduce 방법을 하둡 오픈소스 프로젝트에서 구현하였다.

MapReduce는 주어진 입력에 대해, 이를 여러 개의 부분으로 분할하고 각각의 부분에 대해 필요한 함수를 적용하여 결과값을 저장한다(Map함수와 Reduce함수로 구성)

분산 병렬처리가 가능하다.

Input -> Spliting -> Mapping -> Shuffling -> Reducing -> Final result

```

8. 구글 파일 시스템(GFS)

9. 구글 파일 시스템은 엄청나게 많은 데이터를 보유하는 구글의 핵심 데이터 스토리지와 구글 검색 엔진을 위해 최적화된 분산 파일 시스템이다.

10. 마스터(Master), 청크 서버(Chunk Server), 클라이언트로 구성된다.

- 마스터는 GFS 전체의 상태를 관리하고 통제한다.

- 청크서버는 물리적인 하드디스크의 실제 입출력을 처리한다.

- 클라이언트는 파일을 읽고 쓰는 동작을 요청하는 애플리케이션이다.

11. 파일들은 일반적인 파일 시스템에서의 클러스터들과 섹터들과 비슷하게 64MB로 고정된 크기의 청크들로 나누어서 저장된다.

12. 가격이 저렴한 서버에서도 사용되도록 설계되었기 때문에 하드웨어 안전성이나 자료들의 유실에 대해서 고려하여 설계되었고 응답 시간이 조금 길더라도 데이터의 높은 처리성능에 중점을 두었다.

# CAP 이론 - 기존 데이터 저장 구조의 한계

1. 2002년 버클리 대학의 에릭 브루어 교수가 발표한 이론

2. 분산 컴퓨팅 환경의 특징을 일관성(Consistency), 가용성(Availability), 지속성(Partition Tolerance) 세 가지로 정의 할 수 있는데, 어떤 시스템이든 이 세 가지 특성을 동시에 만족하기 어렵다.

3. 일관성 - 분산 환경에서 모든 노드가 같은 시점에 같은 데이터를 보여줘야 한다.

4. 가용성 - 일부 노드가 다운되어도 다른 노드에 영향을 주지 않아야 한다.

5. 지속성 - 데이터 전송 중에 일부 데이터를 손실하더라도 시스템은 정상 동작해야 한다.

6. RDBMS - 일관성 + 가용성 (트랜잭션 ACID 보장, 금융 서비스)

7. NoSql - 일관성 + 지속성 (대용량 분산 파일 시스템 - 성능 보장), 가용성 + 지속성 (비동기식 서비스 - 아마존, 트위터)

# NoSQL의 기술적 특성

1. 스키마 X(데이터 저장 방식은 크게 열, 값, 문서, 그래프 등의 네 가지를 기반으로 구분한다.), 탄력성(시스템 일부에 장애가 발생해도 클라이언트가 시스템에 접근 가능하다.), 질의 기능, 캐싱

# NoSQL의 데이터 모델

1. NoSQL의 데이터 저장 방식에 따라 키-값 구조, 칼럼기반 구조, 문서기반 구조로 구분할 수 있다.

2. 키-값 데이터베이스 - 데이터를 키와 그에 해당하는 값의 쌍으로 저장하는 데이터 모델에  기반을 둔다. (DynamoDB, Redis)

3. 열기반(컬럼기반) 데이터베이스 - 데이터를 로우가 아닌 컬럼기반으로 저장하고 처리하는 데이터베이스를 말한다. (Cassandra, BigTable, Hbase)

4. 문서기반 데이터베이스 - 문서 형식의 정보를 저장, 검색, 관리하기 위한 데이터베이스 (MongoDB, CouchDB)

# 데이터 관련 정의

1. 데이터 - 관심의 대상이 되는 사물이나 사건의 속성을 일정한 규칙에 의해 측정, 조사, 관찰하여 습득

2. 단위 - 관찰되는 항목 또는 대상을 지칭한다.

3. 관측값 - 각 조사 단위별 기록정보 또는 특성을 말한다.

4. 변수 - 각 단위에서 측정된 특성 결과이다.

5. 원자료 - 표본에서 조사된 최초의 자료를 이야기한다.

# 데이터의 종류

1. 단변량자료 - 자료의 특성을 대표하는 특성 변수가 하나인 자료이다.

2. 다변량자료 - 자료의 특성을 대표하는 특성 변수가 두 가지 이상인 자료이다.

3. 질적자료 - 정성적 또는 범주형 자료라고도 하며 자료를 범주의 형태로 분류한다.

- 분류의 편의상 부여된 수치의 크기자체에는 의미를 부여하지 않는 자료이며 명목자료, 서열자료 등 이질적자료로 분류된다.

- 명목자료 - 측정대상이 범주나 종류에 대해 구분되어지는 것을 수치 또는 기호로 분류되는 자료이다(국번)

- 서열자료 - 명목자료와 비슷하나 수치나 기호가 서열을 나타내는 자료이다(기록경기의 순위)

4. 수치자료 - 정량적 또는 연속형 자료라고도 한다. 숫자의 크기에 의미를 부여할 수 있는 자료를 나타내며 구간자료, 비율자료가 여기에 속한다.

- 구간자료 - 명목자료, 서열자료의 의미를 포함하면서 숫자로 표현된 변수에 대해서 변수 간의 관계가 산술적인 의미를 가지는 자료(온도)

- 비율자료 - 명목자료, 서열자료, 구간자료의 의미를 가지는 자료로서 수치화된 변수에 비율의 개념을 도입할 수 있는 자료(무게)

5. 시계열자료 - 일정한 시간간격 동안에 수집된, 시간개념이 포함되어 있는 자료(일별 주식 가격)

6. 횡적자료 - 횡단면자료라고도 하며 특정 단일 시점에서 여러 대상으로부터 수집된 자료이다. 즉 한 개의 시점에서 여러 대상으로부터 취합하는 자료

7. 종적자료 - 시계열자료와 횡적자료의 결합으로 여러 개체를 여러 시점에서 수집한 자료이다.

8. 데이터의 종류는 앞의 내용에서 변수들의 집합인 자료의 종류와 그 특성을 동일하게 가지므로 데이터의 종류에 따라서 적용방법론이 다양하게 변화할 수 있다.

# 데이터의 정제

1. 수집된 데이터를 대상으로 분석에 필요한 데이터를 추출하고 통합하는 과정이다.

2. 다양한 매체로부터 데이터를 수집, 원하는 형태로 변환, 원하는 장소에 저장, 저장된 데이터의 활용가능성을 타진하기 위한 품질확인, 필요한 시기와 목적에 따라 사용이 원활하도록 관리의 과정이 필요하다.

3. 수집 - 데이터의 입수 방법 및 정책 결정, 입수경로의 구조화, 집계, 저장소 결정

4. 변환 - 데이터 유형의 변환 및 분석 가능한 형태로 가공, ETL, 일반화, 정규화

5. 교정 - 결측치의 처리, 이상치 처리, 노이즈 처리, 비정형데이터 수집 시 필수사항

6. 통합 - 데이터분석이 용이하도록 기존 또는 유사데이터와의 연계 통합, 레거시 데이터와 함께 분석이 필요할 경우 수행

# 데이터 결측값 처리

1. 데이터 분석에서 결측치(결측값)는 데이터가 없음을 의미한다.

- 결측치를 임의로 제거 시 - 분석 데이터의 직접손실로 분석에 필요한 유의수준 데이터 수집에 실패할 가능성이 발생한다.

- 결측치를 임의로 대체 시 - 데이터의 편향(bias)이 발생하여 분석 결과의 신뢰성 저하 가능성이 있다.

2. 결측 데이터의 종류

- 완전 무작위 결측(MACR, Missing Completely At Random) - 어떤 변수상에서 결측 데이터가 관측된 혹은 관측되지 않는 다른 변수와 아무런 연관이 없는 경우이다.

- 무작위 결측(MAR, Missing At Random) - 변수상의 결측데이터가 관측된 다른 변수와 연관되어 있지만 그 자체가 비관측값들과는 연관되지 않은 경우이다.

- 비 무작위 결측(NMAR, Not Missing At Random) - 어떤 변수의 결측 데이터가 완전 무작위 결측 또는 무작위 결측이 아닌 결측데이터로 정의하는 즉, 결측변수값이 결측여부(이유)와 관련이 있는 경우이다.

```

나이대별(X), 성별(Y)과 체중(Z) 분석에 대한 모델링을 가정해보면

X, Y, Z와 관계없이 Z가 없는 경우 - 데이터의 누락(응답 없음) - 완전 무작위 결측(MACR)

여성(Y)은 체중 공개를 꺼려하는 경향 - Z가 누락될 가능성이 Y에만 의존 - 무작위 결측(MAR)

젊은(X) 여성(Y)의 경우는 체중 공개를 꺼리는 경우가 더 높음 - 무작위 결측(MAR)

무거운 or 가벼운 사람들은 체중 공개 가능성이 적음 - Z가 누락될 가능성이 Z값 자체에 관찰되지 않는 값에 달려 있음 - 비 무작위 결측(NMAR)

```

3. 결측값 유형의 분석 및 대치

- 결측치가 존재하는 데이터를 이용한 분석은 다음 세 가지 고려사항이 발생하는데 효율성문제, 자료처리의 복잡성, 편향 문제이다.

4. 단순 대치법

- 기본적으로 결측치에 대하여 MCAR or MAR로 판단하고 이에 대한 처리를 하는 방법

- 완전 분석 - 불완전 자료는 완전하게 무시하고 분석을 수행한다. 분석의 용이성을 보장하나 효율성 상실과 통계적 추론의 타당성에 문제 발생 가능성이 있다.

- 평균 대치법 - 관측 또는 실험으로 얻어진 데이터의 평균으로 결측치를 대치해서 사용한다. 평균에 의한 대치는 효율성의 향상 측면에서 장점이 있으나 통계량의 표준오차가 과소 추정되는 단점이 있다. 비조건부 평균대치법이라고도 한다.

- 회귀 대치법 - 회귀분석(regression)에 의한 예측치로 결측치를 대치하는 방법으로 조건부 평균 대치법이라고도 한다.

- 단순확률 대치법 - 평균 대치법에서 추정량 표준오차의 과소 추정을 보완하는 대치법으로 Hot-deck 방법이라고도 한다. 확률 추출에 의해서 전체 데이터 중 무작위로 대치하는 방법이다.

- 최근접 대치법 - 전체표본을 몇 개의 대체군으로 분류하여 각 층에서의 응답자료를 순서대로 정리한 후 결측값 바로 이전의 응답을 결측치로 대치한다. 응답값이 여러 번 사용될 가능성이 단점이다.

5. 다중 대치법

- 단순 대치법을 복수로 시행하여 통계적 효율성 및 일치성 문제를 보완하기 위하여 만들어진 방법이다.

- 1단계 - 대치단계 - 복수의 대치에 의한 결측을 대치한 데이터를 생성한다.

- 2단계 - 분석단계 - 복수 개의 데이터셋에 대한 분석을 시행한다.

- 3단계 - 결합단계 - 복수 개의 분석결과에 대한 통계적 결합을 통해 결과를 도출한다.

# 데이터 이상치 처리

1. 이상치(이상값, outlier)란 데이터의 전처리 과정에 발생 가능한 문제로 정상의 범주(데이터의 전체적 패턴)에서 벗어난 값을 의미한다.

2. 이상치의 종류

- 단변수 이상치 - 하나의 데이터 분포에서 발생하는 이상치를 말한다.

- 다변수 이상치 - 복수의 연결된 데이터 분포공간에서 발생하는 이상치를 의미한다.

3. 이상치의 발생 원인

- 비자연적 이상치 발생(Non-Natural Outlier)

- 입력실수 - 데이터의 수집과정에서 발생하는 에러로 입력 실수 등을 지칭한다.

- 측정오류 - 데이터의 측정중에 발생하는 에러로 측정기 고장으로 발생되는 문제이다.

- 실험오류 - 실험과정 중 발생하는 에러로 실험환경에서 야기된 모든 문제점을 지칭한다.

- 의도적 이상치 - 자기 보고 측정(Self-reported Measure)에서 발생되는 이상치를 지칭한다. 의도가 포함된 이상치로 예를 들어 남성의 키를 조사 시 의도적으로 키를 높게 기입하는 경우 등

- 자료처리오류 - 복수 개의 데이터셋에서 데이터를 추출,조합하여 분석 시 분석 전의 전처리에서 발생하는 에러를 말한다.

- 표본오류 - 모집단에서 표본을 추출하는 과정에서 편향이 발생하는 경우를 지칭한다.

- 상기 경우 이외에 발생하는 이상치들은 자연적 이상치(Natural Outlier)라고 한다.

4. 이상치의 탐지

- 종속변수가 단변량인지 다변량인지 데이터의 분포를 고려하여 모수적 or 비모수적인지에 따라 다양한 방법으로 고려해야 한다.

- 시각화를 통한 방법(비모수적, 단변량의 경우) - Box Plot, 줄기-잎 그림, 산점도 그림

- Z-Score 통한 방법(모수적 단변량 또는 저변량의 경우) - 정규화를 통해 threshold를 벗어난 경우를 이상치로 판별한다.

- 밀도기반 클러스터링 방법(DBSCAN, Density Based Spatial Clustering of Application with Noise) - 비모수적 다변랴으이 경우 군집간의 밀도를 이용하여 특정 거리 내의 데이터 수가 지정 개수 이상이면

군집으로 정의하는 방법이다. 정의된 군집에서 먼거리에 있는 데이터는 이상치로 간주한다.

- 고립 의사나무 방법(Isolation Forest) - 비모수적 다변량의 경우 의사결정나무(Decision Tree) 기반으로 정상치의 단말 노드(Terminal Node)보다 이상치의 노드에 이르는 길이(Path Length)가 더 짧은 성질을 이용하는 방법을 의미한다.

# 변수별 모형의 분류

1. 전체 모형(FM, Full Model) - 모든 독립변수를 사용한 모형

2. 축소 모형(RM, Reduced Model) - 전체 모형에서 사용된 변수의 개수를 줄여서 얻은 모형

3. 영 모형(NM, Null Model) - 독립변수가 하나도 없는 모형

# 변수의 선택 방법

1. 전진 선택법(Forward Selection) 

- 영 모형에서 시작, 모든 독립변수 중 종속변수와 단순상관계수의 절댓값이 가장 큰 변수를 분석모형에 포함시키는 것을 말한다.

- 부분 F 검정(F test)을 통해 유의성 검증을 시행, 유의한 경우는 가장 큰 F 통계량을 가지는 모형을 선택하고 유의하지 않은 경우는 변수선택 없이 과정을 중단한다.

- 한번 추가된 변수는 제거하지 않는다.

2. 후진 선택법(Backward Selection)

- 전체모델에서 시작, 모든 독립변수 중 종속변수와 단순상관계수의 절댓값이 가장 작은 변수를 분석모형에서 제외시킨다.

- 부분 F 검정(F test)을 통해 유의성 검증을 시행, 유의하지 않은 경우는 변수를 제거하고 유의한 경우는 변수제거 없이 과정을 중단한다.

- 한번 제거된 변수는 추가하지 않는다.

3. 단계적 선택법(Stepwise Selection)

- 전진 선택법과 후진 선택법의 보완방법이다.

- 전진 선택법을 통해 가장 유의한 변수를 모형에 포함 후 나머지 변수들에 대해 후진 선택법을 적용하여 새롭게 유의하지 않은 변수들을 제거한다.

- 제거된 변수는 다시 모형에 포함하지 않으며 유의한 설명변수가 존재하지 않을때까지 과정을 반복한다.

# 요인 분석

1. 다수의 변수들 간의 관계(상관관계)를 분석하여 공통차원을 축약하는 통계분석 과정이다.

2. 독립변수, 종속변수 개념이 없다. 주로 기술 통계에 의한 방법을 이용한다.

3. 주성분 분석(PCA), 공통요인 분석 특이값 분해(SVD), 음수미포함 행렬분해(NMF) 등이 있다.

4. 주성분 분석(PCA, Principal Component Analysis)

- 분포된 데이터들의 특성을 설명할 수 있는 하나 또는 복수 개의 특징(주성분, Principal Component)을 찾는 것을 의미한다.

- 서로 연관성이 있는 고차원공간의 데이터를 선형연관성이 없는 저차원(주성분)으로 변환하는 과정을 거친다(직교번환을 사용)

- 기존의 기본변수들을 새로운 변수의 세트로 변환하여 차원을 줄이되 기존 변수들의 분포특성을 최대한 보존하여 이를 통한 분석결과의 신뢰성을 확보한다.

- 데이터 하나하나에 대한 성분을 분석하는 것이 아니라, 여러 데이터들이 모여 하나의 분포를 이룰 때, 이 분포의 주성분을 분석해 주는 방법이라고 할 수 있다.

- v1의 방향과 크기, 그리고 v2의 방향과 크기를 알면 이 데이터분포가 어떤 형태인지를 가장 단순하면서도 효과적으로 파악할 수 있다.

- 스케일에 대한 영향이 크다. 즉 PCA 수행을 위해선 변수들 간의 스케일링이 필수이다.

5. 특이값 분해(SVD, Singular Value Decomposition)

- 데이터공간을 나타내는 m*n 크기의 행렬 M에 대해, 다음과 같이 분해 가능하다.

- M = U * 시그마 * V^t 

- 여기서 U는 m * m 크기의 직교행렬(Orthogonal Matrix)이고 시그마는 m*n 크기의 대각행렬(Diagonal Matrix), V^t는 n*n 크기의 전치행렬이다.

- 직교행렬 - 행렬의 열벡터가 독립이라는 의미..?!

- 대각행렬 - 행렬의 대각성분을 제외한 나머지행렬의 원소의 값이 모두 0인 행렬

6. 음수 미포함 행렬분해(NMF, Non-negative Matrix Factorization)

- 음수를 포함하지 않은 행렬 V를 음수를 포함하지 않은 두 행렬의 곱으로 분해하는 알고리즘

- 일반적으로 W의 열 개수와 H의 행 개수가 WH=V가 되도록 결정한다.

- 기존 행렬 V와 분해한 음수 미포함 행렬 W와 H의 곱과의 차이를 오차 U라고 이야기한다. V = WH + U

- 행렬 곱셈에서 곱해지는 행렬은 결과행렬보다 훨씬 적은 차원을 가지기 때문에 NMF가 차원을 축소할 수 있다.

# 파생변수

1. 데이터 마트(Data Mart)는 데이터 웨어하우스(Data Warehouse)로부터 복제 또는 자체 수집된 데이터모임의 중간층이지만 분석을 위한 기본단계변수가 모여지는 단계로 요약변수와 파생변수들의 모임이라고 볼 수 있다.

2. 기존의 변수를 조합하여 새로운 변수를 만들어 내는 것을 의미한다.

# 요약변수

1. 수집된 정보를 분석에 맞게 종합한 변수이다.

2. 데이터 마트에서 가장 기본적인 변수이다. 

3. 많은 분석 모델에서 공통으로 사용될 수 있어 재활용성이 높다.

# 변수 변환

1. 데이터를 분석하기 좋은 형태로 바꾸는 작업을 말한다.